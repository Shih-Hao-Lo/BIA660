Filip Radlinski:Serena Villata	NA	Session details: Posters	NA:NA	2018
Dominik Kowald:Paul Seitlinger:Tobias Ley:Elisabeth Lex	In this paper, we present the results of an online study with the aim to shed light on the impact that semantic context cues have on the user acceptance of tag recommendations. Therefore, we conducted a work-integrated social bookmarking scenario with 17 university employees in order to compare the user acceptance of a context-aware tag recommendation algorithm called 3Layers with the user acceptance of a simple popularity-based baseline. In this scenario, we validated and verified the hypothesis that semantic context cues have a higher impact on the user acceptance of tag recommendations in a collaborative tagging setting than in an individual tagging setting. With this paper, we contribute to the sparse line of research presenting online recommendation studies.	The Impact of Semantic Context Cues on the User Acceptance of Tag Recommendations: An Online Study	NA:NA:NA:NA	2018
Ryan A. Rossi:Nesreen K. Ahmed:Eunyee Koh	This paper describes a general framework for learning Higher-Order Network Embeddings (HONE) from graph data based on network motifs. The HONE framework is highly expressive and flexible with many interchangeable components. The experimental results demonstrate the effectiveness of learning higher-order network representations. In all cases, HONE outperforms recent embedding methods that are unable to capture higher-order structures with a mean relative gain in AUC of 19% (and up to 75% gain) across a wide variety of networks and embedding methods.	Higher-order Network Representation Learning	NA:NA:NA	2018
Peng Bao:Jiahui Wang	With the rapid development of scientific impact quantification in the field of science of success, the ability to identify the representative work of a researcher has important implications in a wide range of areas, including hiring, funding, and promotion systems. In this paper, we propose a two-step credit allocation algorithm (TSCA) for identifying the representative work of a researcher. This algorithm explicitly captures the importance of a paper, its relevance to other papers, and the unequally distributed contribution of each citation. We validate TSCA by applying it on the citation data from American Physical Society (APS) in the scenario of identifying the Nobel prize winning papers of the Nobel laureates. Experiments demonstrate that the proposed algorithm can significantly outperform the existing methods.	Identifying Your Representative Work Based on Credit Allocation	NA:NA	2018
Hongru Liang:Qian Li:Haozheng Wang:Hang Li:Jin-Mao Wei:Zhenglu Yang	Learning rap lyrics is an important area of music information retrieval because it is the basis of many applications, such as recommendation systems, automatic classification. In this paper, we tackle the issue pertaining to the lack of an effective approach to aggregate various features of lyrics by proposing an attention-based autoencoder for rap lyrics representation learning (AttAE-RL²). The proposed method appropriately integrates the semantic and prosodic features of rap lyrics. The preliminary experimental results demonstrate that our approach outperforms the state-of-the-art ones.	AttAE-RL²: Attention based Autoencoder for Rap Lyrics Representation Learning	NA:NA:NA:NA:NA:NA	2018
Yuta Takata:Mitsuaki Akiyama:Takeshi Yagi:Kunio Hato:Shigeki Goto	Threats of abusing websites that webmasters have stopped updating have increased. In this poster, we propose a method of predicting potentially abusable websites by retrospectively analyzing updates of software that composes websites. The method captures webmaster behaviors from archived snapshots of a website and analyzes the changes of web servers and web applications used in the past as update histories. A classifier that predicts website abuses is finally built by using update histories from snapshots of known malicious websites before the detections. Evaluation results showed that the classifier could predict various website abuses, such as drive-by downloads, phishes, and defacements, with accuracy: a 76% true positive rate and a 26% false positive rate.	POSTER: Predicting Website Abuse Using Update Histories	NA:NA:NA:NA:NA	2018
Yufei Wen:Lei Guo:Zhumin Chen:Jun Ma	With the advent of online social networks, the use of information hidden in social networks for recommendation has been extensively studied. Unlike previous work regarded social influence as regularization terms, we take advantage of network embedding techniques and propose an embedding based recommendation method. Specifically, we first pre-train a network embedding model on the users' social network to map each user into a low dimensional space, and then incorporate them into a matrix factorization model, which combines both latent and pre-learned features for recommendation. The experimental results on two real-world datasets indicate that our proposed model is more effective and can reach better performance than other related methods.	Network Embedding Based Recommendation Method in Social Networks	NA:NA:NA:NA	2018
Jingtao Ding:Fuli Feng:Xiangnan He:Guanghui Yu:Yong Li:Depeng Jin	Bayesian Personalized Ranking (BPR) is a representative pairwise learning method for optimizing recommendation models. It is widely known that the performance of BPR depends largely on the quality of the negative sampler. In this short paper, we make two contributions with respect to BPR. First, we find that sampling negative items from the whole space is unnecessary and may even degrade the performance. Second, focusing on the purchase feedback of the E-commerce domain, we propose a simple yet effective sampler for BPR by leveraging the additional view data. Compared to the vanilla BPR that applies a uniform sampler on all candidates, our view-aware sampler enhances BPR with a relative improvement of 27.36% and 69.54% on two real-world datasets respectively.	An Improved Sampler for Bayesian Personalized Ranking by Leveraging View Data	NA:NA:NA:NA:NA:NA	2018
Ting-Yu Yen:Yang-Yin Lee:Hen-Hsen Huang:Hsin-Hsi Chen	While recent word embedding models demonstrate their abilities to capture syntactic and semantic information, the demand for sense level embedding is getting higher. In this study, we propose a novel joint sense embedding learning model that retrofits the word representation into sense representation from contextual and ontological information. The experiment shows the effectiveness and robustness of our model that outperforms previous approaches in four public available benchmark datasets.	That Makes Sense: Joint Sense Retrofitting from Contextual and Ontological Information	NA:NA:NA:NA	2018
Qiang Xu:Xin Wang:Yueqi Xin:Zhiyong Feng:Renhai Chen	This paper presents a novel Pregel-based Distributed Subgraph Matching method PDSM to answer subgraph matching queries on big RDF graphs. In our method, the query graph is transformed to a spanning tree based on the breadth-first search (BFS). Two optimization techniques are proposed to filter out part of the unpromising intermediate results and postpone the Cartesian product operations in the Pregel iterative computation. The extensive experiments on both synthetic and real-world datasets show that PDSM outperforms the state-of-the-art methods by an order of magnitude.	PDSM: Pregel-Based Distributed Subgraph Matching on Large Scale RDF Graphs	NA:NA:NA:NA:NA	2018
Yueqi Xin:Bingyi Zhang:Xin Wang:Qiang Xu:Zhiyong Feng	This paper proposes a novel method for answering Pregel-based Parallel Provenance-aware Regular Path Queries (P3RPQ) on large RDF graphs. Our method is developed using the Pregel framework, which utilizes Glushkov automata to keep track of the matching process of RPQs in parallel. Meanwhile, four optimization strategies are devised, which can reduce the response time of the basic algorithm dramatically and overcome the counting paths problem to some extent. The experiments are conducted to verify the performance of our algorithms on both synthetic and real-world datasets.	P3RPQ: Pregel-Based Parallel Provenance-Aware Regular Path Query Processing on Large RDF Graphs	NA:NA:NA:NA:NA	2018
An-Zi Yen:Hen-Hsen Huang:Hsin-Hsi Chen	People are used to log their life on the social media platform. Life event can be expressed explicitly or implicitly in a text description. However, a description does not always contain life events related to a specific individual. To tell if there exist any life events and further know their categories is indispensable for event retrieval. This paper explores various LSTM models to detect and classify life events in tweets. Experiments show that the proposed Multi-Task LSTM model with attention achieves the best performance.	Detecting Personal Life Events from Twitter by Multi-Task LSTM	NA:NA:NA	2018
Reut Apel:Elad Yom-Tov:Moshe Tennenholtz	Users of social networks often focus on specific areas of that network, leading to the well-known "filter bubble" effect. Connecting people to a new area of the network in a way that will cause them to become active in that area could help alleviate this effect and improve social welfare. Here we present preliminary analysis of network referrals, that is, attempts by users to connect peers to other areas of the network. We classify these referrals by their efficiency, i.e., the likelihood that a referral will result in a user becoming active in the new area of the network. We show that by using features describing past experience of the referring author and the content of their messages we are able to predict whether referral will be effective, reaching an AUC of 0.87 for those users most experienced in writing efficient referrals. Our results represent a first step towards algorithmically constructing efficient referrals with the goal of mitigating the "filter bubble" effect pervasive in on line social networks.	Characterizing Efficient Referrals in Social Networks	NA:NA:NA	2018
Moumita Basu:Anurag Shandilya:Kripabandhu Ghosh:Saptarshi Ghosh	During a disaster event, it is essential to know about needs and availabilities of different types of resources, for coordinating relief operations. Microblogging sites are frequently used for aiding post-disaster relief operations, and there have been prior attempts to identify tweets that inform about resource needs and availabilities (termed as need-tweets and availability-tweets respectively). However, there has not been much attempt to effectively utilise such tweets. We introduce the problem of automatically matching need-tweets with appropriate availability-tweets, which is practically important for coordination of post-disaster relief operations. We also experiment with several methodologies for automatically matching need-tweets and availability-tweets.	Automatic Matching of Resource Needs and Availabilities in Microblogs for Post-Disaster Relief	NA:NA:NA:NA	2018
Gaoyang Guo:Chaokun Wang:Xiang Ying	A myriad of community detection methods have been designed to discover communities based on specific network features in different disciplines, such as sociology, physics, and computer science. Consequentially, we have to face the problem of Algorithm Selection for Community Detection (ASCD): Given a specific network, which algorithm should we select to reveal its latent community structures In this study, we propose a model called CYDES to address the ASCD problem. CYDES consists of two parts, namely feature matrix generation and algorithm classification. We combine three effective feature extraction methods with the idea of BOW model to construct a fixed-size feature matrix. After a nonlinear transformation to the feature matrix, a softmax regression model is utilized to generate a classification label representing the best community detection algorithm we select. Extensive experimental results demonstrate that CYDES has high algorithm selection quality for community detection in networks.	Which Algorithm Performs Best: Algorithm Selection for Community Detection	NA:NA:NA	2018
Longtao Huang:Shangwen Lv:Liangjun Zang:Yipeng Su:Jizhong Han:Songlin Hu	This paper proposes a novel approach to retrieve news articles related to a specific event and generate a storyline to help people understand the event evolution. First, a similarity calculation method is proposed to retrieve news articles related to the specific event, which combines textual similarity, temporal similarity and entity similarity. Then a multi-view attribute graph is constructed to represent the relationship between retrieved articles. Finally, a community detection algorithm is developed to segment and chain subevents in the graph. Experimental results on real-world datasets demonstrate that the proposed approach achieve better results than existing methods.0F0F	A Fresh Look at Understanding News Events Evolution	NA:NA:NA:NA:NA:NA	2018
Yu Fu:Dongliang Zhang:Hao Jiang	Besides usability, visual appearance also plays an important role in influencing users' attitudes towards the mobile shopping apps. This article presents a pilot study that explores users' and designers' preferences for mobile shopping app user interfaces (UIs). The study consisted of two phases. (1) Eliciting participants' perception of UIs similarity by sorting, using DISTATIS and cluster analysis, the UIs similarity perceptual space was identified. (2) Eliciting participants' overall preference by rating. The results identified three typical UIs and the distribution of ideal preference UIs for users and designers. Last, users and designers' differences in the UI preference were discussed.	Comparison of Users' and Designers' Differences in Mobile Shopping App Interface Preferences and Designs	NA:NA:NA	2018
Saurabh Sohoney:Nikita Prabhu:Vineet Chaoji	Inverse Propensity Score estimator (IPS) is a basic, unbiased, off-policy evaluation technique to measure the impact of a user-interactive system without serving live traffic. We present our work on applying IPS to real-world settings by addressing some practical challenges, thereby enabling successful policy evaluation. In particular, we show that off-policy evaluation can be impossible in the absence of a complete context and we describe a systematic way of defining the context.	Handling Confounding for Realistic Off-Policy Evaluation	NA:NA:NA	2018
Yunqi Qiu:Manling Li:Yuanzhuo Wang:Yantao Jia:Xiaolong Jin	Topic entity detection is to find out the main entity asked in a question, which is significant in question answering. Traditional methods ignore the information of entities, especially entity types and their hierarchical structures, restricting the performance. To take full advantage of Knowledge Base(KB) and detect topic entities correctly, we propose a deep neural model to leverage type hierarchy and relations of entities in KB. Experimental results demonstrate the effectiveness of the proposed method.	Hierarchical Type Constrained Topic Entity Detection for Knowledge Base Question Answering	NA:NA:NA:NA:NA	2018
Dhruv Gupta:Klaus Berberich	Knowledge graphs capture very little temporal information associated with facts. In this work, we address the problem of identifying time intervals of knowledge graph facts from large document collections annotated with temporal expressions. Prior approaches in this direction have leveraged limited metadata associated with documents in large collections (e.g., publication dates) or have limited techniques to model the uncertainty and dynamics of temporal expressions. Our approach to identify time intervals for time-sensitive facts in knowledge graphs leverages a time model that incorporates uncertainty and models them at different levels of granularity (i.e., day, month, and year). Evaluation on a temporal fact benchmark using two large news archives amounting to more than eleven million documents show the quality of our results.	Identifying Time Intervals for Knowledge Graph Facts	NA:NA	2018
Fei Wu:Yanen Li:Ning Xu	The affinity of a user to a type of items (e.g., stories from the same publisher, and movies of the same genre) is an important signal reflecting the user's interests. Accurately estimating of the user type affinity has various applications in ranking and recommendation systems. For frequent users, simply dividing the number of interactions with content (e.g., clicks) by the number of impressions (e.g., the number of times the content is presented to each user) would be a good estimate. However, such estimates are erroneous for users who have sparse interaction history, (e.g., new users). To alleviate the problem, feature-based approaches aim to learn functions predicting the affinity score using only none-click features, such as user demographics, locations, and interests. Likewise, such approaches do not take full advantage of the interaction history of frequent users. Motivated by the limitations of the two approaches, we propose a Gamma-Poisson model that aims at utilizing the interaction history of frequent users, as well as leveraging a feature-based model for infrequent users. Our intuition is that we should rely more on the interaction history when estimating affinity for frequent users, and weigh more on feature-based model for infrequent users. We present experimental results on large-scale real-world data in a publisher content clicks prediction task to demonstrate the effectiveness of the proposed method in estimating user type affinity scores.	User Type Affinity Estimation Using Gamma-Poisson Model	NA:NA:NA	2018
Qiang Zhang:Emine Yilmaz:Shangsong Liang	A valuable step towards news veracity assessment is to understand stance from different information sources, and the process is known as the stance detection. Specifically, the stance detection is to detect four kinds of stances ("agree'', "disagree'', "discuss'' and "unrelated'') of the news towards a claim. Existing methods tried to tackle the stance detection problem by classification-based algorithms. However, classification-based algorithms make a strong assumption that there is clear distinction between any two stances, which may not be held in the context of stance detection. Accordingly, we frame the detection problem as a ranking problem and propose a ranking-based method to improve detection performance. Compared with the classification-based methods, the ranking-based method compare the true stance and false stances and maximize the difference between them. Experimental results demonstrate the effectiveness of our proposed method.	Ranking-based Method for News Stance Detection	NA:NA:NA	2018
Anurag Roy:Kripabandhu Ghosh:Moumita Basu:Parth Gupta:Saptarshi Ghosh	The Web has several information sources on which an ongoing event is discussed. To get a complete picture of the event, it is important to retrieve information from multiple sources. We propose a novel neural network based model which integrates the embeddings from multiple sources, and thus retrieves information from them jointly, %all the sources together, as opposed to combining multiple retrieval results. The importance of the proposed model is that no document-aligned comparable data is needed. Experiments on posts related to a particular event from three different sources - Facebook, Twitter and WhatsApp - exhibit the efficacy of the proposed model.	Retrieving Information from Multiple Sources	NA:NA:NA:NA:NA	2018
Tanmoy Chakraborty:Zhe Cui:Noseong Park	A community detection (CD) method is usually evaluated by what extent it is able to discover the 'ground-truth' community structure of a network. A certain 'node-centric metadata' is used to define the ground-truth partition. However, nodes in real networks often have multiple metadata types (e.g., occupation, location); each can potentially form a ground-truth partition. Our experiment with 10 CD methods on 5 datasets (having multiple metadata-based ground-truth partitions) show that the metadata-based evaluation is misleading because there is no single CD method that can outperform others by detecting all types of metadata-based partitions. We further show that the community structure obtained from the CD methods is usually topologically stronger than any metadata-based partitions. Finally, we suggest a new task-based evaluation framework for CD methods and show that a certain type of CD methods is useful for a certain type of task.	Metadata vs. Ground-truth: A Myth behind the Evolution of Community Detection Methods	NA:NA:NA	2018
Zeyang Lei:Yujiu Yang:Yi Liu	Sentiment analysis of social media and comment data is an important issue in opinion monitoring. In this work, we propose a Linguistic-Aware Attention Network (LANN) to enhance the performance of convolution neural network (CNN). LANN adopts a two-stage strategy to model the sentiment-specific sentence representation. First, an interactive attention mechanism is designed to model word-level semantics. Second, to capture phrase-level linguistic structure, a dynamic semantic attention is adopted to select the crucial phrase chunks in the sentence. The experiments demonstrate that LANN has robust superiority over competitors and has reached the state-of-the-art performance.	LAAN: A Linguistic-Aware Attention Network for Sentiment Analysis	NA:NA:NA	2018
Daniel Alexandrov:Viktor Karepin:Ilya Musabirov:Daria Chuprina	We use social media and WWW data to analyse international educational migration from Russia. We find substantial regional differences in migration patterns for three contrast directions: the Nordic countries, China and the Middle East. We built a model of migration flows with geographic distances to destination countries, various socio-demographic data and institutional characteristics of educational organisations.	Educational Migration from Russia to the Nordic Countries, China and the Middle East. Social Media Data	NA:NA:NA:NA	2018
Mengjing Chen:Weiran Shen:Pingzhong Tang:Song Zuo	Over the past few years, ride-sharing has been proven to be an effective way to relieve urban traffic congestion, as evidenced by several emerging ride-sharing platforms such as Uber and Didi. A key economic problem for these platforms is to design a revenue-optimal (or welfare-optimal) pricing scheme and a corresponding vehicle dispatching policy that incorporates geographic information, and more importantly, dynamic supply and demand. In this paper, we aim to solve this problem by introducing a unified model that takes into account both travel time and driver redirection. We tackle the non-convexity problem using the "ironing" technique and formulate the optimization problem as a Markov decision process (MDP), where the states are the driver distributions and the decision variables are the prices. Our main finding is to give an efficient algorithm that computes the exact revenue (or welfare) optimal randomized pricing schemes. We characterize the optimal solutions of the MDP by primal-dual analysis of a convex program. We also conduct empirical analysis of our solution with real data of a major ride-sharing platform and show its significant advantages over fixed pricing schemes as well as those prevalent surge-based pricing schemes.	Optimal Vehicle Dispatching for Ride-sharing Platforms via Dynamic Pricing	NA:NA:NA:NA	2018
Deguang Kong:Xiannian Fan:Konstantin Shmakov:Jian Yang	Bid optimization, which aims to find the competitive bid to achieve the best performance for the advertiser, is an important problem in online advertising. The optimal bid recommendation enables the advertisers to make informed decisions without actually spending the budget. In this paper, we consider a bid optimization scenario that the advertiser's budget can be split across multiple campaigns. To achieve the optimal performance, we formalize the bid optimization problem as a constraint combinational optimization problem, and derive an effective method to solve it. Experiment studies on real- world ad campaigns demonstrate the effectiveness of our method.	A Combinational Optimization Approach for Advertising Budget Allocation	NA:NA:NA:NA	2018
Zhile Jiang:Shuai Yu:Qiang Qu:Min Yang:Junyu Luo:Juncheng Liu	Author profiling is an important but challenging task. In this paper, we propose a novel Multi-Task learning framework for Author Profiling (MTAP), in which a document modeling module is shared across three different author profiling tasks (i.e., age, gender and job classification tasks). To further boost author profiling, we integrate hierarchical features learned by different models. Concretely, we employ CNN, LSTM and topic model to learn the character-level, word-level and topic-level features, respectively. MTAP thus leverages the benefits of supervised deep neural neural networks as well as an unsupervised probabilistic generative model to enhance the document representation learning. Experimental results on a real-life blog dataset show that MTAP has robust superiority over competitors and sets state-of-the-art for all the three author profiling tasks	Multi-task Learning for Author Profiling with Hierarchical Features	NA:NA:NA:NA:NA:NA	2018
Qi Zhu:Xiang Ren:Jingbo Shang:Yu Zhang:Frank F. Xu:Jiawei Han	Extracting entities and their relations from text is an important task for understanding massive text corpora. Open information extraction (IE) systems mine relation tuples (i.e., entity arguments and a predicate string to describe their relation) from sentences. However, current open IE systems ignore the fact that global statistics in a large corpus can be collectively leveraged to identify high-quality sentence-level extractions. In this paper, we propose a novel open IE system, called ReMine, which integrates local context signal and global structural signal in a unified framework with distant supervision. The new system can be efficiently applied to different domains as it uses facts from external knowledge bases as supervision; and can effectively score sentence-level tuple extractions based on corpus-level statistics. Specifically, we design a joint optimization problem to unify (1) segmenting entity/relation phrases in individual sentences based on local context; and (2) measuring the quality of sentence-level extractions with a translating-based objective. Experiments on real-world corpora from different domains demonstrate the effectiveness and robustness of ReMine when compared to other open IE systems.	Open Information Extraction with Global Structure Constraints	NA:NA:NA:NA:NA:NA	2018
Wenyu Du:Shuai Yu:Min Yang:Qiang Qu:Jia Zhu	In this paper, we propose GPSP, a novel Graph Partition and Space Projection based approach, to learn the representation of a heterogeneous network that consists of multiple types of nodes and links. Concretely, we first partition the heterogeneous network into homogeneous and bipartite subnetworks. Then, the projective relations hidden in bipartite subnetworks are extracted by learning the projective embedding vectors. Finally, we concatenate the projective vectors from bipartite subnetworks with the ones learned from homogeneous subnetworks to form the final representation of the heterogeneous network. Extensive experiments are conducted on a real-life dataset. The results demonstrate that GPSP outperforms the state-of-the-art baselines in two key network mining tasks: node classification and clustering.	GPSP: Graph Partition and Space Projection based Approach for Heterogeneous Network Embedding	NA:NA:NA:NA:NA	2018
Yong-Yeon Jo:Myung-Hwan Jang:Hyungsoo Jung:Sang-Wook Kim	Existing single-machine based graph engines do not leverage the characteristic of social networks following the power-law degree distribution. We propose a new graph engine tailored for processing and analyzing large-scale social networks efficiently by exploiting the power-law degree property	A High-Performance Graph Engine for Efficient Social Network Analysis	NA:NA:NA:NA	2018
Suman Kalyan Maity:Santosh K. C.:Arjun Mukherjee	In this paper, we propose a semi-supervised framework Spam2Vec to identify spammers in Twitter. This algorithmic framework learns the spam representations of the node in the network by leveraging biased random walks. Our spammer detection method yields an AUC of 0.54 with [email protected] as 0.12 and performs significantly better with 7.77% increase in AUC and a 2.4 times improvement on precision over the best performing baseline.	Spam2Vec: Learning Biased Embeddings for Spam Detection in Twitter	NA:NA:NA	2018
Zhitao Wang:Chengyao Chen:Wenjie Li	In this paper, we propose an attention network for diffusion prediction problem. The developed diffusion attention module can effectively explore the implicit user-to-user diffusion dependency among information cascade users. Besides, the user-to-cascade importance and the time-decay effect are captured and utilized by the model. The superiority of the proposed model over state-of-the-art methods is demonstrated by experiments on real diffusion data.	Attention Network for Information Diffusion Prediction	NA:NA:NA	2018
Po-Cheng Huang:Hen-Hsen Huang:Hsin-Hsi Chen	Knowledge base completion (KBC) involves in discovering missing facts. However, knowledge changes over time. Some facts need to be removed from knowledge base (KB) to keep knowledge base integrity (KBI) while new facts are inserted or old facts are deleted. This paper proposes a path-based learning model to learn the dependency of dynamic relations automatically. In this way, we can eliminate the conflicting facts and keep KB clean. That would be a significant benefit for KBC and other tasks using KB.	Path Ranking with Path Difference Sets for Maintaining Knowledge Base Integrity	NA:NA:NA	2018
Chen Ling:Lei Wang:Jun Lang:Qiufen Xia:Guoxuan Chang:Kun Wang:Peng Zhao	Internet access restriction in various areas unceasingly poses inconvenience for users. Unblocked websites or web pages always accompany couple fate-to-fail links from blocked servers due to such restrictions, while users wait for a long time to see contents of these blocked/invalid links. Therefore, it is better to directly show timeout links to users for better experiences rather than making users excessively wait. In this paper, we present LinCa (Links Catcher), a novel approach that fully considers the Internet access restriction and reduces page loading time on client-side by parsing all HTTP requests, and intercepting all invalid links when a web navigation starts. Thus, we first create and maintain a Rule Base to store invalid links under given access restriction rules. We then update the Rule Base periodically to cover as many invalid links as possible and remove links that become valid. We finally demonstrate the effectiveness of LinCa through experiments by building and deploying a Chrome extension. Experimental results show that LinCa can reduce page loading time with average 28.12% of original page loading time for our data sets.	LinCa: A Page Loading Time Optimization Approach for Users Subject to Internet Access Restriction	NA:NA:NA:NA:NA:NA:NA	2018
Wouter Lightenberg:Yulong Pei:George Fletcher:Mykola Pechenizkiy	We introduce the Tink library for distributed temporal graph analytics. Increasingly, reasoning about temporal aspects of graph-structured data collections is an important aspect of analytics. For example, in a communication network, time plays a fundamental role in the propagation of information within the network. Whereas existing tools for temporal graph analysis are built stand alone, Tink is a library in the Apache Flink ecosystem, thereby leveraging its advanced mature features such as distributed processing and query optimization. Furthermore, Flink requires little effort to process and clean the data without having to use different tools before analyzing the data. Tink focuses on interval graphs in which every edge is associated with a starting time and an ending time. The library provides facilities for temporal graph creation and maintenance, as well as standard temporal graph measures and algorithms. Furthermore, the library is designed for ease of use and extensibility.	Tink: A Temporal Graph Analytics Library for Apache Flink	NA:NA:NA:NA	2018
Debasis Ganguly:Kripabandhu Ghosh	Effective clustering of short documents, such as tweets, is difficult because of the lack of sufficient semantic context. Word embedding is a technique that is effective in addressing this lack of semantic context. However, the process of word vector embedding, in turn, relies on the availability of sufficient contexts to learn the word associations. To get around this problem, we propose a novel word vector training approach that leverages topically similar tweets to better learn the word associations. We test our proposed word embedding approach by clustering a collection of tweets on disasters. We observe that the proposed method improves clustering effectiveness by up to 14%.	Contextual Word Embedding: A Case Study in Clustering Tweets about Emergency Situations	NA:NA	2018
Markus Schedl:Eelco Wiechert:Christine Bauer	We approach the research question whether real-world events, such as sport events or product launches, influence music consumption behavior. To this end, we consider events of different categories from Google Trends and model listening events as time series using Last.fm data. Performing an auto-regressive integrated moving average analysis to decompose the signal and subsequently an intervention time series analysis, we find significant signal discontinuities, in particular for the Google news category. We found that news and events are likely to increase the number of songs listened to per person per day by about 2%, while tech events commonly cause 1% less music being consumed.	The Effects of Real-world Events on Music Listening Behavior: An Intervention Time Series Analysis	NA:NA:NA	2018
Abhijnan Chakraborty:Mohammad Luqman:Sidhartha Satapathy:Niloy Ganguly	With a large number of stories emerging from the newsrooms, media websites need to curate interesting news for their readers. Although traditionally news was curated solely by human editors, increasing news volume has led media outlets to adopt editorial algorithms. However, such algorithms are often proprietary, and smaller outlets do not have the resources to build them from scratch. In this paper, we present a novel framework 'Samar' to automatically curate news by optimizing recency, relevance and diversity of the selected stories. Evaluations over two real-world news datasets show that Samar outperforms several state-of-the-art baselines in matching the news curation performed by human editors.	Editorial Algorithms: Optimizing Recency, Relevance and Diversity for Automated News Curation	NA:NA:NA:NA	2018
Hoang-Long Nguyen:Claudia-Lavinia Ignat:Olivier Perrin	Public key server is a simple yet effective way of key management in secure end-to-end communication. To ensure the trustworthiness of a public key server, transparent log systems such as CONIKS employ a tamper-evident data structure on the server and a gossiping protocol among clients in order to detect compromised servers. However, due to lack of incentive and vulnerability to malicious clients, a gossiping protocol is hard to implement in practice. Meanwhile, alternative solutions such as EthIKS are not scalable. This paper presents Trusternity, an auditing scheme relying on Ethereum blockchain that is easy to implement, scalable and inexpensive to operate.	Trusternity: Auditing Transparent Log Server with Blockchain	NA:NA:NA	2018
Jiaming Song:Xiaowang Zhang:Peng Peng:Zhiyong Feng:Lei Zou	In this paper, we present a plugin-based framework (MapSQ) with three parts for SPARQL queries utilizing high-performance of GPU to accelerate answering in a convenient way. Selector chooses suitable join order according to characteristics of data and queries. Executor answers subqueries and returns intermediate solutions and GPU Computing obtains the join result of intermediate solutions through MapReduce. Finally, we evaluate MapSQ bulit on gStore and RDF-3X on the LUBM benchmark and YAGO datasets (over 200 million triples). The experimental results show that MapSQ significantly improves the performance of SPARQL query engines with speedup up to 33.	MapSQ: A Plugin-based MapReduce Framework for SPARQL Queries on GPU	NA:NA:NA:NA:NA	2018
Deguang Kong:Konstantin Shmakov:Jian Yang	In cost-per-click (CPC) or cost-per-impression (CPM) advertising campaigns, advertisers always run the risk of spending the bud- get without getting enough conversions. Moreover, the bidding on advertising inventory has few connections with propensity that can reach to cost-per-acquisition (CPA) goals. To address this problem, this paper presents a bid optimization scenario to achieve the desired CPA goals for advertisers. In particular, we build the optimization engine to make a decision by solving the constrained optimization problem. The proposed model can naturally recommend the bid that meets the advertisers' expectations by making inference over history auction behaviors. The bid optimization model outperforms the baseline methods on real-world campaigns, and can be applied into a wide range of scenarios for performance improvement and revenue liftup.	Demystifying Advertising Campaign for CPA Goal Optimization	NA:NA:NA	2018
Ugo Tanielian:Anne-Marie Tousch:Flavian Vasile	Over the last decade, the number of devices per person has increased substantially. This poses a challenge for cookie-based personalization applications, such as online search and advertising, as it narrows the personalization signal to a single device environment. A key task is to find which cookies belong to the same person to recover a complete cross-device user journey. Recent work on the topic has shown the benefits of using unsupervised embeddings learned on user event sequences. In this paper, we extend this approach to a supervised setting and introduce the Siamese Cookie Embedding Network (SCEmNet), a siamese convolutional architecture that leverages the multi-modal aspect of sequences, and show significant improvement over the state-of-the-art.	Siamese Cookie Embedding Networks for Cross-Device User Matching	NA:NA:NA	2018
Saket Maheshwary:Hemant Misra	In this paper we investigate the important and challenging task of recommending appropriate jobs for job seeking candidates by matching semi structured resumes of candidates to job descriptions. To perform this task, we propose to use a siamese adaptation of convolutional neural network. The proposed approach effectively captures the underlying semantics thus enabling to project similar resumes and job descriptions closer to each other, and make dissimilar resumes and job descriptions distant from each other in the semantic space. Our experimental results on a set of 1314 resumes and a set of 3809 job descriptions (5,005,026 resume-job description pairs) demonstrate that our approach is better than the current state-of-the-art approaches.	Matching Resumes to Jobs via Deep Siamese Network	NA:NA	2018
Baoxu Shi:Tim Weninger	Understanding and visualizing human discourse has long being a challenging task. Although recent work on argument mining have shown success in classifying the role of various sentences, the task of recognizing concepts and understanding the ways in which they are discussed remains challenging. Given an email thread or a transcript of a group discussion, our task is to extract the relevant concepts and understand how they are referenced and re-referenced throughout the discussion. In the present work, we present a preliminary approach for extracting and visualizing group discourse by adapting Wikipedia's category hierarchy to be an external concept ontology. From a user study, we found that our method achieved better results than 4 strong alternative approaches, and we illustrate our visualization method based on the extracted discourse flows.	Visualizing the Flow of Discourse with a Concept Ontology	NA:NA	2018
Deguang Kong:Konstantin Shmakov:Jian Yang	In online advertising, a common objective for advertisers is to get the maximum returns on investment given the budget. On one hand, if the bid is too high, the advertiser pays more money than he should pay for the same number of clicks. On the other hand, it the bid is too low, the advertiser cannot win in auctions and therefore it loses the opportunity. A challenging problem is how to recommend the bid to achieve the maximum values for advertisers. In this paper, we present an inflection point approach for bid recommendation from discovering the bid price of click(bid)1 function at which the function changes from significant increase (i.e. concave downward) to slow increase (convex upward). We derive the optimal solution using history sparse and noisy observations given the budget limit. In real word advertising campaign evaluations, the proposed bid recommendation scenario brings in 15.37% bid increase and 30.24% click increase over the baselines.	An Inflection Point Approach for Advertising Bid Optimization	NA:NA:NA	2018
Helen Spiers:Alexandra Swanson:Lucy Fortson:Brooke D. Simmons:Laura Trouille:Samantha Blickhan:Chris Lintott	Human-computer systems are increasingly applied to data reduction problems; citizen science platforms (e.g. the Zooniverse) are one type of such a system. These platforms function as social machines, combining volunteer efforts with automated processes to enable distributed data analysis. The rapid growth of this approach is increasing the need to understand how we can improve volunteer interaction and engagement. Here, we utilize the most comprehensive collection of online citizen science data gathered to date to examine multiple variables across 63 Zooniverse projects. Our analyses reveal how subtle design changes can influence many facets of volunteer interaction, generating insights that have implications for the design and study of citizen science projects, and future research.	Patterns of Volunteer Behaviour Across Online Citizen Science	NA:NA:NA:NA:NA:NA:NA	2018
Evgeny Krivosheev:Bahareh Harandizadeh:Fabio Casati:Boualem Benatallah	In this paper we describe how crowd and machine classifier can be efficiently combined to screen items that satisfy a set of predicates. We show that this is a recurring problem in many domains, present machine-human (hybrid) algorithms that screen items efficiently and estimate the gain over human-only or machine-only screening in terms of performance and cost.	Crowd-Machine Collaboration for Item Screening	NA:NA:NA:NA	2018
Anurag Shandilya:Kripabandhu Ghosh:Saptarshi Ghosh	We propose to evaluate extractive summarization algorithms from a completely new perspective. Considering that an extractive summarization algorithm selects a subset of the textual units in the input data for inclusion in the summary, we investigate whether this selection is fair. We use several summarization algorithms over datasets that have a sensitive attribute (e.g., gender, political leaning) associated with the textual units, and find that the generated summaries often have very different distributions of the said attribute. Specifically, some classes of the textual units are under-represented in the summaries according to the fairness notion of adverse impact. To our knowledge, this is the first work on fairness of summarization, and is likely to open up interesting research problems.	Fairness of Extractive Text Summarization	NA:NA:NA	2018
Elias Moons:Tinne Tuytelaars:Marie-Francine Moens	Images have a prominent role in the communication of news on the Web. We propose a novel method for image classification with subject categories when limited annotated images are available for training the classifier. A neural network based encoder learns image representations from paired news images and their texts. Once trained, this encoder transforms any image to a text-enriched representation of the image, which is then used as input for the classifier that categorizes an image according to its subject category. We have trained classifiers with different amounts of annotated images and found that the image classifier that uses the text-enriched image representations outperforms a baseline model that only uses image features especially in cases with limited training examples.	Text-Enriched Representations for News Image Classification	NA:NA:NA	2018
Jurek Leonhardt:Avishek Anand:Megha Khosla	Recent works in recommendation systems have focused on diversity in recommendations as an important aspect of recommendation quality. In this work we argue that the post-processing algorithms aimed at only improving diversity among recommendations lead to discrimination among the users. We introduce the notion of user fairness which has been overlooked in literature so far and propose measures to quantify it. Our experiments on two diversification algorithms show that an increase in aggregate diversity results in increased disparity among the users.	User Fairness in Recommender Systems	NA:NA:NA	2018
Tomoki Sato:Hiroaki Shiokawa:Yuto Yamaguchi:Hiroyuki Kitagawa	ObjectRank is one of the popular graph mining methods that enables us to evaluate the importance of each vertex on heterogeneous graphs. However, it is computationally expensive to apply it to large graphs since ObjectRank needs to compute the importance of all vertices iteratively. In this work, we present a fast ObjectRank algorithm,FORank, that accurately approximates the keyword search results. FORank iteratively prunes vertices whose convergence score likely has less impact on the results during iterative computation. The experiments showed that FORank runs 7 times faster than ObjectRank computation with over 90% accuracy approximation.	FORank: Fast ObjectRank for Large Heterogeneous Graphs	NA:NA:NA:NA	2018
Amit Sarkar:G. Srinivasaraghavan	We investigate the task of reading context-biased web summarization, where the goal is to extract information relevant to the current reading context from a cited web article. In certain kind of linked document sets such as Wikipedia articles, scientific papers as well as news and blogs, such contextual summaries can be useful in providing additional related information to the user helping in the reading task. In this work, we focus on web articles only and try to find out the set of key components that contribute to building up the reading context. We build a supervised model for ranking sentences from the cited document according to their contextual salience. Initial evaluation based on annotated data-set of web articles show that our ranking model performs better than the generic summaries as well as baseline context-biased summaries.	Contextual Web Summarization: A Supervised Ranking Approach	NA:NA	2018
Madian Khabsa:Ahmed El Kholy:Ahmed Hassan Awadallah:Imed Zitouni:Milad Shokouhi	Digital assistants are emerging to become more prevalent in our daily lives. In interacting with these assistants, users may engage in multiple tasks within a short period of time. Identifying task boundaries and isolating them within a session is critical for measuring the performance of the system on each individual task. In this paper we aim to automatically identify sequences of interactions that together form a task. To this end, we sample interactions from a real world digital assistant and use crowd judges to segment a session into multiple tasks. After that, we use a machine learned model to identify task boundaries. Our learned model with its features significantly outperform the baselines. To the best of our knowledge, this is the first work that aims to identify tasks within digital assistant sessions.	Identifying Task Boundaries in Digital Assistants	NA:NA:NA:NA:NA	2018
Martin Atzmueller:Florian Lemmerich	Academic conferences are a backbone for the exchange of ideas in scientific communities. However, so far little is known about the communication networks emerging at those venues. Besides personal knowledge, network homophily has been identified as a driving factor for establishing contacts and followerships in social networks, i.e., people are more likely to engage with others if they are similar with respect to certain attributes. In this paper, we describe work in progress on investigating homophily at four academic conferences based on face-to-face (F2F) contact data collected using wearable sensors between conference participants. In particular, we study which personal attributes are predictive for face-to-face contacts. For that purpose, we obtained diverse personal attributes from online sources in order to elicit a variety of hypotheses, which can then be compared using descriptive statistics and a Bayesian method for comparing hypotheses in networks. Our results suggest that personal knowledge (as derived from DBLP and ResearchGate networks) and homophilic behavior with respect to several attributes, e.g., gender or country of origin, are important factors for contacts at academic conferences.	Homophily at Academic Conferences	NA:NA	2018
William Brendel:Fangqiu Han:Luis Marujo:Luo Jie:Aleksandra Korolova	Making friend recommendations is an important task for social networks, as having more friends typically leads to a better user experience. Most current friend recommendations systems grow the existing network at the cost of privacy. In particular, any given user's friend graph may be directly or indirectly leaked as a result of such recommendations. In many situations this is not desirable, as the friend list may reveal much about the user--from their identity to their sexual orientation and interests. In this work, we focus on the "cold start" problem of making friend recommendations for new users while raising the bar on protecting the privacy of the friend list of all users. We propose a practical friend recommendation framework, tested on the Snapchat social network, that preserves the privacy of users' friends lists with respect to brute-force attacks and scales to millions of users.	Practical Privacy-Preserving Friend Recommendations on Social Networks	NA:NA:NA:NA:NA	2018
Yingying Wu:Yiqun Liu:Ke Zhou:Xiaochuan Wang:Min Zhang:Shaoping Ma	Diversifying search results to satisfy as many users' intentions as possible is NP-hard. Some research employs a pruned exhaustive search, and some uses a greedy approach. However, the objective function of the result diversification problem adopts the cascade assumption which assumes users' information needs will drop once their subtopic search intents are satisfied. As a result, the intent distribution of diversified results deviates from the actual distribution of user intentions, and each subtopic tends to be chosen equally. This phenomenon is unreasonable, especially when the original distribution of user intent is unbalanced. In this paper, we present empirical evidence of the diversification equilibrium by showing that the standard deviations of subtopic distribution approach zero.	Treating Each Intent Equally: The Equilibrium of IA-Select	NA:NA:NA:NA:NA:NA	2018
Peizhi Wu:Yi Tu:Zhenglu Yang:Adam Jatowt:Masato Odagaki	Modeling the evolution of user preferences and item attributes in a dynamic social network is important because it is the basis for many applications, including recommendation systems and user behavior analysis. This study introduces a comprehensive general neural framework with several optimal strategies to jointly model the evolution of user preferences and item attributes in dynamic social networks. Preliminary experimental results conducted on real-world datasets demonstrate that our model performs better than the state-of-the-art methods.	Deep Modeling of the Evolution of User Preferences and Item Attributes in Dynamic Social Networks	NA:NA:NA:NA:NA	2018
Yingru Lin:Soyeon Caren Han:Byeong Ho Kang	The peer assessment approach is considered to be one of the best solutions for scaling both assessment and peer learning to global classrooms, such as MOOCs. However, some academic staff hesitate to use a peer assessment approach for their classes due to concerns about its credibility and reliability. The focus of our research is to detect the credibility level of each assessment performed by students during peer assessment. We found three major scopes in assessing the credibility level of evaluations, 1) Informativity, 2) Accuracy, and 3) Consistency. We collect assessments, including comments and grades provided by students during the peer assessment process and then each feedback-and-grade pair is labeled with its credibility level by Mechanical Turk evaluators. We extract relevant features from each labeled assessment and use them to build a classifier that attempts to automatically assess its level of credibility in C5.0 Decision Tree classifier. The evaluation results show that the model can be used to automatically classify peer assessments as credible or non-credible, with accuracy in the range of 88%.	Machine Learning for the Peer Assessment Credibility	NA:NA:NA	2018
Qian Li:Ziwei Li:Jin-Mao Wei:Zhenglu Yang:Yanhui Gu:R. Uday Kiran	Predicting the ending of a story is an interesting issue that has attracted considerable attention, as in case of the ROC Story Cloze Task (SCT). Although several studies have addressed this issue, the performance remains unsatisfactory due to ineffectiveness of story comprehension. In this paper, we propose to construct a story coherence based neural network model (SCNN) with well-designed optimizations. The preliminary evaluation demonstrates the effectiveness of our model which is superior to that of state-of-the-art approaches.	A Story Coherence based Neural Network Model for Predicting Story Ending	NA:NA:NA:NA:NA:NA	2018
Min Gui:Zhengkun Zhang:Zhenglu Yang:Yanhui Gu:Guandong Xu	Document summarization is an important research issue and has attracted much attention from the academe. The approaches for document summarization can be classified as extractive and abstractive. In this work, we introduce an effective joint framework that integrates extractive and abstractive summarization models, which is much closer to the way human write summaries (first underlining important information). Preliminary experiments on real benchmark dataset demonstrate that our model is competitive with the state-of-the-art methods.	An Effective Joint Framework for Document Summarization	NA:NA:NA:NA:NA	2018
Paul Groth:Amélie Gyrard	The Demo Track is one of the most exciting parts of any Web Conference. It allows researchers and practitioners to demonstrate new systems in an engaging and hands-on manner to the community. The Web has been driven forward by building systems and technology. The demo track is a venue that encourages this sort of important type of result This year the track received 71 submissions of those 30 were accepted for a 42% accept rate. We had a comprehensive review procedure that looked at a number of dimensions including the novelty of the demo, its fit with the conference, its research content, and its potential for audience engagement. We were pleased by the number of submissions that included links to online demonstrations and/or videos. This gave reviewers additional information about how the demo would be presented. Overall, we had 232 reviews across all submissions. Many of the reviews provided not only a their expert judgement but ways in which the submissions could be improved. It is often difficult judging demonstrations as there are multiple factors to be taken into account. We want to thank the entire committee for taking the time to support the track. The resulting set of selected demos reflects the wide-variety of technology and research interests impacting the wide. Demonstrations cover topics such as using data on the web, the integration of the web and the physical world, knowledge graphs, search engines, security and privacy, and dealing with multimedia data. We believe that these demos provide an exciting taste of the future of the Web.	Demo Track Chairs' Welcome & Organization	NA:NA	2018
Ruben Taelman:Miel Vander Sande:Ruben Verborgh	The Linked Open Data cloud is evergrowing and many datasets are frequently being updated. In order to fully exploit the potential of the information that is available in and over historical dataset versions, such as discovering evolution of taxonomies or diseases in biomedical datasets, we need to be able to store and query the different versions of Linked Datasets efficiently. In this demonstration, we introduce OSTRICH, which is an efficient triple store with supported for versioned query evaluation. We demonstrate the capabilities of OSTRICH using a Web-based graphical user interface in which a store can be opened or created. Using this interface, the user is able to query in, between, and over different versions, ingest new versions, and retrieve summarizing statistics.	OSTRICH: Versioned Random-Access Triple Store	NA:NA:NA	2018
Hong-Han Shuai:Yueh-Hsue Li:Chun-Chieh Feng:Wen-Chih Peng	Nowadays, people get used to buying a variety of commodities from e-commerce platforms because of the convenience and easy access to Internet. As a result, the sales on e-commerce platforms have grown exponentially. It is promising to customize the online shops for different users under VR environments since users do not need to waste time of going upstairs or downstairs for finding interesting commodities. Therefore, in this demo paper, we propose a novel four-dimensional shopping mall that offers online group shopping services with the customized shop recommendation, where a group of friends in the proposed four-dimensional shopping mall can teleport to the next shop by one click. We formulate a Sequential group wIllingness OptimizatioN (SION) problem, prove SION is NP-hard, and provide an efficient algorithm called ζ-GSS. The experimental results show that the solution quality of the proposed ζ-GSS is close to the optimal solution, while the execution time only requires 3.3%. Finally, we build the prototype of the four-dimensional shopping mall, which can be demonstrated for users to experience the next-generation online shopping.	Four-Dimensional Shopping Mall: Sequential Group Willingness Optimization under VR Environments	NA:NA:NA:NA	2018
Benjamin Klotz:Raphaël Troncy:Daniel Wilms:Christian Bonnet	In this paper, we use semantic technologies for enriching trajectory data in the automotive industry for offline analysis. We proposed to re-use a combination of existing ontologies and we designed a Vehicle Signal Specification ontology to provide an environment in which we developed an application that analyzes the variations of signal values and enables to infer the "driving smoothness'' that we represent as additional annotations of semantic trajectories.	Generating Semantic Trajectories Using a Car Signal Ontology	NA:NA:NA:NA	2018
Johannes Doleschal:Nico Höllerich:Wim Martens:Frank Neven	Chisel is a tool for flexible manipulation of CSV-like data, motivated by the recent effort of the World Wide Web Consortium (W3C) towards a recommendation for tabular data and metadata on the Web. In brief, Chisel supports an expressive built-in schema language for CSV-like data, that can handle both tabular and non-tabular data. Furthermore, it supports a simple programming language for transforming tabular and non-tabular CSV-like data. In the demo, we showcase the system for specifying and validating schemas, building transformations, and setting up a pipeline for automatic conversion of "wild" CSV-like data into structured tabular data. We present use cases for Chisel specifically targeted at exemplifying the ease of specifying, modifying, and understanding Sculpt schemas as well as extracting and transforming data.	Chisel: Sculpting Tabular and Non-Tabular Data on the Web	NA:NA:NA:NA	2018
Ram G. Athreya:Axel-Cyrille Ngonga Ngomo:Ricardo Usbeck	In this demo, we introduce the DBpedia chatbot, a knowledge-graph-driven chatbot designed to optimize community interaction. The bot was designed for integration into community software to facilitate the answering of recurrent questions. Four main challenges were addressed when building the chatbot, namely (1) understanding user queries, (2) fetching relevant information based on the queries, (3) tailoring the responses based on the standards of each output platform (i.e. Web, Slack, Facebook) as well as (4) developing subsequent user interactions with the DBpedia chatbot. With this demo, we will showcase our solutions to these four challenges.	Enhancing Community Interactions with Data-Driven Chatbots--The DBpedia Chatbot	NA:NA:NA	2018
Mayank Kejriwal:Daniel Gilley:Pedro Szekely:Jill Crisman	In this demonstration, we present the Text-enabled Humanitarian Operations in Real-time (THOR) framework, which is being prototyped to provide visual and analytical situational awareness to humanitarian and disaster relief (HADR) planners. THOR is a collaborative effort between industrial and university research laboratories, designed with an intent to support both military and civilian HADR operations. At its core, THOR is powered by a domain-specific knowledge graph, which is derived from natural language outputs and is amenable to real-time analytics. THOR is designed to operate in low-resource linguistic environments, process heterogeneous data, including news and social media, reason about arbitrary disasters not knowable in advance, and provide advanced graphical interaction capabilities. We will demo the latest prototype of THOR using an interactive case study situation.	THOR: Text-enabled Analytics for Humanitarian Operations	NA:NA:NA:NA	2018
Mehdi Bahrami:Junhee Park:Lei Liu:Wei-Peng Chen	Application Programming Interface (API) exposes data and functions of a software application to third-party users. In digital business, API economy is one of the key component for determining the value of provided services. With the rise in number of publicly available APIs, understanding each API endpoint manually is not only labor intensive but it is also an error prone task for software engineers. Due to the complexity of understanding the sheer number of APIs, it is difficult for software developers to find the best possible API combinations (i.e. API Mashups). In this demonstration, we introduce API Learning platform which employs machine-learning based technologies to efficiently search APIs, validate APIs, and generate API mashups. These technologies enable a machine to automatically generate machine-readable API specification from API documentations, understand variety of APIs, validate extracted information through automated API validation, and finally recommend API mashups for a specific purpose. As of now, API Learning platform collected over 14,000 API documentations and generates a machine readable format for REST APIs with an accuracy of 84%. The proposed demo prototype shows how it enables users to quickly find relevant APIs, automatically verify API availability, and get the best possible API mashup recommendations.	API Learning: Applying Machine Learning to Manage the Rise of API Economy	NA:NA:NA:NA	2018
Kashyap Popat:Subhabrata Mukherjee:Jannik Strötgen:Gerhard Weikum	Rapid increase of misinformation online has emerged as one of the biggest challenges in this post-truth era. This has given rise to many fact-checking websites that manually assess doubtful claims. However, the speed and scale at which misinformation spreads in online media inherently limits manual verification. Hence, the problem of automatic credibility assessment has attracted great attention. In this work, we present CredEye, a system for automatic credibility assessment. It takes a natural language claim as input from the user and automatically analyzes its credibility by considering relevant articles from the Web. Our system captures joint interaction between language style of articles, their stance towards a claim and the trustworthiness of the sources. In addition, extraction of supporting evidence in the form of enriched snippets makes the verdicts of CredEye transparent and interpretable.	CredEye: A Credibility Lens for Analyzing and Explaining Misinformation	NA:NA:NA:NA	2018
Arseny Kurnikov:Klaudia Krawiecka:Andrew Paverd:Mohammad Mannan:N. Asokan	Although passwords are by far the most widely-used user authentication mechanism on the web, their security is threatened by password phishing and password database breaches. SafeKeeper is a system for protecting web passwords against very strong adversaries, including sophisticated phishers and compromised servers. Compared to other approaches, one of the key differentiating aspects of SafeKeeper is that it provides web users with verifiable assurance that their passwords are being protected. In this paper, we demonstrate precisely how SafeKeeper can be used to protect web passwords in real-world systems. We first explain two important deployability aspects: i) how SafeKeeper can be integrated into the popular WordPress platform, and ii) how ordinary web users can use Intel SGX remote attestation to verify that SafeKeeper is running on a particular server. We then describe three demonstrations to illustrate the use of SafeKeeper: i) showing the user experience when visiting a legitimate website; ii) showing the encryption of the password in transit via live packet-capture; and iii) showing how SafeKeeper performs in the presence of phishing.	Using SafeKeeper to Protect Web Passwords	NA:NA:NA:NA:NA	2018
Welderufael B. Tesfay:Peter Hofmann:Toru Nakamura:Shinsaku Kiyomoto:Jetzabel Serna	With the continuing growth of the Internet landscape, users share large amount of personal, sometimes, privacy sensitive data. When doing so, often, users have little or no clear knowledge about what service providers do with the trails of personal data they leave on the Internet. While regulations impose rather strict requirements that service providers should abide by, the defacto approach seems to be communicating data processing practices through privacy policies. However, privacy policies are long and complex for users to read and understand, thus failing their mere objective of informing users about the promised data processing behaviors of service providers. To address this pertinent issue, we propose a machine learning based approach to summarize the rather long privacy policy into short and condensed notes following a risk-based approach and using the European Union (EU) General Data Protection Regulation (GDPR) aspects as assessment criteria. The results are promising and indicate that our tool can summarize lengthy privacy policies in a short period of time, thus supporting users to take informed decisions regarding their information disclosure behaviors.	I Read but Don't Agree: Privacy Policy Benchmarking using Machine Learning and the EU GDPR	NA:NA:NA:NA:NA	2018
Alo Allik:Florian Thalmann:Mark Sandler	MusicLynx is a web application for music discovery that enables users to explore an artist similarity graph constructed by linking together various open public data sources. It provides a multifaceted browsing platform that strives for an alternative, graph-based representation of artist connections to the grid-like conventions of traditional recommendation systems. Bipartite graph filtering of the Linked Data cloud, content-based music information retrieval, machine learning on crowd-sourced information and Semantic Web technologies are combined to analyze existing and create new categories of music artists through which they are connected. The categories can uncover similarities between artists who otherwise may not be immediately associated: for example, they may share ethnic background or nationality, common musical style or be signed to the same record label, come from the same geographic origin, share a fate or an affliction, or have made similar lifestyle choices. They may also prefer similar musical keys, instrumentation, rhythmic attributes, or even moods their music evokes. This demonstration is primarily meant to showcase the graph-based artist discovery interface of MusicLynx: how artists are connected through various categories, how the different graph filtering methods affect the topology and geometry of linked artists graphs, and ways in which users can connect to external services for additional content and information about objects of their interest.	MusicLynx: Exploring Music Through Artist Similarity Graphs	NA:NA:NA	2018
Yihong Zhang:Panote Siriaraya:Yuanyuan Wang:Shoko Wakamiya:Yukiko Kawai:Adam Jatowt	For a traveler to enjoy a trip in a city, one important factor is the diversity of sceneries and facilities along the route. Current navigation systems can provide the shortest route between two points, as well as scenic or safe routes. However, diversity is largely ignored in existing works. In this paper, we present a system that provides diversity-based route recommendation. It measures visual-based diversity and facility-based diversity with information extracted from publicly available data such as Google Street View images and FourSquare venues. As we will show, the current prototype system is able to provide diversity-based route recommendation for city areas in San Fransisco and Kyoto.	Walking down a Different Path: Route Recommendation based on Visual and Facility based Diversity	NA:NA:NA:NA:NA:NA	2018
Quyu Kong:Marian-Andrei Rizoiu:Siqi Wu:Lexing Xie	What makes content go viral Which videos become popular and why others don't Such questions have elicited significant attention from both researchers and industry, particularly in the context of online media. A range of models have been recently proposed to explain and predict popularity; however, there is a short supply of practical tools, accessible for regular users, that leverage these theoretical results. HIPie--an interactive visualization system--is created to fill this gap, by enabling users to reason about the virality and the popularity of online videos. It retrieves the metadata and the past popularity series of Youtube videos, it employs the Hawkes Intensity Process, a state-of-the-art online popularity model for explaining and predicting video popularity, and it presents videos comparatively in a series of interactive plots. This system will help both content consumers and content producers in a range of data-driven inquiries, such as to comparatively analyze videos and channels, to explain and to predict future popularity, to identify viral videos, and to estimate responses to online promotion.	Will This Video Go Viral: Explaining and Predicting the Popularity of Youtube Videos	NA:NA:NA:NA	2018
Michel Buffa:Jerome Lebrun	The ANR project WASABI will last 42 months and consists in developing a 2 million songs database with interactive WebAudio enhanced client applications. Client applications target composers, music schools, sound engineering schools, musicologists, music streaming services and journalists. In this paper, we present a virtual pedal board (a set of chainable audio effects on the form of "pedals"), and a guitar tube amplifier simulation for guitarists, that will be associated with songs from the WASABI database. Music schools and music engineering schools are interested in such tools that can be run in a Web page, without the need to install any further software. Take a classic rock song: isolate the guitar solo, study it, then mute it and play guitar real-time along the other tracks using an online guitar amplifier that reproduces the real guitar amp model used in the song, with its signature sound, proper dynamic and frequency response. Add some audio effects such as a reverberation, a delay, a flanger, etc. in order to reproduce Pink Floyd's guitar sound or Eddie Van Halen famous "Brown Sound". Learn interactively, guitar in hands, how to fine tune a compressor effect, or how to shape the sound of a tube guitar amp, how to get a "modern metal" or a "Jimi Hendrix" sound, using only your Web browser.	Real-Time Emulation of a Marshall JCM 800 Guitar Tube Amplifier, Audio FX Pedals, in a Virtual Pedal Board	NA:NA	2018
Giulio Rossetti:Letizia Milli:Salvatore Rinzivillo	Nowadays the analysis of dynamics of and on networks represents a hot topic in the Social Network Analysis playground. To support students, teachers, developers and researchers we introduced a novel framework, named NDlib, an environment designed to describe diffusion simulations. NDlib is designed to be a multi-level ecosystem that can be fruitfully used by different user segments. Upon NDlib, we designed a simulation server that allows remote execution of experiments as well as an online visualization tool that abstracts its programmatic interface and makes available the simulation platform to non-technicians.	NDlib: A Python Library to Model and Analyze Diffusion Processes over Complex Networks	NA:NA:NA	2018
Angela Bonifati:Wim Martens:Thomas Timm	In this demonstration, we showcase DARQL, the first tool for deep, large-scale analysis of SPARQL queries. We have harvested a large corpus of query logs with different lineage and sizes, from DBPedia to BioPortal and Wikidata, whose total number of queries amounts to 180M. We ran a wide range of analyses on the corpus, spanning from simple tasks (keyword counts, triple counts, operator distributions), moderately deep tasks (projection test, query classification), and deep analysis (shape analysis, well-designedness, weakly well-designedness, hypertreewidth, and fractional edge cover). The key goal of our demonstration is to let the users dive into the SPARQL query logs of our corpus and let them discover the inherent characteristics of the queries. The entire corpus of SPARQL queries is stored in a DBMS. The tool has a GUI that allows users to ask sophisticated analytical queries on the SPARQL logs. These analytical queries can both be directly written in SQL or composed by a visual query builder tool. The results of the analytical queries are represented both textually (as SPARQL queries) and visually. The DBMS performs the searches within the corpus quite efficiently. To the best of our knowledge, this is the first demonstration of this kind on such a large corpus and with such a number of varied tests.	DARQL: Deep Analysis of SPARQL Queries	NA:NA:NA	2018
Sepideh Mesbah:Alessandro Bozzon:Christoph Lofi:Geert-Jan Houben	This demo presents SmartPub, a novel web-based platform that supports the exploration and visualization of shallow meta-data (e.g., author list, keywords) and deep meta-data--long tail named entities which are rare, and often relevant only in specific knowledge domain--from scientific publications. The platform collects documents from different sources (e.g. DBLP and Arxiv), and extracts the domain-specific named entities from the text of the publications using Named Entity Recognizers (NERs) which we can train with minimal human supervision even for rare entity types. The platform further enables the interaction with the Crowd for filtering purposes or training data generation, and provides extended visualization and exploration capabilities. SmartPub will be demonstrated using sample collection of scientific publications focusing on the computer science domain and will address the entity types Dataset (i.e. dataset presented or used in a publication), and Methods (i.e. algorithms used to create/enrich/analyse a data set)	SmartPub: A Platform for Long-Tail Entity Extraction from Scientific Publications	NA:NA:NA:NA	2018
Andrea Mauri:Achilleas Psyllidis:Alessandro Bozzon	Having a thorough understanding of energy consumption behavior is an important element of sustainability studies. Traditional sources of information about energy consumption, such as smart meter devices and surveys, can be costly to deploy, may lack contextual information or have infrequent updates. In this paper, we examine the possibility of extracting energy consumption-related information from user-generated content. More specifically, we develop a pipeline that helps identify energy-related content in Twitter posts and classify it into four categories (dwelling, food, leisure, and mobility), according to the type of activity performed. We further demonstrate a web-based application--called Social Smart Meter--that implements the proposed pipeline and enables different stakeholders to gain an insight into daily energy consumption behavior, as well as showcase it in case studies involving several world cities.	Social Smart Meter: Identifying Energy Consumption Behavior in User-Generated Content	NA:NA:NA	2018
Freya Behrens:Sebastian Bischoff:Pius Ladenburger:Julius Rückin:Laurenz Seidel:Fabian Stolp:Michael Vaichenker:Adrian Ziegler:Davide Mottin:Fatemeh Aghaei:Emmanuel Müller:Martin Preusse:Nikola Müller:Michael Hunger	We present MetaExp, a system that assists the user during the exploration of large knowledge graphs, given two sets of initial nodes. At its core, MetaExp presents a small set of meta-paths to the user, which are sequences of relationships among node types. Such meta-paths do not overwhelm the user with complex structures, yet they preserve semantically-rich relationships in a graph. MetaExp engages the user in an interactive procedure, which involves simple meta-paths evaluations to infer a user-specific similarity measure. This similarity measure incorporates the domain knowledge and the preferences of the user, overcoming the fundamental limitations of previous methods based on local node neighborhoods or statically determined similarity scores. Our system provides a user-friendly interface for searching initial nodes and guides the user towards progressive refinements of the meta-paths. The system is demonstrated on three datasets, Freebase, a movie database, and a biological network.	MetaExp: Interactive Explanation and Exploration of Large Knowledge Graphs	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Rudolf Schneider:Sebastian Arnold:Tom Oberhauser:Tobias Klatt:Thomas Steffek:Alexander Löser	We demonstrate Smart-MD, an information retrieval system for medical professionals. The system supports topical queries in the form [disease topic], such as ["lyme disease", treatments]. In contrast to document-oriented retrieval systems, Smart-MD retrieves relevant paragraphs and reduces the reading load of a medical doctor drastically. We recognize diseases and topical aspects with a novel paragraph retrieval method based on bidirectional LSTM neural networks. We demonstrate Smart-MD on a dataset that contains 3,469 diseases from the English language part of Wikipedia and 6,876 distinct medical aspects extracted from Wikipedia headlines.	Smart-MD: Neural Paragraph Retrieval of Medical Topics	NA:NA:NA:NA:NA:NA	2018
Yanyan Wang:Qun Chen:Xin Liu:Murtadha Ahmed:Zhanhuai Li:Wei Pan:Hailong Liu	The state-of-the-art techniques for aspect-level sentiment analysis focus on feature modeling using a variety of deep neural networks (DNN). Unfortunately, their practical performance may fall short of expectations due to semantic complexity of natural languages. Motivated by the observation that linguistic hints (e.g. explicit sentiment words and shift words) can be strong indicators of sentiment, we present a joint framework, SenHint, which integrates the output of deep neural networks and the implication of linguistic hints into a coherent reasoning model based on Markov Logic Network (MLN). In SenHint, linguistic hints are used in two ways: (1) to identify easy instances, whose sentiment can be automatically determined by machine with high accuracy; (2) to capture implicit relations between aspect polarities. We also empirically evaluate the performance of SenHint on both English and Chinese benchmark datasets. Our experimental results show that SenHint can effectively improve accuracy compared with the state-of-the-art alternatives.	SenHint: A Joint Framework for Aspect-level Sentiment Analysis by Deep Neural Networks and Linguistic Hints	NA:NA:NA:NA:NA:NA:NA	2018
Michele Ruta:Floriano Scioscia:Giuseppe Loseto:Filippo Gramegna:Saverio Ieva:Agnese Pinto:Eugenio Di Sciascio	ThePhysical Semantic Web (PSW) is a novel paradigm built upon the Google Physical Web (PW) approach and devoted to improve the quality of interactions in the Web of Things. Beacons expose semantic annotations instead of basic identifiers, ıe\ machine-understandable descriptions of physical resources. This enables novel ontology-based object advertisement and discovery and --in turn-- advanced user-to-thing and autonomous thing-to-thing interactions. The demo shows the evolution from the PW to the PSW in a discovery scenario set in a winery, where bottles are equipped with Bluetooth Low Energy beacons and a customer can discover them using her smartphone. The final goal is to prove benefits of PSW over basic PW, including: rich semantic-based object annotation; dynamic annotations exploiting on-board sensors; enhanced discovery and ranking of nearby objects through semantic matchmaking; availability of interactions even without working Internet infrastructure, by means of point-to-point data exchanges.	A journey from the Physical Web to the Physical Semantic Web	NA:NA:NA:NA:NA:NA:NA	2018
Mohamed Abdel Maksoud:Gaurav Pandey:Shuaiqiang Wang	This paper addresses a novel tour discovery problem in the domain of travel search. We create a ranking of tours for a set of travel interests, where a tour is a group of city documents and a travel interest is a query. While generating and ranking tours, it is aimed that each interest (from the interest set) is satisfied by at least one city in a tour and the distance traveled to cover the tour is not too large. Firstly, we generate tours for the interest set, by utilizing the available ranking of cities for the individual interests and the distances between the cities. Then, in absence of existing methods directly related to our problem, we devise our novel techniques to calculate ranking scores for the tours and present a comparison of these techniques in our results. We demonstrate our web application Travición, that utilizes the best tour scoring technique.	Finding Tours for a Set of Interests	NA:NA:NA	2018
Mirko Marras:Matteo Manca:Ludovico Boratto:Gianni Fenu:David Laniado	The massive amount and variety of city-related data raise equally big challenges to enable citizens to make sense of such data for improving their daily life and fostering collective decision making. The existing dashboards include limited pre-defined use cases which can only address the most common needs of citizens, but do not allow for personalization. In this work, we propose an open source environment that enables citizens to easily explore city-related data. A backend continuously collects heterogeneous data, turns them into a unified format and exposes them through an API; a front-end allows users to create interactive visualizations combining different data sources and visual models, and to share them to engage others. In this way, citizens can build up a data-driven public awareness supporting an open, transparent, and collaborative city.	BarcelonaNow: Empowering Citizens with Interactive Dashboards for Urban Data Exploration	NA:NA:NA:NA:NA	2018
Giorgos Argyriou:George Papadakis:George Stamoulis:Efi Karra Taniskidou:Nikiforos Pittaras:George Giannakopoulos:Sergio Albani:Michele Lazzarini:Emanuele Angiuli:Anca Popescu:Argyros Argyridis:Manolis Koubarakis	GeoSensor is a novel system that enriches change detection over satellite images with event detection over news items and social media content. GeoSensor faces the major challenges of Big Data: volume (a single satellite image may be a few GBs), variety (its data sources include two different types of satellite images and various types of user-generated content) and veracity, as the accuracy of the end result is crucial for the usefulness of our system. To overcome these three challenges, while offering on-line functionality, GeoSensor comprises a complex architecture that is based on the open-source platform developed in the H2020 project Big Data Europe. Through the presented demonstration, both the effectiveness and the efficiency of GeoSensor's functionalities are highlighted.	GeoSensor: On-line Scalable Change and Event Detection over Big Data	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Weijian Zhang:Jonathan Deakin:Nicholas J. Higham:Shuaiqiang Wang	We present Etymo (https://etymo.io), a discovery engine to facilitate artificial intelligence (AI) research and development. It aims to help readers navigate a large number of AI-related papers published every week by using a novel form of search that finds relevant papers and displays related papers in a graphical interface. Etymo constructs and maintains an adaptive similarity-based network of research papers as an all-purpose knowledge graph for ranking, recommendation, and visualisation. The network is constantly evolving and can learn from user feedback to adjust itself. A screencast is available at: https://youtu.be/T4FDPk_TmN0	Etymo: A New Discovery Engine for AI Research	NA:NA:NA:NA	2018
Tanya Chowdhury:Aashay Mittal:Tanmoy Chakraborty	In this demo, we present VIZ-Wiki, a browser extension which generates an overview of summarizable threads in Question Answering forums. It reduces a user's effort to go through lengthy text-based, sarcastic and highly critiqued answers. Our tool can be used to collect community opinion from popular discussion sites like Quora, Yahoo! Answers, Reddit etc. as well as topic-centric ones such as Askubuntu, Stackoverflow. We rely on textual information of these forums to extract insightful summaries for a reader. VIZ-Wiki provides users a pie-graph view marking popular choices when such a question link is raised. A button guides them to detailed statistics and relevant list of answers. It further highlights sentences relevant to an answer choice in the text. VIZ-Wiki deals with answers contradicted by other users, prioritizes highly-recommended ones and avoids sarcasm. We test our model on the factoid questions dataset of Yahoo! Answers and obtain a macro precision of 0.6 on displayed answers and a macro recall of 0.69, beating the baseline significantly. To the best of our knowledge, VIZ-Wiki is the first attempt to visualize answers for questions in community question answering services. In the spirit of reproducibility, we have released the code and a demonstration video public at \urlhttp://goo.gl/cyx3EF and \urlhttps://youtu.be/XNmRa_jtmC8 respectively	VIZ-Wiki: Generating Visual Summaries to Factoid Threads in Community Question Answering Services	NA:NA:NA	2018
Benjamin D. Horne:William Dron:Sara Khedr:Sibel Adali	Today, journalist, information analyst, and everyday news consumers are tasked with discerning and fact-checking the news. This task has became complex due to the ever-growing number of news sources and the mixed tactics of maliciously false sources. To mitigate these problems, we introduce the The News Landscape (NELA) Toolkit: an open source toolkit for the systematic exploration of the news landscape. NELA allows users to explore the credibility of news articles using well-studied content-based markers of reliability and bias, as well as, filter and sort through article predictions based on the user's own needs. In addition, NELA allows users to visualize the media landscape at different time slices using a variety of features computed at the source level. NELA is built with a modular, pipeline design, to allow researchers to add new tools to the toolkit with ease. Our demo is an early transition of automated news credibility research to assist human fact-checking efforts and increase the understanding of the news ecosystem as a whole.	Assessing the News Landscape: A Multi-Module Toolkit for Evaluating the Credibility of News	NA:NA:NA:NA	2018
Debabrata Mahapatra:Ragunathan Mariappan:Vaibhav Rajan:Kuldeep Yadav:Seby A.:Sudeshna Roy	The number of high quality online videos is increasing rapidly. Online courses as well as universities do not fully leverage the content due to several open challenges in video search, indexing, summarization and customization requirements for specific courses, instructors or learners. We present a new web-based social learning platform called Videoken. Using novel video summarization algorithms, Videoken automatically creates Table of Contents for videos. This allows a textbook-like facility for non-linear search and navigation through the video, enables extraction of semantically coherent clips from within a video and improves video search through better semantic indexing. The platform also allows new ways of course creation and sharing of learning modules; and can be both integrated with existing Learning Management Systems and used independently.	VideoKen: Automatic Video Summarization and Course Curation to Support Learning	NA:NA:NA:NA:NA:NA	2018
Harshita Jhavar:Paramita Mirza	We present EMOFIEL, a system that identifies characters and scenes in a story from a fictional narrative summary, generates appropriate scene descriptions, identifies the emotion flow between a given directed pair of story characters in each interaction, and organizes them along the story timeline. These emotions are identified using two emotion modelling approaches: categorical and dimensional emotion models. The generated plots show that in a particular scene, two characters can share multiple emotions together with different intensity. Furthermore, the directionality of the emotion can be captured as well, depending on which character is more dominant in each interaction. EMOFIEL provides a web-based GUI that allows users to query the annotated stories to explore the emotion mapping of a given character pair throughout a given story, and to explore scenes for which a certain emotion peaks.	EMOFIEL: Mapping Emotions of Relationships in a Story	NA:NA	2018
Miltiadis Lytras:Naif Radi Aljohani:Amir Hussain:Jiebo Luo:Jacky Xi Zhang	Cognitive Computing has received increasing attention from academia and industries as it brings cognitive science and computing together for the development of new computational platforms, infrastructures, systems and algorithms. Artificial intelligence and computational intelligence are key elements to succeed in cognitive computing. The Cognitive Computing track was successfully organized at WWW2017. This year, WWW2018 continues this track with a focus on all applications. Particularly, applications in healthcare, environment, education, sustainability, smart cities and food science are among the important global challenges in 21st Century.	Cognitive Computing Track Chairs' Welcome & Organization	NA:NA:NA:NA:NA	2018
Iqra Safder:Saeed-Ul Hassan:Naif Radi Aljohani	Although, over the years, information retrieval systems have shown tremendous improvements in searching for relevant scientific literature, human cognition is still required to search for specific document elements in full text publications. For instance, pseudocodes pertaining to algorithms published in scientific publications cannot be correctly matched against user queries, hence the process requires human involvement. AlgorithmSeer, a state-of-the-art technique, claims to replace humans in this task, but one of the limitations of such an algorithm search engine is that the metadata is simply a textual description of each pseudocode, without any algorithm-specific information. Hence, the search is performed merely by matching the user query to the textual metadata and ranking the results using conventional textual similarity techniques. The ability to automatically identify algorithm-specific metadata such as precision, recall, or f-measure would be useful when searching for algorithms. In this article, we propose a set of algorithms to extract further information pertaining to the performance of each algorithm. Specifically, sentences in an article that convey information about the efficiency of the corresponding algorithm are identified and extracted using a recurrent convolutional neural network (RCNN). Furthermore, we propose improving the efficacy of the pseudocode detection task by using a multi-layer perceptron (MLP) classification trained with 15 features, which improves the classification performance of the state-of-the-art pseudocode detection methods used in AlgorithmSeer by 27%. Finally, we show the advantages of the AI-enabled search engine (based on RCNN and MLP models) over conventional text-retrieval models.	AI Cognition in Searching for Relevant Knowledge from Scholarly Big Data, Using a Multi-layer Perceptron and Recurrent Convolutional Neural Network Model	NA:NA:NA	2018
Ali Daud:Akbar Hussain:Rabeeh Ayaz Abbasi:Naif Radi Aljohani:Tehmina Amjad:Hassan Dawood	Players are ranked in various sports to show their importance over other players. Existing methods only consider intra-type links (e.g., player to player and team to team), but ignore inter-type links (e.g., one type of player to another type of player, such as batsman to bowler and player to team) based on cognitive aspects. They also ignore the spatiality of the players. There is a strong relationship among players and their teams, which can be represented as a network consisting of multi-type interrelated objects. In this paper, we propose a players' ranking method, called Region-wise Players Link Fusion (RPLF) which is applied to the sport of cricket. RPLF considers players' region-wise intra-type and inter-type relation-based features to rank the players. Considering multi-type interrelated objects is based on the intuition that a batsman scoring high against top bowlers of a strong team or a bowler taking wickets against top batsmen of a strong team is considered as a good player. The experimental results show that RPLF provides promising insights of players' rankings. RLFP is a generic method and can be applied to different sports for ranking players.	Region-wise Ranking of Sports Players based on Link Fusion	NA:NA:NA:NA:NA:NA	2018
Debabrata Mahapatra:Ragunathan Mariappan:Vaibhav Rajan	The number of freely available online educational videos from universities and other organizations is growing rapidly. Accurate indexing and summarization are essential for efficient search, recommendation and effective consumption of videos. In this paper, we describe a new method of automatically creating a hierarchical table of contents for a video. It provides a summary of the video content along with a textbook--like facility for nonlinear navigation and search through the video. Our multimodal approach combines new methods for shot level video segmentation and for hierarchical summarization. Empirical results demonstrate the efficacy of our approach on many educational videos.	Automatic Hierarchical Table of Contents Generation for Educational Videos	NA:NA:NA	2018
Kwan Hui Lim:Kate E. Lee:Dave Kendal:Lida Rashidi:Elham Naghizade:Stephan Winter:Maria Vasardani	Green spaces are believed to improve the well-being of users in urban areas. While there are urban research exploring the emotional benefits of green spaces, these works are based on user surveys and case studies, which are typically small in scale, intrusive, time-intensive and costly. In contrast to earlier works, we utilize a non-intrusive methodology to understand green space effects at large-scale and in greater detail, via digital traces left by Twitter users. Using this methodology, we perform an empirical study on the effects of green spaces on user sentiments and emotions in Melbourne, Australia and our main findings are: (i) tweets in green spaces evoke more positive and less negative emotions, compared to those in urban areas; (ii) each season affects various emotion types differently; (iii) there are interesting changes in sentiments based on the hour, day and month that a tweet was posted; and (iv) negative sentiments are typically associated with large transport infrastructures such as train interchanges, major road junctions and railway tracks. The novelty of our study is the combination of psychological theory, alongside data collection and analysis techniques on a large-scale Twitter dataset, which overcomes the limitations of traditional methods in urban research.	The Grass is Greener on the Other Side: Understanding the Effects of Green Spaces on Twitter User Sentiments	NA:NA:NA:NA:NA:NA:NA	2018
Lin Mu:Peiquan Jin:Lizhou Zheng:En-Hong Chen:Lihua Yue	Microblog like Twitter and Sina Weibo has been an important information source for event detection and monitoring. In many decision-making scenarios, it is not enough to only provide a structural tuple for an event, e.g., a 5W1H record like <who, where, when, what, whom, how>. However, in addition to event structural tuples, people need to know the evolution lifecycle of an event. The lifecycle description of an event is more helpful for decision making because people can focus on the progress and trend of events. In this paper, we propose a novel method for efficiently detecting and tracking event evolution on microblogging platforms. The major features of our study are: (1) It provides a novel event-type-driven method to extract event tuples, which forms the foundation for event evolution analysis. (2) It describes the lifecycle of an event by a staged model, and provides effective algorithms for detecting the stages of an event. (3) It offers emotional analysis over the stages of an event, through which people are able to know the public emotional tendency over a specific event at different time periods. We build a prototype system and present its architecture and implemental details in the paper. In addition, we conduct experiments on real microblog datasets and the results in terms of precision, recall, and F-measure suggest the effectiveness and efficiency of our proposal.	Lifecycle-Based Event Detection from Microblogs	NA:NA:NA:NA:NA	2018
Jiongqian Liang:Peter Jacobs:Srinivasan Parthasarathy	Hurricane-induced flooding can lead to substantial loss of life and huge damage to infrastructure. Mapping flood extent from satellite or aerial imagery is essential for prioritizing relief efforts and for assessing future flood risk. Identification of water extent in such images can be challenging considering the heterogeneity in water body size and shape, cloud cover, and natural variations in land cover. In this effort, we introduce a novel cognitive framework based on a semi-supervised learning algorithm, called HUman-Guided Flood Mapping (HUG-FM), specifically designed to tackle the flood mapping problem. Our framework first divides the satellite or aerial image into patches leveraging a graph-based clustering approach. A domain expert is then asked to provide labels for a few patches (as opposed to pixels which are harder to discern). Subsequently, we learn a classifier based on the provided labels to map flood extent. We test the efficacy and efficiency of our framework on imagery from several recent flood-induced emergencies and results show that our algorithm can robustly and correctly detect water areas compared to the state-of-the-art. We then evaluate whether expert guidance can be replaced by the wisdom of a crowd (e.g., crisis volunteers). We design an online crowdsourcing platform based on HUG-FM and propose a novel ensemble method to leverage crowdsourcing efforts. We conduct an experiment with over $50$ participants and show that crowdsourced HUG-FM (CHUG-FM) can approach or even exceed the performance of a single expert providing guidance (HUG-FM).	Human-Guided Flood Mapping: From Experts to the Crowd	NA:NA:NA	2018
Patrick Watson:TengFei Ma:Ravi Tejwani:Maria Chang:JaeWook Ahn:Sharad Sundararajan	The availability of open educational resources (OER) has enabled educators and researchers to access a variety of learning assessments online. OER communities are particularly useful for gathering multiple choice questions (MCQs), which are easy to grade, but difficult to design well. To account for this, OERs often rely on crowd-sourced data to validate the quality of MCQs. However, because crowds contain many non-experts, and are susceptible to question framing effects, they may produce ratings driven by guessing on the basis of surface-level linguistic features, rather than deep topic knowledge. Consumers of OER multiple choice questions (and authors of original multiple choice questions) would benefit from a tool that automatically provided feedback on assessment quality, and assessed the degree to which OER MCQs are susceptible to framing effects. This paper describes a model that is trained to use domain-naive strategies to guess which multiple choice answer is correct. The extent to which this model can predict the correct answer to an MCQ is an indicator that the MCQ is a poor measure of domain-specific knowledge. We describe an integration of this model with a front-end visualizer and MCQ authoring tool.	Human-level Multiple Choice Question Guessing Without Domain Knowledge: Machine-Learning of Framing Effects	NA:NA:NA:NA:NA:NA	2018
Rui Yan:Dongyan Zhao	To establish an automatic conversation system between human and computer is regarded as one of the most hardcore problems in computer science. It requires interdisciplinary techniques of information retrieval, natural language processing, data management as well as artificial intelligence. The arrival of big data era reveals the feasibility to create a conversation system empowered by data-driven approaches. Now we are able to collect extremely large conversational data on Web, and organize them to launch a human-computer conversation system. Owing to the diversity of Web resources available, a retrieval-based conversation system will be able to find at least some responses from the massive data repository for any user inputs. Given a human issued utterance, i.e., a query, a retrieval-based conversation system will search for appropriate replies, conduct a relevance ranking, and then output the highly relevant one as the response. In this paper, we propose a novel retrieval model named NeuRetrieval for short text understanding, representation and semantic matching. The proposed model is general and unified for both single-turn and multi-turn conversation scenarios in open domain. In the experiments, we investigate the effectiveness of the proposed deep neural network model for human-computer conversations. We demonstrate performance improvement against a series of baseline methods in several evaluation metrics. In contrast with previously proposed methods, NeuRetrieval is tailored for conversation scenarios and demonstrated to be more effective.	A NeuRetrieval Model for Human-Computer Conversations	NA:NA	2018
Yihang Cheng:Xi Zhang:Hao Wang:Shang Jiang	Community Question Answering (CQA) has emerged recently and it becomes popular among people. During the process of the communication, different knowledge can be merged. Recently, several vendors use the business model of paying for knowledge to make these knowledge to the monetary benefits, then the Pay-for-Knowledge Communities (PKC) have been applied. Even PKC has interesting business model, there are several problems to be solved, and one of the most salient problem is that questioners may takes too long time to choose the most valuable answers, leading to questioners not able to pay for suitable answerers and many problems about platforms' operation There are several previous research has focused on this problem but still have not found satisfactory solutions as questions and answers are more and more complex in PKC platforms. With the development of cognitive computing techniques, applying an intelligent QA system in PKC to improve the answering effectiveness may be possible. In this paper, we tried to investigate how to apply the intelligent QA system into PKC platform to improve the answering effectiveness. For solving the problems of matching complex questions and answers, we present a Four Module QA Model based on the normal intelligent QA System. Compared to normal intelligent QA System, our model uses categories to classify the questions with traditional machine learning methods. We use answers in each category of corresponding questions as one dataset, answers in each entity of corresponding question as the other dataset, finally, these two datasets make up the document database. Then we got the best answer among past answers through comparing the TF-IDF weighted bag-of word vectors of two datasets or the new answer including key words through Long Short-Term Memory (LSTM) algorithm with PKC's features composed of centrality and money. Experiments were developed on a dataset with 1222 users' QA sites collected from a QA community. The model we proposed is expected to increase QA's effectiveness and improve the business model of Pay-for-Knowledge Communities.	How to Improve the Answering Effectiveness in Pay-for-Knowledge Community: An Exploratory Application of Intelligent QA System	NA:NA:NA:NA	2018
Sylvio Barbon Junior:Gabriel Marques Tavares:Victor G. Turrisi da Costa:Paolo Ceravolo:Ernesto Damiani	One of the main challenges of Cognitive Computing (CC) is reacting to evolving environments in near-real time. Therefore, it is expected that CC models provide solutions by examining a summary of past history, rather than using full historical data. This strategy has significant benefits in terms of response time and space complexity but poses new challenges in term of concept-drift detection, where both long term and short terms dynamics should be taken into account. In this paper, we introduce the Concept-Drift in Event Stream Framework (CDESF) that addresses some of these challenges for data streams recording the execution of a Web-based business process. Thanks to CDESF support for feature transformation, we perform density clustering in the transformed feature space of the process event stream, observe track concept-drift over time and identify anomalous cases in the form of outliers. We validate our approach using logs of an e-healthcare process.	A Framework for Human-in-the-loop Monitoring of Concept-drift Detection in Event Log Stream	NA:NA:NA:NA:NA	2018
Peijun Zhao:Jia Jia:Yongsheng An:Jie Liang:Lexing Xie:Jiebo Luo	Emojis can be regarded as a language for graphical expression of emotions, and have been widely used in social media. They can express more delicate feelings beyond textual information and improve the effectiveness of computer-mediated communication. Recent advances in machine learning make it possible to automatic compose text messages with emojis. However, the usages of emojis can be complicated and subtle so that analyzing and predicting emojis is a challenging problem. In this paper, we first construct a benchmark dataset of emojis with tweets and systematically investigate emoji usages in terms of tweet content, tweet structure and user demographics. Inspired by the investigation results, we further propose a multitask multimodality gated recurrent unit (mmGRU) model to predict the categories and positions of emojis. The model leverages not only multimodality information such as text, image and user demographics, but also the strong correlations between emoji categories and their positions. Our experimental results show that the proposed method can significantly improve the accuracy for predicting emojis for tweets (+9.0% in F1-value for category and +4.6% in F1-value for position). Based on the experimental results, we further conduct a series of case studies to unveil how emojis are used in social media.	Analyzing and Predicting Emoji Usages in Social Media	NA:NA:NA:NA:NA:NA	2018
Vitobha Munigala:Abhijit Mishra:Srikanth G. Tamilselvam:Shreya Khare:Riddhiman Dasgupta:Anush Sankaran	Persuasiveness is a creative art which aims at inducing certain set of beliefs in the target audience. In an e-commerce setting, for a newly launched product, persuasive descriptions are often composed to motivate an online buyer towards a successful purchase. Such descriptions can be catchy taglines, product-summaries, style-tipsetc.. In this paper, we present PersuAIDE! - a persuasive system based on linguistic creativity to generate various forms of persuasive sentences from the input product specification. To demonstrate the effectiveness of the proposed system, we have applied the technology to fashion domain, where, for a given fashion product like"red collar shirt" we were able to generate descriptive sentences that not only explain the item but also garner positive attention, making it persuasive. PersuAIDE! identifies fashion related keywords from input specifications and intelligently expands the keywords to creative phrases. Once such compatible phrases are obtained, persuasive descriptions are synthesized from the set of phrases and input keywords with the help of a neural language model trained on a large domain-specific fashion corpus. We evaluate the system on a large fashion corpus collected from different sources using (a) automatic text generation metrics used for Machine Translation and Automatic Summarization evaluation and Readability measurement, and (b) human judgment scores evaluating the persuasiveness and fluency of the generated text. Experimental results and qualitative analysis show that an unsupervised system like ours can produce more creative and better constructed persuasive output than supervised generative counterparts based on neural sequence-to-sequence models and statistical machine translation.	PersuAIDE ! An Adaptive Persuasive Text Generation System for Fashion Domain	NA:NA:NA:NA:NA:NA	2018
Tianlang Chen:Yuxiao Chen:Han Guo:Jiebo Luo	WeChat Business, developed on WeChat, the most extensively used instant messaging platform in China, is a new business model that bursts into people's lives in the e-commerce era. As one of the most typical WeChat Business behaviors, WeChat users can advertise products, advocate companies and share customer feedback to their WeChat friends by posting a WeChat Moment--a public status that contains images and a text. Given its popularity and significance, in this paper, we propose a novel Bilateral-Attention LSTM network (BiATT-LSTM) to identify WeChat Business Moments based on their texts and images. In particular, different from previous schemes that equally consider visual and textual modalities for a joint visual-textual classification task, we start our work with a text classification task based on an LSTM network, then we incorporate a bilateral-attention mechanism that can automatically learn two kinds of explicit attention weights for each word, namely 1) a global weight that is insensitive to the images in the same Moment with the word, and 2) a local weight that is sensitive to the images in the same Moment. In this process, we utilize visual information as a guidance to figure out the local weight of a word in a specific Moment. Two-level experiments demonstrate the effectiveness of our framework. It outperforms other schemes that jointly model visual and textual modalities. We also visualize the bilateral-attention mechanism to illustrate how this mechanism helps joint visual-textual classification.	When E-commerce Meets Social Media: Identifying Business on WeChat Moment Using Bilateral-Attention LSTM	NA:NA:NA:NA	2018
Hogun Park:Hamid Reza Motahari Nezhad	A lot of knowledge about procedures and how-tos are described in text. Recently, extracting semantic relations from the procedural text has been actively explored. Prior work mostly has focused on finding relationships among verb-noun pairs or clustering of extracted pairs. In this paper, we investigate the problem of learning individual procedure-specific relationships (e.g. is method of, is alternative of, or is subtask of) among sentences. To identify the relationships, we propose an end-to-end neural network architecture, which can selectively learn important procedure-specific relationships. Using this approach, we could construct a how-to knowledge base from the largest procedure sharing-community, wiki-how.com. The evaluation of our approach shows that it outperforms the existing entity relationship extraction algorithms.	Learning Procedures from Text: Codifying How-to Procedures in Deep Neural Networks	NA:NA	2018
Zara Mansoor:Mustansar Ali Ghazanfar:Syed Muhammad Anwar:Ahmed S. Alfakeeh:Khaled H. Alyoubi	This research article focuses on the analysis of electroencephalography (EEG) signals of the brain during pain perception. The proposed system is based on the hypothesis that a noticeable change occurs in mental conditions while experiencing pain. When the human body is injured, sensory receptors in the brain enter a stimulated state. The injury may be the result of attention or an accident. Pain warnings are natural in humans and protect the body from further negative effects. In this article, an innovative and robust system based on prominent features extracted from the brain activity recorded using EEG, is proposed to predict the state of pain perception. The brain signals of subjects are observed using two low-cost EEG headsets including neurosky mindwave mobile and emotiv insight. Time and frequency domain features are selected to represent the observed signals. The results show that a combination of time and frequency domain features is the most informative approach for pain prediction using the observed brain activity.	Pain Prediction in Humans using Human Brain Activity Data	NA:NA:NA:NA:NA	2018
Yunhao Zheng:Xi Zhang:Yuting Xiao	Recommending proper experts to knowledge buyers is a significant problem in online paid Q&A community (OPQC). Existing approaches for online expert recommendation have been mainly focused on exploiting semantic similarities and social network influence, while personalizing recommendation according to individuals' motivations has not received much attention. In this paper, we propose a personalized expert recommender system, which integrates buyer's motivation for knowledge, social influence, and money in a unified framework. As an innovative application of cognitive computing, our recommender system is capable of providing users with the best matching experts so as to help them make the most cost-effective choice in OPQC. To this end, Paragraph Vector technique is implemented to construct domain knowledge base (KB) in a multilayer information retrieval (IR) framework. Then we perform knowledge pricing based on buyer's query and bid in the context of bilateral monopoly knowledge market. After that, a Markov Chain based method with user motivation learning is introduced to find the best matching experts. Finally, we evaluate the proposed approach using datasets collected from two OPQC. The experimental results show encouraging success as effectively offering reasonable personalization options. As an innovative approach to solve the expert matching problem in OPQC, this research provides flexibility in customizing the recommendation heuristics based on user motivation, and demonstrate its contribution to a higher rate of optimal knowledge seller-buyer matching.	Making the Most Cost-effective Decision in Online Paid Q&A Community: An Expert Recommender System with Motivation Modeling and Knowledge Pricing	NA:NA:NA	2018
Tehmina Amjad:Ali Daud:Min Song	With the increase in collaboration among researchers of various disciplines, changing the research topic or working on multiple topics is not an unusual behavior. Several comprehensive efforts have been made for predicting, quantifying, and studying the researcher's impact. The question, that how the change in the field of interest over time or working in more than one topics can influence the scientific impact, remains unanswered. In this research, we study the effect of topic drift on the scientific impact of an author. We apply Author Conference Topic (ACT) model to extract topic distribution of individual authors who are working on multiple topics to compare and analyze with authors who work on a single topic. We analyze the productivity of the authors on the basis of publication count, citation count and h-index. We find that authors who stick to one topic, produce a higher impact and gain more attention. To further strengthen our results we gather the h-index of top-ranked authors working on one topic and top-ranked authors working on multiple topics and examine whether there are similar trends in their progress. The results show an evidence of significant impact of topic drift on career choices of researchers.	Measuring the Impact of Topic Drift in Scholarly Networks	NA:NA:NA	2018
Aditya Mogadala:Bhargav Kanuparthi:Achim Rettinger:York Sure-Vetter	Growth of multimodal content on the web and social media has generated abundant weakly aligned image-sentence pairs. However, it is hard to interpret them directly due to intrinsic intension. In this paper, we aim to annotate such image-sentence pairs with connotations as labels to capture the intrinsic intension. We achieve it with a connotation multimodal embedding model (CMEM) using a novel loss function. It's unique characteristics over previous models include: (i) the exploitation of multimodal data as opposed to only visual information, (ii) robustness to outlier labels in a multi-label scenario and (iii) works effectively with large-scale weakly supervised data. With extensive quantitative evaluation, we exhibit the effectiveness of CMEM for detection of multiple labels over other state-of-the-art approaches. Also, we show that in addition to annotation of image-sentence pairs with connotation labels, byproduct of our model inherently supports cross-modal retrieval i.e. image query - sentence retrieval.	Discovering Connotations as Labels for Weakly Supervised Image-Sentence Data	NA:NA:NA:NA	2018
Zhiyuan He:Su Yang:Weishan Zhang:Jiulong Zhang	Different urban regions usually have different commercial hotness due to the different social contexts inside. As satellite imagery promises high-resolution, low-cost, real-time, and ubiquitous data acquisition, this study aims to solve commercial hotness prediction as well as the correlated social contexts mining problem via visual pattern analysis on satellite images. The goal is to reveal the underlying law correlating visual patterns of satellite images with commercial hotness so as to infer the commercial hotness map of a whole city for government regulation and business planning. We propose a novel deep learning-based model, which learns semantic information from raw satellite images to enable predicting regional commercial hotness. First, we collect satellite images from Google Map and label such images with POI categories according to the annotations from OpenStreetMap. Then, we train a model of deep convolutional networks that leverage raw images to infer the social attributes of the region of interest. Finally, we use three classical regression methods to predict regional commercial hotness from the corresponding social contexts reflected in satellite images in Shanghai, where the applied deep features are learned from the examples of Beijing to guarantee the generality. The result shows that the proposed model is robust enough to reach 82% precision at average. To the best of our knowledge, it is the first work focused on discovering relations between commercial hotness and satellite images. A web service is developed to demonstrate how business planning can be done in reference to the predicted commercial hotness of a given region.	Perceiving Commerial Activeness Over Satellite Images	NA:NA:NA:NA	2018
Wen Hua Lin:Kuan-Ting Chen:Hung Yueh Chiang:Winston Hsu	Recently, deep neural network models have achieved promising results in image captioning task. Yet, "vanilla'' sentences, only describing shallow appearances (e.g., types, colors), generated by current works are not satisfied netizen style resulting in lacking engagements, contexts, and user intentions. To tackle this problem, we propose Netizen Style Commenting (NSC), to automatically generate characteristic comments to a user-contributed fashion photo. We are devoted to modulating the comments in a vivid "netizen'' style which reflects the culture in a designated social community and hopes to facilitate more engagement with users. In this work, we design a novel framework that consists of three major components: (1) We construct a large-scale clothing dataset named NetiLook, which contains 300K posts (photos) with 5M comments to discover netizen-style comments. (2) We propose three unique measures to estimate the diversity of comments. (3) We bring diversity by marrying topic models with neural networks to make up the insufficiency of conventional image captioning works. Experimenting over Flickr30k and our NetiLook datasets, we demonstrate our proposed approaches benefit fashion photo commenting and improve image captioning tasks both in accuracy and diversity.	Netizen-Style Commenting on Fashion Photos: Dataset and Diversity Measures	NA:NA:NA:NA	2018
Mu-Yen Chen:Tien-Chi Huang:Yu Shu:Chia-Chen Chen:Tsung-Che Hsieh:Neil Y. Yen	This study retains the meanings of the original text using Autoencoder (AE) in this regard. This study uses the different loss (includes three types) to train the neural network model, hopes that after compressing sentence features, it can still decompress the original input sentences and classify the correct targets, such as positive or negative sentiment. In this way, it supposed to get the more relative features (compressing sentence features) in the sentences to classify the targets, rather than using the classification loss that may classify by the meaningless features (words). In the result, this study discovers that adding additional features for correction of errors does not interfere with the learning. Also, not all words are needed to be restored without distortion after applying the AE method.	Learning the Chinese Sentence Representation with LSTM Autoencoder	NA:NA:NA:NA:NA:NA	2018
Shreya Ghosh:Soumya K. Ghosh:Rahul Deb Das:Stephan Winter	Several studies have shown that the spatio-temporal mobility traces of human movements can be used to identify an individual. However, this work presents a novel framework for activity-based mobility profiling of individuals using only the temporal information. The proposed framework is conducive to model individuals' activity patterns in temporal scale, and quantifies the uniqueness measures based on certain temporal features of the activity sequence.	Activity-based Mobility Profiling: A Purely Temporal Modeling Approach	NA:NA:NA:NA	2018
Muhammad Usman Ilyas:Jalal Suliman Alowibdi	Several prior studies have demonstrated the possibility of tracking the outbreak and spread of diseases using public tweets and other social media platforms. However, almost all such prior studies were restricted to geographically filtered English language tweets only. This study is the first to attempt a similar approach for Arabic language tweets originating from the Gulf Cooperation Council (GCC) countries. We obtained a list of commonly occurring diseases in the region from the Saudi Ministry of Health. We used both the English disease names as well as their Arabic translations to filter the stream of tweets. We acquired old tweets for a period spanning 29 months. All tweets were geographically filtered for the Middle East and the list of disease names in both English and Arabic languages. We observed that only a small fraction of tweets were in English, demonstrating that prior approaches to disease tracking relying on English language features are less effective for this region. We also demonstrate how Arabic language tweets can be used rather effectively to track the spread of some infectious diseases in the region. We verified our approach by demonstrating that a high degree of correlation between the occurrence of MERS-Coronavirus cases and Arabic language tweets on the disease. We also show that infectious diseases generating fewer tweets and non-infectious diseases do not exhibit the same high correlation. We also verify the usefulness of tracking cases using Twitter mentions by comparing against a ground truth data set of MERS-CoV cases obtained from the Saudi Ministry of Health.	Disease Tracking in GCC Region Using Arabic Language Tweets	NA:NA	2018
Manuel Tomas Carrasco Benitez:Pascal Hitzler:Irwin King:Moussa Lo:Erik Mannens:Daniel Schwabe	NA	Session details: International Project	NA:NA:NA:NA:NA:NA	2018
Martin Serrano:Amelie Gyrard:Elias Tragos:Hung Nguyen	FIESTA-IoT project provides a blueprint experimental infrastructure, software tools, semantic techniques, certification processes and best practices enabling IoT testbed/platforms to interconnect their facility's resources in an interoperable semantic way. FIESTA-IoT project enables the integration of IoT platform's resources, testbeds infrastructure and their associated applications. FIESTA-IoT opens up new opportunities in the development and deployment of experiments using data from IoT testbeds. The FIESTA-IoT infrastructure enables experimenters to use a single EaaS API (i.e. the FIESTA-IoT EaaS API) for executing experiments over multiple IoT federated testbeds in a testbed agnostic way i.e. like accessing a single large scale virtualized testbed. The main goal of the FIESTA-IoT project is to open new horizons in the development and deployment of IoT applications and experiments at a EU (and global) scale, based on the interconnection and interoperability of diverse IoT platforms and testbeds. FIESTA-IoT project's experimental infrastructure provides to the European experimenters in the IoT domain with the unique capability for accessing and sharing IoT semantically annotated datasets in a testbed-agnostic way. FIESTA-IoT enables execution of experiments across multiple IoT testbeds, based on a single API for submitting the experiment and a single set of credentials for the researcher and the portability of IoT experiments across different testbeds and the provision of interoperable standards-based IoT/cloud interfaces over diverse IoT experimental facilities.	FIESTAIoT Project: Federated Interoperable Semantic IoT/cloud Testbeds and Applications	NA:NA:NA:NA	2018
Mark A. Musen:Susanna-Assunta Sansone:Kei-Hoi Cheung:Steven H. Kleinstein:Morgan Crafts:Stephan C. Schürer:John Graybeal	There is an expectation that scientists will archive their experimental data online in public repositories to enable other investigators to verify their work and to re-explore their data in search of new discoveries. When left to their own devices, however, scientists do a poor job creating the metadata that describe their datasets. A lack of standardization makes it difficult for other investigators to find relevant datasets and to perform secondary analyses. The Center for Expanded Data Annotation and Retrieval (CEDAR) was founded with the goal of enhancing the authoring of experimental metadata to make online datasets more useful to the scientific community. CEDAR technology includes Web-based methods for creating and managing libraries of templates for representing metadata. CEDAR's templates interoperate with a repository of scientific ontologies to standardize the way in which the templates may be filled out. Collaborations with several major research projects are allowing us to explore how CEDAR may ease access to scientific data sets stored in public repositories.	CEDAR: Semantic Web Technology to Support Open Science	NA:NA:NA:NA:NA:NA:NA	2018
Philip E. Schreur	Linked Data for Production (LD4P) is a collaboration between six institutions (Columbia, Cornell, Harvard, Library of Congress, Princeton, and Stanford) to begin the transition of technical services production workflows from a series of library-centric data formats (MARC) to ones based in Linked Open Data (LOD). This first phase of the transition focuses on the development of the ability to produce metadata as LOD communally, the enhancement of the BIBFRAME ontology to encompass the multiple resource formats that academic libraries must process, and the engagement of the broader academic library community to ensure a sustainable and extensible environment. As its name implies, LD4P focuses on the immediate needs of metadata production such as ontology coverage and workflow transition. The LD4P partners' work will be based, in part, on a collection of tools that currently exist, such as those developed by the Library of Congress. The cyclical feedback of use and enhancement request to the developers of these tools will allow for their enhancement based on use in an actual production environment. The six institutions involved will focus on materials ranging from art to rare books, from cartographic materials to music, from annotations to workflows. Tool development and enhancement will also be a key aspect of the project. By the end of the first phase of this project (Spring 2018), the partners will have the minimal tooling, workflows, and standards developed to begin the transformation from MARC to LOD in Phase 2 of the project.	Linked Data for Production (LD4P): a Multi-Institutional Approach to Technical Services Transformation	NA	2018
Hsin-Hsi Chen:Manabu Okumura	Memory loss, common seen in elderly people, affects their social interaction in the daily life very much. This 3-year international project jointly funded by Taiwan Ministry of Science and Technology (MOST) and Japan Science and Technology Agency (JST) investigates together the crucial issues behind the hyper aged societies. We aim at developing technologies and systems to provide information recall support for elderly people at the right time and at the right place.	Information Recall Support for Elderly People in Hyper Aged Societies	NA:NA	2018
Pedro Szekely:Mayank Kejriwal	The DARPA Memex program was established with the goal of funding research into building domain-specific search systems that integrated state-of-the-art focused crawling (domain discovery) information extraction and semantic search, and that could be used by users and domain experts with no programming or technical experience. Domain-specific Insight Graphs (DIG) was proposed and funded under Memex and has led to an end-to-end search system currently being used by over 200 law enforcement for combating human trafficking, by investigators from the Securities and Exchange Commission (SEC) in the US for investigating securities fraud, and for numerous other domains of a difficult, socially consequential (e.g., investigative) and unusual nature.	Domain-specific Insight Graphs (DIG)	NA:NA	2018
Manolis Koubarakis:Herve Caumont:Ulrike Daniels:Erwin Goor:Lara König:Valentijn Venus	Copernicus App Lab is a two year project (November 2016 to October 2018) funded by the European Commission under the H2020 program. The consortium consists of AZO (project coordinator), National and Kapodistrian University of Athens, Terradue, RAMANI and VITO. The main objective of Copernicus App Lab is to make Earth observation data produced by the Copernicus program of the European Union available on the Web as linked data to aid their use by mobile developers.	Copernicus App Lab: A Platform for Easy Data Access Connecting the Scientific Earth Observation Community with Mobile Developers	NA:NA:NA:NA:NA:NA	2018
Fosca Giannotti:Roberto Trasarti:Kalina Bontcheva:Valerio Grossi	One of the most pressing and fascinating challenges scientists face today, is understanding the complexity of our globally interconnected society. The big data arising from the digital breadcrumbs of human activities has the potential of providing a powerful social microscope, which can help us understand many complex and hidden socio-economic phenomena. Such challenge requires high-level analytics, modeling and reasoning across all the social dimensions above. There is a need to harness these opportunities for scientific advancement and for the social good, compared to the currently prevalent exploitation of big data for commercial purposes or, worse, social control and surveillance. The main obstacle to this accomplishment, besides the scarcity of data scientists, is the lack of a large-scale open ecosystem where big data and social mining research can be carried out. The SoBigData Research Infrastructure (RI) provides an integrated ecosystem for ethic-sensitive scientific discoveries and advanced applications of social data mining on the various dimensions of social life as recorded by "big data". The research community uses the SoBigData facilities as a "secure digital wind-tunnel" for large-scale social data analysis and simulation experiments. SoBigData promotes repeatable and open science and supports data science research projects by providing: (i) an ever-growing, distributed data ecosystem for procurement, access and curation and management of big social data, to underpin social data mining research within an ethic-sensitive context; (ii) an ever-growing, distributed platform of interoperable, social data mining methods and associated skills: tools, methodologies and services for mining, analysing, and visualising complex and massive datasets, harnessing the techno-legal barriers to the ethically safe deployment of big data for social mining; (iii) an ecosystem where protection of personal information and the respect for fundamental human rights can coexist with a safe use of the same information for scientific purposes of broad and central societal interest. SoBigData has a dedicated ethical and legal board, which is implementing a legal and ethical framework.	SoBigData: Social Mining & Big Data Ecosystem	NA:NA:NA:NA	2018
Mathieu d'Aquin:Dominik Kowald:Angela Fessl:Elisabeth Lex:Stefan Thalmann	The goal of AFEL is to develop, pilot and evaluate methods and applications, which advance informal/collective learning as it surfaces implicitly in online social environments. The project is following a multi-disciplinary, industry-driven approach to the analysis and understanding of learner data in order to personalize, accelerate and improve informal learning processes. Learning Analytics and Educational Data Mining traditionally relate to the analysis and exploration of data coming from learning environments, especially to understand learners' behaviours. However, studies have for a long time demonstrated that learning activities happen outside of formal educational platforms, also. This includes informal and collective learning usually associated, as a side effect, with other (social) environments and activities. Relying on real data from a commercially available platform, the aim of AFEL is to provide and validate the technological grounding and tools for exploiting learning analytics on such learning activities. This will be achieved in relation to cognitive models of learning and collaboration, which are necessary to the understanding of loosely defined learning processes in online social environments. Applying the skills available in the consortium to a concrete set of live, industrial online social environments, AFEL will tackle the main challenges of informal learning analytics through 1) developing the tools and techniques necessary to capture information about learning activities from (not necessarily educational) online social environments; 2) creating methods for the analysis of such informal learning data, based on combining feature engineering and visual analytics with cognitive models of learning and collaboration; and 3) demonstrating the potential of the approach in improving the understanding of informal learning, and the way it is better supported; 4) evaluate all the former items in real world large scale applications and platforms.	AFEL - Analytics for Everyday Learning	NA:NA:NA:NA:NA	2018
Valerio Basile:Roberto Navigli	The exponential growth of the Web is resulting in vast amounts of online content. However, the information expressed therein is not at easy reach: what we typically browse is only an infinitesimal part of the Web. And even if we had time to read all the Web we could not understand it, as most of it is written in languages we do not speak. Rather than time, a key problem for a machine is language comprehension, that is, enabling a machine to transform sentences, i.e., sequences of characters, into machine-readable semantic representations linked to existing meaning inventories such as computational lexicons and knowledge bases.	From MultiJEDI to MOUSSE: Two ERC Projects for Innovating Multilingual Disambiguation and Semantic Parsing of Text	NA:NA	2018
Claudia d'Amato:Francesco Marcelloni:Rudi Studer	It is our great pleasure to welcome you to the first edition of WWW 2018 Journal Track. The track is new track within WWW conference series and it is intended as a forum for presentations of significant Web-related research results that have been published recently in well-known and established journals, and have never been presented at any Web-related conference. The goal is to give visibility of these results to a conference audience as well as to promote discussions concerning such results. The call for papers of the journal track was open for two categories of papers: 1) self-nominations from authors promoting their own journal publication(s) and matching the prerequisites reported above; 2) invited papers, selected on the basis of interest, appropriateness and attractiveness for the WWW 2018 audience, from articles published since January 1st 2015 (even only in the electronic version) in the following journals: Journal of Network and Computer Applications, Journal of Web Semantics, Semantic Web Journal, IEEE Transactions on Fuzzy Systems, IEEE Transactions on Neural Networks and Learning Systems, Journal of Machine Learning Research, Data Mining and Knowledge Discovery, ACM Transactions on the Web, ACM Computing Surveys, Knowledge-based systems, Artificial Intelligence. Exceptions have been also considered for papers judged as potentially very influential but published before January 2015 or in a journal not included in the list. We received 61 submissions and accepted 12 of them for presentation to the WWW 2018 Journal Track. The papers have been evaluated according to the following criteria: novelty, relevance to the conference, quality of the extended abstract, metrics values (e.g. number of citations) computed for the original journal paper, representativeness and impact factor of the journals in which the papers have been published (secondarily). We also took in account the coverage of the different areas related to WWW.	Journal Track Chairs' Welcome & Organization	NA:NA:NA	2018
Ulle Endriss:Umberto Grandi	Graph aggregation is the process of computing a single output graph that constitutes a good compromise between several input graphs, each provided by a different source. One needs to perform graph aggregation in a wide variety of situations, e.g., when applying a voting rule (graphs as preference orders), when consolidating conflicting views regarding the relationships between arguments in a debate (graphs as abstract argumentation frameworks), or when computing a consensus between several alternative clusterings of a given dataset (graphs as equivalence relations). Other potential applications include belief merging, data integration, and social network analysis. In this short paper, we review a recently introduced formal framework for graph aggregation that is grounded in social choice theory. Our focus is on understanding which properties shared by the individual input graphs will transfer to the output graph returned by a given aggregation rule. Our main result is a powerful impossibility theorem that generalises Arrow's seminal result regarding the aggregation of preference orders to a large collection of different types of graphs. We also provide a discussion of existing and potential applications of graph aggregation.	Graph Aggregation	NA:NA	2018
Stefano Calzavara:Riccardo Focardi:Marco Squarcina:Mauro Tempesta	We survey the most common attacks against web sessions, i.e., attacks which target honest web browser users establishing an authenticated session with a trusted web application. We then review existing security solutions which prevent or mitigate the different attacks, by evaluating them along four different axes: protection, usability, compatibility and ease of deployment. Based on this survey, we identify five guidelines that, to different extents, have been taken into account by the designers of the different proposals we reviewed. We believe that these guidelines can be helpful for the development of innovative solutions approaching web security in a more systematic and comprehensive way.	Surviving the Web: A Journey into Web Session Security	NA:NA:NA:NA	2018
Cataldo Musto:Pasquale Lops:Marco de Gemmis:Giovanni Semeraro	In this contribution we propose a hybrid recommendation framework based on classification algorithms such as Random Forests and Naive Bayes, which are fed with several heterogeneous groups of features. We split our features into two classes: classic features, as popularity-based, collaborative and content-based ones, and extended features gathered from the Linked Open Data (LOD) cloud, as basic ones (i.e. genre of a movie or the writer of a book) and graph-based features calculated on the ground of the different topological characteristics of the tripartite representation connecting users, items and properties in the LOD cloud. In the experimental session we evaluate the effectiveness of our framework on varying of different groups of features, and results show that both LOD-based and graph-based features positively affect the overall performance of the algorithm, especially in highly sparse recommendation scenarios. Our approach also outperforms several state-of-the-art recommendation techniques, thus confirming the insights behind this research. This extended abstract summarizes the content of the journal paper published on Knowledge-based Systems.	Semantics-aware Recommender Systems Exploiting Linked Open Data and Graph-based Features	NA:NA:NA:NA	2018
Rathachai Chawuthai:Hideaki Takeda:Vilas Wuwongse:Utsugi Jinbo	Linked Open Data (LOD) technology enables web of data and exchangeable knowledge graphs through the Internet. However, the change in knowledge is happened everywhere and every time, and it becomes a challenging issue of linking data precisely because the misinterpretation and misunderstanding of some terms and concepts may be dissimilar under different context of time and different community knowledge. To solve this issue, we introduce an approach to the preservation of knowledge graph, and we select the biodiversity domain to be our case studies because knowledge of this domain is commonly changed and all changes are clearly documented. Our work produces an ontology, transformation rules, and an application to demonstrate that it is feasible to present and preserve knowledge graphs and provides open and accurate access to linked data. It covers changes in names and their relationships from different time and communities as can be seen in the cases of taxonomic knowledge.	Presenting and Preserving the Change in Taxonomic Knowledge for Linked Data (Extended Abstract)	NA:NA:NA:NA	2018
Lorenz Bühmann:Jens Lehmann:Patrick Westphal:Simon Bin	The following paper is an extended summary of the journal paper "DL-Learner A framework for inductive learning on the Semantic Web". In this system paper, we describe the DL-Learner framework. It is beneficial in various data and schema analytic tasks with applications in different standard machine learning scenarios, e.g. life sciences, as well as Semantic Web specific applications such as ontology learning and enrichment. Since its creation in 2007, it has become the main OWL and RDF-based software framework for supervised structured machine learning and includes several algorithm implementations, usage examples and has applications building on top of the framework.	DL-Learner Structured Machine Learning on Semantic Web Data	NA:NA:NA:NA	2018
Mehrdad Farajtabar:Manuel Gomez-Rodriguez:Yichen Wang:Shuang Li:Hongyuan Zha:Le Song	Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when they are exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes---information diffusion and network evolution---have been typically studied separately, ignoring their co-evolutionary dynamics. In this work, we propose a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. The model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Moreover, we develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. Experiments in both synthetic data and real data gathered from Twitter show that our model provides a good fit to the data as well as more accurate predictions than alternatives.	COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution	NA:NA:NA:NA:NA:NA	2018
Valeria Fionda:Giuseppe Pirro:Claudio Gutierrez	We research the problem of building knowledge maps of graph-like information. We live in the digital era and similarly to the Earth, the Web is simply too large and its interrelations too complex for anyone to grasp much of it through direct observation. Thus, the problem of applying cartographic principles also to digital landscapes is intriguing. We introduce a mathematical formalism that captures the general notion of map of a graph and enables its development and manipulation in a semi-automated way. We describe an implementation of our formalism on the Web of Linked Data graph and discuss algorithms that efficiently generate and combine (via an algebra) regions and maps. Finally, we discuss examples of knowledge maps built with a tool implementing our framework.	Building Knowledge Maps of Web Graphs	NA:NA:NA	2018
Deepayan Chakrabarti:Stanislav Funiak:Jonathan Chang:Sofus A. Macskassy	We consider the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Existing approaches such as Label Propagation fail to consider interactions between the label types. Our proposed method, called EdgeExplain, explicitly models these interactions, while still allowing scalable inference under a distributed message-passing architecture. On a large subset of the Facebook social network, collected in a previous study, EdgeExplain outperforms label propagation for several label types, with lifts of up to $120%$ for [email protected] and 60% for [email protected]	Joint Label Inference in Networks	NA:NA:NA:NA	2018
Elaheh Momeni:Claire Cardie:Nicholas Diakopoulos	User-generated content (UGC) on the Web, especially on social media platforms, facilitates the association of additional information with digital resources and online social topics and it can provide valuable supplementary content. However, UGC varies in quality and, consequently, raises the challenge of how to maximize its utility for a variety of end-users, in particular in the age of misinformation. This study aims to provide researchers and Web data curators with answers to the following questions: (1) What are the existing approaches and methods for assessing and ranking UGC (2) What features and metrics have been used successfully to assess and predict UGC value across a range of application domains This survey is composed of a systematic review of approaches for assessing and ranking UGC: results obtained by identifying and comparing methodologies within the context of short text-based UGC on the Web. This survey categorizes existing assessment and ranking approaches into four framework types and discusses the main contributions and considerations of each type. Furthermore, it suggests a need for further experimentation and encourages the development of new approaches for the assessment and ranking.	How to Assess and Rank User-Generated Content on Web	NA:NA:NA	2018
Jianguo Lu:Hao Wang:Dingding Li	We show that uniform random sampling is not as effective as PPS (probability proportional to size) sampling in many estimation tasks. In the setting of (graph) size estimation, this paper demonstrates that random edge sampling outperforms random node sampling, with a performance ratio proportional to the normalized graph degree variance. This result is particularly important in the era of big data, when data are typically large and scale-free, resulting in large degree variance. We derive the result by first giving the variances of random node and random edge estimators. A simpler and more intuitive result is obtained by assuming that the data is large and degree distribution follows a power law.	Uniform Random Sampling Not Recommended	NA:NA:NA	2018
Maribel Acosta:Elena Simperl:Fabian Flöck:Maria-Esther Vidal	We propose HARE, a SPARQL query engine that encompasses human-machine query processing to augment the completeness of query answers. We empirically assessed the effectiveness of HARE on 50 SPARQL queries over DBpedia. Experimental results clearly show that our solution accurately enhances answer completeness.	HARE: An Engine for Enhancing Answer Completeness of SPARQL Queries via Crowdsourcing	NA:NA:NA:NA	2018
Muhammad Imran:Carlos Castillo:Fernando Diaz:Sarah Vieweg	Millions of people use social media to share information during disasters and mass emergencies. Information available on social media, particularly in the early hours of an event when few other sources are available, can be extremely valuable for emergency responders and decision makers, helping them gain situational awareness and plan relief efforts. Processing social media content to obtain such information involves solving multiple challenges, including parsing brief and informal messages, handling information overload, and prioritizing different types of information. These challenges can be mapped to information processing operations such as filtering, classifying, ranking, aggregating, extracting, and summarizing. This work highlights these challenges and presents state of the art computational techniques to deal with social media messages, focusing on their application to crisis scenarios.	Processing Social Media Messages in Mass Emergency: Survey Summary	NA:NA:NA:NA	2018
Giovanni Luca Ciampaglia:Kristina Lerman:Panagiotis Metaxas	It is our pleasure to welcome you to the WWW 2018 Journalism, Misinformation and Fact Checking Alternate Track. Although the problem of misinformation and deceptive information is as old as Web itself, the topic has gained a lot of attention recently. Phenomena, such as misinformation propagation, fabricated news reports (also known as "fake news",) computational propaganda, astroturf, and ideological polarization have become more common on the Web and the social Web, calling for a cross-cutting approach to better understand the topic. One approach that has gained some traction is that of the establishment of fact-checking organizations. This track solicited contributions that explore the range of computational, social, cognitive, economic, and communication topics related to the above phenomena. We received submissions covering a broad range of topics, including computational approaches for detecting misinformation and propaganda on the Web and social media, as well as proposals to improve fact checking, critical thinking, information and media literacy, crowdsourcing, and societal decision-making processes.	Journalism, Misinformation and Fact Checking Chairs' Welcome & Organization	NA:NA:NA	2018
Sebastian Tschiatschek:Adish Singla:Manuel Gomez Rodriguez:Arpit Merchant:Andreas Krause	Our work considers leveraging crowd signals for detecting fake news and is motivated by tools recently introduced by Facebook that enable users to flag fake news. By aggregating users' flags, our goal is to select a small subset of news every day, send them to an expert (e.g., via a third-party fact-checking organization), and stop the spread of news identified as fake by an expert. The main objective of our work is to minimize the spread of misinformation by stopping the propagation of fake news in the network. It is especially challenging to achieve this objective as it requires detecting fake news with high-confidence as quickly as possible. We show that in order to leverage users' flags efficiently, it is crucial to learn about users' flagging accuracy. We develop a novel algorithm, DETECTIVE, that performs Bayesian inference for detecting fake news and jointly learns about users' flagging accuracy over time. Our algorithm employs posterior sampling to actively trade off exploitation (selecting news that maximize the objective value at a given epoch) and exploration (selecting news that maximize the value of information towards learning about users' flagging accuracy). We demonstrate the effectiveness of our approach via extensive experiments and show the power of leveraging community signals for fake news detection.	Fake News Detection in Social Networks via Crowd Signals	NA:NA:NA:NA:NA	2018
Xuezhi Wang:Cong Yu:Simon Baumgartner:Flip Korn	With the support of major search platforms such as Google and Bing, fact-checking articles, which can be identified by their adoption of the schema.org ClaimReview structured markup, have gained widespread recognition for their role in the fight against digital misinformation. A claim-relevant document is an online document that addresses, and potentially expresses a stance towards, some claim. The claim-relevance discovery problem, then, is to find claim-relevant documents. Depending on the verdict from the fact check, claim-relevance discovery can help identify online misinformation. In this paper, we provide an initial approach to the claim-relevance discovery problem by leveraging various information retrieval and machine learning techniques. The system consists of three phases. First, we retrieve candidate documents based on various features in the fact-checking article. Second, we apply a relevance classifier to filter away documents that do not address the claim. Third, we apply a language feature based classifier to distinguish documents with different stances towards the claim. We experimentally demonstrate that our solution achieves solid results on a large-scale dataset and beats state-of-the-art baselines. Finally, we highlight a rich set of case studies to demonstrate the myriad of remaining challenges and that this problem is far from being solved.	Relevant Document Discovery for Fact-Checking Articles	NA:NA:NA:NA	2018
Dylan Bourgeois:Jérémie Rappaz:Karl Aberer	News entities must select and filter the coverage they broadcast through their respective channels since the set of world events is too large to be treated exhaustively. The subjective nature of this filtering induces biases due to, among other things, resource constraints, editorial guidelines, ideological affinities, or even the fragmented nature of the information at a journalist's disposal. The magnitude and direction of these biases are, however, widely unknown. The absence of ground truth, the sheer size of the event space, or the lack of an exhaustive set of absolute features to measure make it difficult to observe the bias directly, to characterize the leaning's nature and to factor it out to ensure a neutral coverage of the news. In this work, we introduce a methodology to capture the latent structure of media's decision process on a large scale. Our contribution is multi-fold. First, we show media coverage to be predictable using personalization techniques, and evaluate our approach on a large set of events collected from the GDELT database. We then show that a personalized and parametrized approach not only exhibits higher accuracy in coverage prediction, but also provides an interpretable representation of the selection bias. Last, we propose a method able to select a set of sources by leveraging the latent representation. These selected sources provide a more diverse and egalitarian coverage, all while retaining the most actively covered events.	Selection Bias in News Coverage: Learning it, Fighting it	NA:NA:NA	2018
Shweta Bhatt:Sagar Joglekar:Shehar Bano:Nishanth Sastry	This paper aims to shed light on alternative news media ecosystems that are believed to have influenced opinions and beliefs by false and/or biased news reporting during the 2016 US Presidential Elections. We examine a large, professionally curated list of 668 hyper-partisan websites and their corresponding Facebook pages, and identify key characteristics that mediate the traffic flow within this ecosystem. We uncover a pattern of new websites being established in the run up to the elections, and abandoned after. Such websites form an ecosystem, creating links from one website to another, and by 'liking' each others' Facebook pages. These practices are highly effective in directing user traffic internally within the ecosystem in a highly partisan manner, with right-leaning sites linking to and liking other right-leaning sites and similarly left-leaning sites linking to other sites on the left, thus forming a filter bubble amongst news producers similar to the filter bubble which has been widely observed among consumers of partisan news. Whereas there is activity along both left- and right-leaning sites, right-leaning sites are more evolved, accounting for a disproportionate number of abandoned websites and partisan internal links. We also examine demographic characteristics of consumers of hyper partisan news and find that some of the more populous demographic groups in the US tend to be consumers of more right-leaning sites.	Illuminating an Ecosystem of Partisan Websites	NA:NA:NA:NA	2018
Andreas Spitz:Michael Gertz	The increasing number of news outlets and the frequency of the news cycle have made it all but impossible to obtain the full picture from online news. Consolidating news from different sources has thus become a necessity in online news processing. Despite the amount of research that has been devoted to different aspects of new event detection and tracking in news streams, solid solutions for such entangled streams of full news articles are still lacking. Many existing works focus on streams of microblogs since the analysis of news articles raises the additional problem of summarizing or extracting the relevant sections of articles. For the consolidation of identified news snippets, schemes along numerous different dimensions have been proposed, including publication time, temporal expressions, geo-spatial references, named entities, and topics. The granularity of aggregated news snippets then includes such diverse aspects as events, incidents, threads, or topics for various subdivisions of news articles. To support this variety of granularity levels, we propose a comprehensive network model for the representation of multiple entangled streams of news documents. Unlike previous methods, the model is geared towards entity-centric explorations and enables the consolidation of news along all dimensions, including the context of entity mentions. Since the model also serves as a reverse index, it supports explorations along the dimensions of sentences or documents for an encompassing view on news events. We evaluate the performance of our model on a large collection of entangled news streams from major news outlets of English speaking countries and a ground truth that we generate from event summaries in the Wikipedia Current Events portal.	Exploring Entity-centric Networks in Entangled News Streams	NA:NA	2018
Sylvie Cazalens:Philippe Lamarre:Julien Leblay:Ioana Manolescu:Xavier Tannier	Fact checking has captured the attention of the media and the public alike; it has also recently received strong attention from the computer science community, in particular from data and knowledge management, natural language processing and information retrieval; we denote these together under the term "content management". In this paper, we identify the fact checking tasks which can be performed with the help of content management technologies, and survey the recent research works in this area, before laying out some perspectives for the future. We hope our work will provide interested researchers, journalists and fact checkers with an entry point in the existing literature as well as help develop a roadmap for future research and development work.	A Content Management Perspective on Fact-Checking	NA:NA:NA:NA:NA	2018
Svitlana Volkova:Jin Yea Jang	Deceptive information in online news and social media has had dramatic effect on our society in recent years. This study is the first to gain deeper insights into writers' intent behind digital misinformation by analyzing psycholinguistic signals: moral foundations and connotations extracted from different types of deceptive news ranging from strategic disinformation to propaganda and hoaxes. To ensure consistency of our findings and generalizability across domains, we experiment with data from: (1) confirmed cases of disinformation in news summaries, (2) propaganda, hoax, and disinformation news pages, and (3) social media news. We first contrast lexical markers of biased language, syntactic and stylistic signals, and connotations across deceptive news types including disinformation, propaganda, and hoaxes, and deceptive strategies including misleading or falsification. We then incorporate these insights to build machine learning and deep learning predictive models to infer deception strategies and deceptive news types. Our experimental results demonstrate that unlike earlier work on deception detection, content combined with biased language markers, moral foundations, and connotations leads to better predictive performance of deception strategies compared to syntactic and stylistic signals (as reported in earlier work on deceptive reviews). Falsification strategy is easier to identify than misleading strategy. Disinformation is more difficult to predict than to propaganda or hoaxes. Deceptive news types (disinformation, propaganda, and hoaxes), unlike deceptive strategies (falsification and misleading), are more salient, and thus easier to identify in tweets than in news reports. Finally, our novel connotation analysis across deception types provides deeper understanding of writers' perspectives and therefore reveals the intentions behind digital misinformation.	Misleading or Falsification: Inferring Deceptive Strategies and Types in Online News and Social Media	NA:NA	2018
Jing Ma:Wei Gao:Kam-Fai Wong	In recent years, an unhealthy phenomenon characterized as the massive spread of fake news or unverified information (i.e., rumors) has become increasingly a daunting issue in human society. The rumors commonly originate from social media outlets, primarily microblogging platforms, being viral afterwards by the wild, willful propagation via a large number of participants. It is observed that rumorous posts often trigger versatile, mostly controversial stances among participating users. Thus, determining the stances on the posts in question can be pertinent to the successful detection of rumors, and vice versa. Existing studies, however, mainly regard rumor detection and stance classification as separate tasks. In this paper, we argue that they should be treated as a joint, collaborative effort, considering the strong connections between the veracity of claim and the stances expressed in responsive posts. Enlightened by the multi-task learning scheme, we propose a joint framework that unifies the two highly pertinent tasks, i.e., rumor detection and stance classification. Based on deep neural networks, we train both tasks jointly using weight sharing to extract the common and task-invariant features while each task can still learn its task-specific features. Extensive experiments on real-world datasets gathered from Twitter and news portals demonstrate that our proposed framework improves both rumor detection and stance classification tasks consistently with the help of the strong inter-task connections, achieving much better performance than state-of-the-art methods.	Detect Rumor and Stance Jointly by Neural Multi-task Learning	NA:NA:NA	2018
Miriam Fernandez:Harith Alani	Misinformation has become a common part of our digital media environments and it is compromising the ability of our societies to form informed opinions. It generates misperceptions, which have affected the decision making processes in many domains, including economy, health, environment, and elections, among others. Misinformation and its generation, propagation, impact, and management is being studied through a variety of lenses (computer science, social science, journalism, psychology, etc.) since it widely affects multiple aspects of society. In this paper we analyse the phenomenon of misinformation from a technological point of view. We study the current socio-technical advancements towards addressing the problem, identify some of the key limitations of current technologies, and propose some ideas to target such limitations. The goal of this position paper is to reflect on the current state of the art and to stimulate discussions on the future design and development of algorithms, methodologies, and applications.	Online Misinformation: Challenges and Future Directions	NA:NA	2018
Amy X. Zhang:Aditya Ranganathan:Sarah Emlen Metz:Scott Appling:Connie Moon Sehat:Norman Gilmore:Nick B. Adams:Emmanuel Vincent:Jennifer Lee:Martin Robbins:Ed Bice:Sandro Hawke:David Karger:An Xiao Mina	The proliferation of misinformation in online news and its amplification by platforms are a growing concern, leading to numerous efforts to improve the detection of and response to misinformation. Given the variety of approaches, collective agreement on the indicators that signify credible content could allow for greater collaboration and data-sharing across initiatives. In this paper, we present an initial set of indicators for article credibility defined by a diverse coalition of experts. These indicators originate from both within an article's text as well as from external sources or article metadata. As a proof-of-concept, we present a dataset of 40 articles of varying credibility annotated with our indicators by 6 trained annotators using specialized platforms. We discuss future steps including expanding annotation, broadening the set of indicators, and considering their use by platforms and the public, towards the development of interoperable standards for content credibility.	A Structured Response to Misinformation: Defining and Annotating Credibility Indicators in News Articles	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Michele Bedard:Chianna Schoenthaler	Ever since the surprising results from the 2016 U.S. presidential race, the subject of Fake News in our worldwide media consumption has grown steadily. On a smaller scale, mainstream media have taken a closer look at the relatively narrow genre of satirical news content. Ed Koltonski of Kent State, defines satirical news as designed specifically to entertain the reader, usually with irony or wit, to critique society or a social figure and invoke change or reform. Using field experiment, survey and focus group methods we sought to determine if media consumers' ability to differentiate between satirical news and fake news is tied to socio-demographic factors. We found that age, education, sex, and political affiliation predict understanding of "fake news" and satire. Furthermore, the ability to identify different types of misinformation when presented with screen shots from social media posts appears to be related to these variables. Focus group comments were also analyzed to gain a richer perspective on how participants interpreted the SMS screen shots. Using our primary research, we seek to determine if there is a correlation between social media consumers understanding of the difference between satirical news versus fake news and their varying socio-demographic factors	Satire or Fake News: Social Media Consumers' Socio-Demographics Decide	NA:NA	2018
Fred Morstatter:Yunqiu Shao:Aram Galstyan:Shanika Karunasekera	In the 2017 German Federal elections, the "Alternative for Deutschland'', or AfD, party was able to take control of many seats in German parliament. Their success was credited, in part, to their large online presence. Like other "alt-right'' organizations worldwide, this party is tech savvy, generating a large social media footprint, especially on Twitter, which provides an ample opportunity to understand their online behavior. In this work we present an analysis of Twitter data related to the aforementioned election. We show how users self-organize into communities, and identify the themes that define those communities. Next we analyze the content generated by those communities, and the extent to which these communities interact. Despite these elections being held in Germany, we note a substantial impact from the English-speaking Twittersphere. Specifically, we note that many of these accounts appear to be from the American alt-right movement, and support the German alt-right movement.	From Alt-Right to Alt-Rechts: Twitter Analysis of the 2017 German Federal Election	NA:NA:NA:NA	2018
Evgeniy Gabrilovich:Kira Radinsky:Kuansan Wang	It is our great pleasure to welcome you to the BIG Web Track of the Web Conference 2018. Many of today's most successful enterprises in business and in science are built on the collection and analysis of data. The sheer volume and richness of these data sets has stimulated a massive wave of innovation. In addition, this revolution has also sparked important debate on data privacy policies, ethics, and governance. This track started as a co-located event called BigData Innovators Gathering (BIG) with a vision to bring together academic and industry leaders in the Big Data space to share the state of the art and its successful applications in business. This event will be co-located with the Web conference for the fifth time, but now as a fully fledged alternate track named The BIG Web in The Web Conference 2018 in Lyon. This year's track consists of two keynotes, a panel on machine learning in the field of medicine, and 11 invited talks. In addition, we have accepted 6 papers from 35 submissions (with an acceptance ratio of 17%).	The BIG Web Track Chairs' Welcome & Organization	NA:NA:NA	2018
Kunwoo Park:Meeyoung Cha:Eunhee Rhim	Customer ratings are valuable sources to understand their satisfaction and are critical for designing better customer experiences and recommendations. The majority of customers, however, do not respond to rating surveys, which makes the result less representative. To understand overall satisfaction, this paper aims to investigate how likely customers without responses had satisfactory experiences compared to those respondents. To infer customer satisfaction of such unlabeled sessions, we propose models using recurrent neural networks (RNNs) that learn continuous representations of unstructured text conversation. By analyzing online chat logs of over 170,000 sessions from Samsung's customer service department, we make a novel finding that while labeled sessions contributed by a small fraction of customers received overwhelmingly positive reviews, the majority of unlabeled sessions would have received lower ratings by customers. The data analytics presented in this paper not only have practical implications for helping detect dissatisfied customers on live chat services but also make theoretical contributions on discovering the level of biases in online rating platforms.	Positivity Bias in Customer Satisfaction Ratings	NA:NA:NA	2018
Ya-Lin Zhang:Longfei Li:Jun Zhou:Xiaolong Li:Zhi-Hua Zhou	In this paper, we consider the problem of anomaly detection. Previous studies mostly deal with this task in either supervised or unsupervised manner according to whether label information is available. However, there always exists settings which are different from the two standard manners. In this paper, we address the scenario when anomalies are partially observed, i.e., we are given a large amount of unlabeled instances as well as a handful labeled anomalies. We refer to this problem as anomaly detection with partially observed anomalies, and proposed a two-stage method ADOA to solve it. Firstly, by addressing the difference between the anomalies, the observed anomalies are clustered, while the unlabeled instances are filtered to get potential anomalies and reliable normal instances. Then, with the above instances, a weight is attached to each instance according to the confidence of its label, and a weighted multi-class model is built, which will be further used to distinguish different anomalies to the normal instances. Experimental results show that in the aforementioned setting, existing methods behave unsatisfactorily and the proposed method performs significantly better than all these methods, which validates the effectiveness of the proposed approach.	Anomaly Detection with Partially Observed Anomalies	NA:NA:NA:NA:NA	2018
Wenshan Wang:Su Yang:Zhiyuan He:Minjie Wang:Jiulong Zhang:Weishan Zhang	People can percept social attributes from streetscapes such as safety, richness, and happiness by means of visual perception, which inspires the research in terms of urban perception. To the best of our knowledge, this is the first work focused on revealing the relationship between visual patterns of satellite images as well as streetscapes and commercial activeness. We propose to make use of bag of features (BoF) in the context of computer vision and sparse representation in the sense of machine learning to predict commercial activeness of urban commercial districts. After obtaining the urban commercial districts via clustering, we predict the commercial activeness degrees of them using four image features, namely, Histogram of Oriented Gradients (HOG), Autoencoder, GIST, and multifractal spectra for satellite images and street view images, respectively. The performance evaluation with four large-scale datasets demonstrates that the presented computational framework can not only predict the commercial activeness with satisfactory precision compared with that based on Point of Interest (POI) data but also discover the visual patterns related.	Urban Perception of Commercial Activeness from Satellite Images and Streetscapes	NA:NA:NA:NA:NA:NA	2018
Dotan Di Castro:Iftah Gamzu:Irena Grabovitch-Zuyev:Liane Lewin-Eytan:Abhinav Pundir:Nil Ratan Sahoo:Michael Viderman	Mail extraction is a critical task whose objective is to extract valuable data from the content of mail messages. This task is key for many types of applications including re-targeting, mail search, and mail summarization, which utilize the important personal data pieces in mail messages to achieve their objectives. We focus on machine generated traffic, which comprises most of the Web mail traffic today, and use its structured and large-scale repetitive nature to devise a fully automated extraction method. Our solution builds on an advanced structural clustering technique previously presented by some of the authors of this work. The heart of our solution is an offline process that leverages the structural mail-specific characteristics of the clustering, and automatically creates extraction rules that are later applied online for each new arriving message. We provide of a full description of our process, which has been productized in Yahoo mail backend. We complete our work with large-scale experiments carried over real Yahoo mail traffic, and evaluate the performance of our automatic extraction method.	Automated Extractions for Machine Generated Mail	NA:NA:NA:NA:NA:NA:NA	2018
Qianyun Zhang:Shawndra Hill:David Rothschild	Although consumer behavior in response to search engine marketing has been studied extensively, few efforts have been made to understand how consumers search and respond to ads post purchase. Advertising to existing customers the same way as to prospective customers inevitably leads to wasteful and inefficient marketing. Employing a unique dataset that combines both search query and purchase data, we examine consumers' searching behavior and response to search engine marketing after purchase. We study large advertising campaigns for two popular technology products. We find that over half of the branded keyword searches come from consumers who already purchased the products, and that advertising response varies based on whether searchers are pre- or post-purchase. In general, post-purchase searchers are less likely to click on focal brand ads (i.e., they are less responsive to ads for products they already own). However, post-purchase searchers are still responsive to advertising and much more likely to click on ads for complementary products (i.e., they are more responsive to ads for relevant products other than the focal product).	Post Purchase Search Engine Marketing	NA:NA:NA	2018
Xinpeng Chen:Jingyuan Chen:Lin Ma:Jian Yao:Wei Liu:Jiebo Luo:Tong Zhang	Nowadays, billions of videos are online ready to be viewed and shared. Among an enormous volume of videos, some popular ones are widely viewed by online users while the majority attract little attention. Furthermore, within each video, different segments may attract significantly different numbers of views. This phenomenon leads to a challenging yet important problem, namely fine-grained video attractiveness prediction, which only relies on video contents to forecast video attractiveness at fine-grained levels, specifically video segments of several second length in this paper. However, one major obstacle for such a challenging problem is that no suitable benchmark dataset currently exists. To this end, we construct the first fine-grained video attractiveness dataset (FVAD), which is collected from one of the most popular video websites in the world. In total, the constructed FVAD consists of 1,019 drama episodes with 780.6 hours covering different categories and a wide variety of video contents. Apart from the large amount of videos, hundreds of millions of user behaviors during watching videos are also included, such as view counts, "fast-forward, "fast-rewind, and so on, where "view counts" reflects the video attractiveness while other engagements capture the interactions between the viewers and videos. First, we demonstrate that video attractiveness and different engagements present different relationships. Second, FVAD provides us an opportunity to study the fine-grained video attractiveness prediction problem. We design different sequential models to perform video attractiveness prediction by relying solely on video contents. The sequential models exploit the multimodal relationships between visual and audio components of the video contents at different levels. Experimental results demonstrate the effectiveness of our proposed sequential models with different visual and audio representations, the necessity of incorporating the two modalities, and the complementary behaviors of the sequential prediction models at different levels.	Fine-grained Video Attractiveness Prediction Using Multimodal Deep Learning on a Large Real-world Dataset	NA:NA:NA:NA:NA:NA:NA	2018
Manuel Serrano:Sukyoung Ryu	It is our great pleasure to welcome you to the WWW 2018 Web Programming alternate track. We received 14 proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty, selecting 1 full-day talks.	Web Programming Chairs' Welcome & Organization	NA:NA	2018
Gabriel Radanne:Jérôme Vouillon	Tierless Web programming languages allow combining client-side and server-side programming in a single program. This allows defining expressions with both client and server parts, and at the same time provides good static guarantees regarding client-server communication. However, these nice properties come at a cost: most tierless languages offer very poor support for modularity and separate compilation. To regain this modularity and offer a larger-scale notion of composition, we propose to leverage a well-known tool: ML-style modules. In modern ML languages, the module system is a layer separate from the expression language. Eliom is an extension of OCaml for tierless Web programming which provides type-safe communication and an efficient execution model. In this article, we present how the Eliom module system combines the flexibility of tierless Web programming languages with a powerful module system, thus providing good support for abstraction, modularity and separate compilation. We also show that we can provide all these advantages while providing seamless integration with OCaml and its ecosystem.	Tierless Web Programming in the Large	NA:NA	2018
Arthur Charguéraud:Alan Schmitt:Thomas Wood	We present JSExplain, a reference interpreter for JavaScript that closely follows the specification and that produces execution traces. These traces may be interactively investigated in a browser, with an interface that displays not only the code and the state of the interpreter, but also the code and the state of the interpreted program. Conditional breakpoints may be expressed with respect to both the interpreter and the interpreted program. In that respect, JSExplain is a double-debugger for the specification of JavaScript.	JSExplain: A Double Debugger for JavaScript	NA:NA:NA	2018
Stéphane Letz:Yann Orlarey:Dominique Fober	\beginabstract This paper demonstrates how FAUST, a functional programming language for sound synthesis and audio processing, can be used to develop efficient audio code for the Web. After a brief overview of the language, its compiler and the architecture system allowing to deploy the same program as a variety of targets, the generation of WebAssembly code and the deployment of specialized WebAudio nodes will be explained. Several use cases will be presented. Extensive benchmarks to compare the performance of native and WebAssembly versions of the same set of DSP have be done and will be commented. \endabstract	FAUST Domain Specific Audio DSP Language Compiled to WebAssembly	NA:NA:NA	2018
Véronique Benzaken:Giuseppe Castagna:Laurent Daynès:Julien Lopez:Kim Nguyen:Romain Vernoux	We present BOLDR, a modular framework that enables the evaluation in databases of queries containing application logic and, in particular, user-defined functions. BOLDR also allows the nesting of queries for different databases of possibly different data models. The framework detects the boundaries of queries present in an application, translates them into an intermediate representation together with the relevant language environment, rewrites them in order to avoid query avalanches and to make the most out of database optimizations, and converts the results back to the application. Our experiments show that the techniques we implemented are applicable to real-world database applications, successfully handling a variety of language-integrated queries with good performances.	Language-Integrated Queries: a BOLDR Approach	NA:NA:NA:NA:NA:NA	2018
Nick ten Veen:Daco C. Harkes:Eelco Visser	Modern web applications are interactive. Reactive programming languages and libraries are the state-of-the-art approach for declara- tively specifying these interactive applications. However, programs written with these approaches contain error-prone boilerplate code for e ciency reasons. In this paper we present PixieDust, a declarative user-interface language for browser-based applications. PixieDust uses static de- pendency analysis to incrementally update a browser-DOM at run- time, without boilerplate code. We demonstrate that applications in PixieDust contain less boilerplate code than state-of-the-art ap- proaches, while achieving on-par performance.	PixieDust: Declarative Incremental User Interface Rendering Through Static Dependency Tracking	NA:NA:NA	2018
Minh Ngo:Nataliia Bielova:Cormac Flanagan:Tamara Rezk:Alejandro Russo:Thomas Schmitz	Multiple Facets (MF) is a dynamic enforcement mechanism which has proved to be a good fit for implementing information flow security for JavaScript. It relies on multi executing the program, once per each security level or view, to achieve soundness. By looking inside programs, MF encodes the views to reduce the number of needed multi-executions. In this work, we extend Multiple Facets in three directions. First, we propose a new version of MF for arbitrary lattices, called Generalised Multiple Facets, or GMF. GMF strictly generalizes MF, which was originally proposed for a specific lattice of principals. Second, we propose a new optimization on top of GMF that further reduces the number of executions. Third, we strengthen the security guarantees provided by Multiple Facets by proposing a termination sensitive version that eliminates covert channels due to termination.	A Better Facet of Dynamic Information Flow Control	NA:NA:NA:NA:NA:NA	2018
Achim D. Brucker:Michael Herzberg	At its core, the Document Object Model (DOM) defines a tree-like data structure for representing documents in general and HTML documents in particular. It is the heart of any modern web browser. Formalizing the key concepts of the DOM is a prerequisite for the formal reasoning over client-side JavaScript programs and for the analysis of security concepts in modern web browsers. We present a formalization of the core DOM, with focus on the node-tree and the operations defined on node-trees, in Isabelle/HOL. We use the formalization to verify the functional correctness of the most important functions defined in the DOM standard. Moreover, our formalization is extensible, i.e., can be extended without the need of re-proving already proven properties and executable, i.e., we can generate executable code from our specification.	A Formal Semantics of the Core DOM in Isabelle/HOL	NA:NA	2018
Amy Guy:Thomas Steiner	It is our great pleasure to welcome you to the WWW 2018 Developers' Track. We had 12 submissions in total, out of which 7 were papers for the proceedings (5 long, 2 short), and 5 were free-form formats that did not go in the proceedings. A lot of great publications are accompanied by great implementations that sometimes risk going almost unnoticed in favor of the more glamorous research results they helped produce. Likewisethe Web being a moving and ever-developing targeta lot of sometimes tedious and oftentimes less obvious work happens on standardization of future Web APIs and programming languages. The Developers' Track aims to put this implementation and standardization work front and center. It highlights research submissions that describe technically challenging Web applications of all sorts. Apart from classic papers (that we do understand are a fixed requirement for some people in order be allowed to the conference), the Developers' Track was not limited to formats that can be printed, and authors were encouraged to be creative in finding the most effective way to communicate their work, and we have included dynamic or interactive contributions.	Developers' Track Chairs' Welcome & Organization	NA:NA	2018
Ksenia Peguero:Nan Zhang:Xiuzhen Cheng	\textitBackground: JavaScript frameworks are widely used to create client-side and server-side parts of contemporary web applications. Vulnerabilities like cross-site scripting introduce significant risks in web applications.\\ \textitAim: The goal of our study is to understand how the security features of a framework impact the security of the applications written using that framework.\\ \textitMethod: In this paper, we present four locations in an application, relative to the framework being used, where a mitigation can be applied. We perform an empirical study of JavaScript applications that use the three most common template engines: Jade/Pug, EJS, and Angular. Using automated and manual analysis of each group of applications, we identify the number of projects vulnerable to cross-site scripting, and the number of vulnerabilities in each project, based on the framework used.\\ \textitResults: We analyze the results to compare the number of vulnerable projects to the mitigation locations used in each framework and perform statistical analysis of confounding variables.\\ \textitConclusions: The location of the mitigation impacts the application's security posture, with mitigations placed within the framework resulting in more secure applications.	An Empirical Study of the Framework Impact on the Security of JavaScript Web Applications	NA:NA:NA	2018
Michel Buffa:Jérôme Lebrun:Jari Kleimola:Oliver larkin:Stéphane Letz	Web Audio is a recent W3C API that brings the world of computer music applications to the browser. Although developers have been actively using it since the first beta implementations in 2012, the number of web apps built using Web Audio API cannot yet compare to the number of commercial and open source audio software tools available on native platforms. Many of the sites using this new technology are of an experimental nature or are very limited in their scope. While JavaScript and Web standards are increasingly flexible and powerful, C and C++ are the languages most often used for real-time audio applications and domain specific languages such as FAUST facilitate rapid development with high performance. Our work aims to create a continuum between native and browser based audio app development and to appeal to programmers from both worlds. This paper presents our proposal including guidelines and proof of concept implementations for an open Web Audio plug-in standard - essentially the infrastructure to support high level audio plug-ins for the browser.	Towards an open Web Audio plugin standard	NA:NA:NA:NA:NA	2018
Andrea Gallidabino:Cesare Pautasso	In the past years the average number of Web-enabled devices owned by each user has significantly increased. Liquid Web applications enable users to take advantage of all their devices sequentially to migrate their running applications across them or simultaneously when running different views of the same application at the same time on each device. Developers of liquid Web application need to control how to expose the liquid behavior of their cross-device Web applications to the users. To do so, they can use the API of Liquid.js we describe in this paper. Liquid.js is a framework for building component-based rich Web applications which run across multiple Web-enabled devices. The framework is based on technologies such as Polymer, WebRTC, WebWorkers, PouchDB and Yjs. Liquid.js helps to build decentralized Web applications whose components can seamlessly flow directly between Web browsers carrying along their execution state. The Liquid.js API gives developers fine-grained control over the liquid user experience primitives, device discovery, and the lifecycle of liquid Web components.	The Liquid User Experience API	NA:NA	2018
Pasquale Lisena:Raphaël Troncy	SPARQL endpoints are one possible access method to linked data. The results of SPARQL queries serialized in JSON are, however, not suitable to be directly used by web developers in end-user applications who often need to merge the values resulting from variable bindings. In this work, we propose a generic approach implemented in a JavaScript module that takes as input a JSON file describing both the SPARQL query and the shape of the expected output at the same time.	Transforming the JSON Output of SPARQL Queries for Linked Data Clients	NA:NA	2018
István Koren:Ralf Klamma	New Internet-enabled devices and Web services are introduced on a daily basis. Documentation formats are available that describe their functionalities in terms of API endpoints and parameters. In particular, the OpenAPI specification has gained considerable influence over the last years. Web-based solutions exist that generate interactive OpenAPI documentation with HTML5 & JavaScript. They allow developers to quickly get an understanding what the services and devices do and how they work. However, the generated user interfaces are far from real-world practices of designers and end users. We present an approach to overcome this gap, by using a model-driven methodology resulting in state-of-the-art responsive Web user interfaces. To this end, we use the Interaction Flow Modeling Language (IFML) as intermediary model specification to bring together APIs and frontends. Our implementation is based on open standards like Web Components and SVG. A screencast of our tool is available at https://youtu.be/KFOPmPShak4	The Exploitation of OpenAPI Documentation for the Generation of Web Frontends	NA:NA	2018
Thomas Steiner	Progressive Web Apps (PWA) are a new class of Web applications, enabled for the most part by the Service Workers APIs. Service Workers allow apps to work offline by intercepting network requests to deliver programmatic or cached responses, Service Workers can receive push notifications and synchronize data in the background even when the app is not running, andtogether with Web App Manifestsallow users to install PWAs to their devices' home screens. Service Workers being a Web standard, support has landed in several stand-alone Android Web browsersamong them (but not limited to) Chrome and its open-source foundation Chromium, Firefox, Edge, Opera, UC Browser, Samsung Internet, andeagerly awaitediOS Safari. In this paper, we examine the PWA feature support situation in Web Views, that is, in-app Web experiences that are explicitly not stand-alone browsers. Such in-app browsers can commonly be encountered in chat applications like WeChat or WhatsApp, online social networks like Facebook or Twitter, but also email clients like Gmail, or simply anywhere where Web content is displayed inside native apps. We have developed an open-source application called PWA Feature Detector that allows for easily testing in-app browsers (and naturally stand-alone browsers), and have evaluated the level of support for PWA features on different devices and Web Views. On the one hand, our results show that there are big differences between the various Web View technologies and the browser engines they are based upon, but on the other hand, that for Android the results are independent from the devices' operating systems, which is good news given the problematic update policy of many device manufacturers. These findings help developers make educated choices when it comes to determining whether a PWA is the right approach given their target users' means of Web access.	What is in a Web View: An Analysis of Progressive Web App Features When the Means of Web Access is not a Web Browser	NA	2018
Erik Wilde	The Web is based on numerous standards that together make up the surface of the Web: By knowing and supporting those standards, problems can be solved in well-known ways. This general design pattern on the Web applies to APIs in the very same way as it does to the human Web: By using an (evolving) set of standards, API developers benefit by not having to reinvent the wheel, and developers benefit by the same problem being solved in the same way across a variety of APIs. The evolving set of standards for Web APIs can be regarded as a set of building blocks or vocabularies for API design. Web Concepts is a site (webconcepts.info) and a repository (github.com/dret/webconcepts) that can be used to manage how within organizations these building blocks are used, thus helping to establish a Web API design culture. The main idea of Web Concepts is to promote reuse of existing standards and technologies, and to therefore make it easier for teams to understand which options are available generally speaking, and maybe which ones are popular choices within their organization.	Surfing the API Web: Web Concepts	NA	2018
Sylvie Calabretto:Lalana Kagal:Maria Maleshkova	It is our great pleasure to welcome you to the WWW 2018 PhD Symposium. The goal of the PhD Symposium is to provide a supportive atmosphere for PhD students to present and receive feedback on their ongoing work. Students at different stages in their research get an opportunity to present and discuss their problem statements, goals, methods and results. The symposium aims to provide students with useful guidance on various aspects of their research from established researchers and other PhD students, working in areas related to the World Wide Web. Finally, the symposium also aims to enable PhD students to interact with other participants and potential collaborators in order to stimulate an exchange of ideas, suggestions and experiences. We received 25 PhD proposal submissions from all around the world covering a broad range of topics. We evaluated them with respect to methodology, approach, relevance and novelty, selecting 8 papers for long presentation and 4 papers for short presentation. We also took in account the coverage of the different areas related to WWW as well as the potential audience and the possibility to receive feedback from seniors and peers.	PhD Symposium Chairs' Welcome	NA:NA:NA	2018
Lucas Azevedo	In the actual scenario of ever-growing data consumption speed and quantity, factors like news source decentralization, citizen journalism and democratization of media, make the task of manually checking and correcting disinformation across the internet impractical or infeasible . Here, there is an imperative need for a fast and reliable way to account for the veracity of what is produced and spread as information: Automatic fact-checking. In this work we present the problem of fact-checking in the era of big data and post-truth. Some existing approaches for this task are presented and their main features discussed and compared. Concluding, a new approach inspired on the best components of the existing ones is presented.	Truth or Lie: Automatically Fact Checking News	NA	2018
Tobias Grubenmann	Inspired by the World Wide Web, the Web of Data is a network of interlinked data fragments. One of the main advantages of the Web of Data is that all of its content is processable by machines. However, this also has its drawbacks when it comes to monetization of the content: advertisements and donationstwo important financial motors in the World Wide Webdo not translate into the Web of Data as they rely on exposing the user to advertisement/call for donations. The remedy this situation, we propose two different monetization strategies for the Web of Data. The first strategy involves a marketplace where users can buy data in an integrated way. The second strategy allows third parties to promote certain data. In return, the sponsors pay money whenever a user follows a link contained in the sponsored data. We identified two different kind of datacommercial and sponsored datawhich can benefit from the two respective monetization strategies. With our work, we propose solutions to the problem of financing the creation and maintenance of content in the Web of Data.	Monetization Strategies for the Web of Data	NA	2018
Tobias Weller	The number of users of the world wide web is constantly increasing. However, this also increases the risks. There is the possibility that other users illegally gain access to a users' account of social networks, web shops or other web services. Previous work use graph-based methods to identify hijacked or compromised accounts. Most often posts are used in social networks to detect fraudulences. However, not every compromised account is used to spread propaganda information or phishing attacks. Therefore, we restrict ourselves to the clickstreams from the accounts. In order to identify compromised accounts by means of clickstreams, we will also consider a temporal aspect, since the preferences of a user change over time. We choose a hybrid approach consisting of methods from subsymbolic and symbolic AI to detect fraudulences in clickstreams. We will also take into account the experience of domain experts. Our approach can also be used to identify not only compromised accounts but also shared accounts on instance streaming sites.	Compromised Account Detection Based on Clickstream Data	NA	2018
Robin Marx	Web performance is important for the user experience and can heavily influence web page revenues. While there are many established Web Performance Optimization (WPO) methods, our work so far has clearly shown that new network protocols, optimized browsers and cutting-edge web standards can have a significant impact on known best practices. Additionally, there is still low-hanging fruit to be exploited, in the form of personalizing performance based on user context (i.e., current device, network, browser) and user preferences (e.g., text reading vs multimedia experience). In our PhD project, we strive to integrate this user-specific metadata into dynamic configurations for both existing and new automated WPO techniques. An intermediate server can (pre)generate optimized versions of a web page, which are then selected based on user context and preferences. Additional metadata is also passed along to the browser, enabling improvements on that side, and used to steer new network protocols to speed up the incremental delivery of page resources. We use the Speeder platform to perform and evaluate full-factorial objective measurements and use subjective user studies across a range of groups to assess the applicability of our methods to end users. Our aim is to provide insights in how WPO can be tweaked for specific users, in the hopes of leading to new web standards that enable this behavior.	Web Performance Automation for the People	NA	2018
Dakshi Tharanga Kapugama Geeganage	Text contents are overloaded with the digitization of the data and new contents are transmitted through many sources by generating a large volume of information, which spreads all over the world through different communication media. Therefore, text data is available everywhere and reading, understanding and analysing the text data has become a main activity in daily routine. With the increment of the volume and the variety of information, organizing and searching, the required information has become vital. Topic modelling is the state of the art for information organization, understanding and extracting the content. Most of the prevailing topic models use the probabilistic approaches and consider the frequency and the co-occurrence to discover the topics from collections of documents. The proposed research aims to address the existing problems of topic modeling by introducing a concept embedded topic model which generates the most relevant and meaningful topics by understanding the content. The research includes approaches to understand the semantic elements from the content, domain identification of concepts and provide most suitable topics without getting the number of topics from the user beforehand. Capturing the semantics of document collections and generating the most related set of topics according to the actual meaning will be the significance of this research.	Concept Embedded Topic Modeling Technique	NA	2018
Reshmi Gopalakrishna Pillai	The ability to detect human stress and relaxation is central for timely diagnosing stress-related diseases, ensuring customer satisfaction in services and managing human-centric applications such as traffic management. Traditional methods employ stress measuring scales or physiological monitoring which may be intrusive and inconvenient. Instead, the ubiquitous nature of social media can be leveraged to identify stress and relaxation. In this PhD research, we introduce an improved method to detect expressions of stress and relaxation in social media content. It uses word sense vectors for word sense disambiguation to improve the performance of the first ever lexicon-based stress/relaxation detection algorithm TensiStrength. Experimental results show that TensiStrength with word sense disambiguation performs better than the original TensiStrength and state-of-the-art machine learning methods in terms of Pearson's correlation and accuracy. We also suggest a novel, word-vector based approach for detecting causes of stress and relaxation in social media content.	Detection of Strength and Causal Agents of Stress and Relaxation for Tweets	NA	2018
Xuanxing Yang	Recently with dynamic information being ubiquitous on the Web, there have been efforts to extend RDF and SPARQL for representing streaming information and continuous querying functionalities, respectively. While existing works focusing on formalization and implementation of continuous querying process over RDF streams, little attention has deserved the problem of querying the complex temporal correlations among RDF stream tuples and the effective, scalable implementation of RDF stream processing system. To fill this gap, in this paper we propose CT-SPARQL and AIMRS, a language specifying for the compositional stream patterns and an architecture for adaptive incremental maintenance of RDF stream tuples defined in CT-SPARQL. We believe that this work will benefit a wide range of real-time analyzing and future predicting applications.	Query for Streaming Information: Dynamic Processing and Adaptive Incremental Maintenance of RDF Stream	NA	2018
Laura Koesten	Structured data is becoming critical in every domain and its availability on the web is increasing rapidly. Despite its abundance and variety of applications, we know very little about how people find data, understand it, and put it to use. This work aims to inform the design of data discovery tools and technologies from a user centred perspective. We aim to better understand what type of information supports people in finding and selecting data relevant for their respective tasks. We conducted a mixed-methods study looking at the workflow of data practitioners when searching for data. From that we identified textual summaries as a key element that supports the decision making process in information seeking activities for data. Based on these results we performed a mixed-methods study to identify attributes people consider important when summarising a dataset. We found text summaries are laid out according to common structures, contain four main information types, and cover a set of dataset features. We describe follow-up studies that are planned to validate these findings and to evaluate their applicability in a dataset search scenario.	A User Centred Perspective on Structured Data Discovery	NA	2018
Dawid Wisniewski	The process of ontology authoring is inseparably connected with the quality assurance phase. One can verify the maturity and correctness of a given ontology by evaluating how many competency questions give correct answers. Competency questions are defined as a set of questions expressed in natural language that the finished ontology should be able to answer to correctly. Although this method can easily indicate what is the development status of an ontology, one has to translate competency questions from natural language into an ontology query language. This task is very hard and time consuming. To overcome this problem, my PhD thesis focuses on methods for automatically checking answerability of competency questions for a given ontology and proposing SPARQL-OWL query (OWL-aware SPARQL query) for each question where it is possible to create the query. Because the task of automatic translation from competency questions to SPARQL-OWL queries is a novel one, besides a method, we have proposed a new benchmark to evaluate such translation.	Automatic Translation of Competency Questions into SPARQL-OWL Queries	NA	2018
Maulik R. Kamdar	The vision of the Semantic Web has stimulated the development of Web-scale architectures for discovering implicit associations from multiple heterogeneous data and knowledge sources. In biomedicine, using W3C-established standards and Linked Data principles, data publishers have transformed and linked several datasets to create a huge web of Life Sciences Linked Open Data (LSLOD). However, mining the LSLOD cloud is still very difficult and often impossible for biomedical researchers due to several challenges: structural heterogeneity, lack of vocabulary reuse, inconsistencies, and incompleteness. To discover drug-adverse reaction associations and their mechanistic explanations, I have developed a novel architecture that combines information retrieval and association discovery. The architecture demonstrates favorable AUROC statistics against baseline methods in pharmacovigilance, and provides confidence values on underlying biological mechanisms. I quantify the several challenges associated with mining the LSLOD cloud for biomedical applications through an empirical analysis of more than 40 different sources. Ideally, the architecture can be extended in other domains to realize the goal of implicit association discovery.	Mining the Web of Life Sciences Linked Open Data for Mechanism-Based Pharmacovigilance	NA	2018
Isa Inuwa-Dutse	Contemporary social media networks can be viewed as a break to the early two-step flow model in which influential individuals act as intermediaries between the media and the public for information diffusion. Today's social media platforms enable users to both generate and consume online contents. Users continuously engage and disengage in discussions with varying degrees of interaction leading to formation of distinct online communities. Such communities are often formed at high-level either based on metadata, such as hashtags on Twitter, or popular content triggered by few influential users. These online communities often do not reflect true connectivity and lack the cohesiveness of traditional communities. In this study, we investigate real-time formation of temporal communities on Twitter. We aim at defining both high and low levels connections and to reveal the magnitude of clustering cohesion on temporal basis. Inspired by a real-life event center sitting arrangement scenario, the proposed method aims to cluster users into distinct and cohesive online temporal communities. Membership to a community relies on intrinsic tweet properties to define similarity as the basis for interaction networks. The proposed method can be useful for local event monitoring and clique-based marketing among other applications.	Modelling Formation of Online Temporal Communities	NA	2018
Varsha Bhat Kukkala	Social networks have been a popular choice of study, given the surge of online data on friendship networks, communication networks, collaboration networks etc. This popularity, however, is not true for all types of social networks. In the current work, we draw the reader's attention to a class of social networks which are investigated to a limited extent, classified as distributed sensitive social networks. It constitutes of networks where the presence or absence of edges in the network is distributedly known to a set of parties, who regard this information as their private data. Supply chain networks, informal networks such as trust network, advice network, enmity network, etc. are a few examples of the same. A major reason for the lack of any substantial study on these networks has been the unavailability of data. As a solution, we propose a privacy preserving approach to investigating these networks. We show the feasibility of using secure multiparty computation techniques to perform the required analysis, while preserving the privacy of every individual's data. The possible approaches that can be considered to ensure the design of efficient secure protocols are discussed such as efficient circuit design, ORAM based secure computation, use of oblivious data structures, etc. The results obtained in the direction of secure network analysis algorithms are also presented.	Privacy Preserving Distributed Analysis of Social Networks	NA	2018
Angela Bonifati:Romain Wuillemot	It is our great pleasure to welcome you to the WWW 2018 Panels Track. We have selected three panel proposals among the ones received. We also took in account the coverage of the different areas related to WWW, inputs from the community with a shared document, as well as the suggestions of the authors of the research tracks in deciding the subjects of the three panels.	Panels Track Chairs' Welcome	NA:NA	2018
Davood Rafiei:Eugene Agichtein:Ricardo Baeza-Yates:Jon Kleinberg:Jure Leskovec	The Web's content has been going through major changes, triggered by multiple factors including changes in user demographic and authoring behaviour, a shift in device types that access the Web, and changes in common use cases of the Web. More specifically, the number of mobile internet users has surpassed the desktop users according to different statistics; a considerable portion of web use cases are in the form of social interactions rather than information seeking; and the authoring behaviour has transformed from compiling a page and linking resources to sharing content with like-minded followers and leaving likes and comments on posts. Those changes have influenced and are expected to shape the way the content is organized, searched, ranked and analyzed. This panel brings together researchers who have been working in different established areas related to web search and mining, web content and social network analysis, and semantics and knowledge management. The panel will draw from the experience of the panellists, dealing with changes in their respective fields. In the first (role-playing) round, each panellist will strongly take a side on where the changes are heading, arguing that one form of content will dominate in the near future. In the second round, the panellists will counter each other and will share their vision on what future holds in terms of research problems and directions. The members of the audience will participate, in a QA session with the panellists, bringing their own perspectives to the discussion.	The Shifting Landscape of Web Search and Mining: Past, Present, and Future	NA:NA:NA:NA:NA	2018
Boualem Benatallah:Fabio Casati	Cognitive services and conversational digital assistants are emerging as the engine that powers natural interactions between humans, software services, devices and "things" - supported by advances in AI and human computations. Not surprisingly, many large and small tech companies are rushing to occupy this space by providing platforms for building cognitive services and conversational bots. Digital assistants interact in a natural way (through text or voice) with both software and humans to get information and perform actions, from checking the weather to booking a restaurants and a cab ride, managing cloud resources, answering simple scientific questions, and preparing a decaf latte using IoT enabled coffee machines. User requests or tasks are often expressed in natural language, an interaction ensues to clarify the intent and the details, and the answer is sought - or the appropriate service or device is invoked - based on the cognitive service understanding. While the potential of this new wave of services is exciting, it also brings significant challenges: we are far away from the comfort of developing deterministic software that responds to API calls by invoking other APIs. Now we have to understand, guess, explore options, take decisions based on probabilistic models over a large set of possible intents and services, all while engaging with users. Doing so brings a large set of engineering challenges related to the development, training, tuning and evolution of such services. This panel will discuss such challenges and identify interesting opportunities for research as well as promising trends.	Panel on Cognitive Service Engineering	NA:NA	2018
Steffen Staab:Jens Lehmann:Ruben Verborgh	Structured Knowledge on the Web had an intriguing history before it has become successful. We briefly revisit this history, before we go into the longer discussion about how structured knowledge on the Web should be devised such that it benefits even more applications. Core to this discussion will be issues like trust, information infrastructure usability and resilience, promising realms of structured knowledge and principles and practices of data sharing.	Structured Knowledge on the Web 7.0	NA:NA:NA	2018
Eyhab Al-Masri:Marie-Christine Rousset	It is our great pleasure to welcome you to the WWW 2018 Workshops. This year's workshops of WWW 2018 feature a number of co-located workshops that are intended to provide a forum for researchers and practitioners in Web technologies to discuss and exchange positions on current and emergent Web topics. We received forty proposals from all around the world covering a broad range of topics. We evaluated them regarding relevance, quality, and novelty selecting eighteen full-day workshops and ten half-day workshops. We also took into account the coverage of the different areas related to WWW as well as the potential audience, to schedule them in two consecutive days with the minimal audience interest overlap.	Workshop Chairs' Welcome	NA:NA	2018
Leonidas Anthopoulos:Marijn Janssen:Vishanth Weerakkody	Following up the success of the past events at WWW2015, WWW2016 and WWW2017, the 4th AW4City 2018 aims to keep on attracting a significant international attention with regard to web applications for smart cities. More specifically, the aim of this workshop is to focus on the applications smart city component and more specifically on the design and implementation of web-based applications and Apps that deliver smart services or address smart city challenges. This year, the proposed workshop will emphasize on the contribution of web applications and Apps to citizen centricity. In the era of cities, municipal leaders, service and utility providers are making an important shift regarding thinking of people as customers and of customer experience. This shift is not a simple task since it demands a continuous service monitoring, assessment and improvement [1;2;3], which normally is based on accurate data analysis and appears as a thinking that makes government and providers more personal and responsive.	AW4City 2018 Chairs' Welcome & Organization	NA:NA:NA	2018
Zeenat Rehena:Marijn Janssen	In the last few years, the smart city concept resulted in the development and deployment of platforms for providing innovative services to improve sustainability and the living standards. These platforms integrate data collected from devices and citizen-generated data and thereafter employ big data analytics to create insights from the data. These platform enable the creation of context-aware Intelligent traffic management systems (ITMS), however the involvement of various actors at different stages hampers development. In this paper, we propose a framework to support sustainable traffic management system for providing better commute, safety and security during travel based on real-time information. The framework should help to integrate the activities performed by the various actors. The main key elements of this framework are Datasets, Traffic Management Analytics, Actors and Actions which are taken by these users. The framework helps to create an overall overview of the activities needed. In this way it can be used to improve the quality of the traffic flow, increase efficient use of resources, smooth and safe commute of the citizens.	Towards a Framework for Context-Aware Intelligent Traffic Management System in Smart Cities	NA:NA	2018
Auriol Degbelo:Tomi Kauppinen	Recent years have witnessed progress of public institutions in making their datasets available online, free of charge, for re-use. This notwithstanding, there is still a long way to go to put the power of data in the hands of citizens. This article suggests that transparency in the context of open government can be increased through web maps featuring: i) Application Programming Interfaces (APIs) which support app and data usage tracking; and (ii) 'transparency badges' which inform the users about the presence/absence of extra, useful contextual information. Eight examples of web maps are introduced as proof of concept for the idea. Designing and implementing these web maps has reminded of the need of interactive guidelines to help non-experts select vocabularies, and datasets to link to. The ideas presented are relevant to making existing open data more user friendly (and ultimately more usable).	Increasing Transparency through Web Maps	NA:NA	2018
Vaia Moustaka:Zenonas Theodosiou:Athena Vakali:Anastasis Kounoudes	As smart cities infrastructures mature, data becomes a valuable asset which can radically improve city services and tools. Registration, acquisition and utilization of data, which will be transformed into smart services, are becoming more necessary than ever. Online social networks with their enormous momentum are one of the main sources of urban data offering heterogeneous real-time data at a minimal cost. However, various types of attacks often appear on them, which risk users' privacy and affect their online trust. The purpose of this article is to investigate how risks on online social networks affect smart cities and study the differences between privacy and security threats with regard to smart people and smart living dimensions.	Smart Cities at Risk!: Privacy and Security Borderlines from Social Networking in Cities	NA:NA:NA:NA	2018
Gabriela Viale Pereira:Gregor Eibl:Peter Parycek	This paper presents the analysis of the SmartGov project as a case of smart technologies application, such as expert-based Fuzzy Cognitive Maps, social media applications and open data, to promote citizen engagement and support decision-making. The objective of this paper is to analyze the role of digital technologies as inputs to achieve smart city governance. The main results are illustrated in a framework that combines the smart city governance elements in a real case description.	The Role of Digital Technologies in Promoting Smart City Governance	NA:NA:NA	2018
Agnes Mainka:Tobias Siebenlist:Lisa Beutelspacher	Participatory smartphone apps empower citizens to interact with the city's administration. The purpose of this case study is to investigate the current state of participatory apps in Germany. The 29 apps that have been found can be categorized into four topics: Information Awareness, City Service, Transparency and Public Safety. Most citizen apps can be assigned to the category Travel & Local. None of the identified apps is based on open-source code, and the citizens' reports are not publicly visible, e.g., for other citizens. It is unclear to whom the data generated by citizens belong.	Citizen Participation: Case Study on Participatory Apps in Germany	NA:NA:NA	2018
Thivya Kandappu:Archan Misra:Desmond Koh:Randy Daratan Tandriansyah:Nikita Jaiman	Active citizenry, whereby citizens actively participate in reporting and addressing challenges in urban service delivery is a strategic goal of smart cities such as Singapore. In spite of the promise, we believe that the success of such large-scale nation-wide crowdsourcing deployments depend on the real-word user preferences and behavioral characteristics of citizens. In this paper, we first present our findings on behavioral preferences and key concerns of citizens regarding smart-city services via an opinion survey conducted with 1300 participants. We then propose a "citizen-controlled" urban services reporting platform where citizens actively report on the status of various municipal resources. We advocate the importance of matching user mobility patterns against task locations to make the platform more efficient (i.e., higher task completion rate and lower detour overhead).	A Feasibility Study on Crowdsourcing to Monitor Municipal Resources in Smart Cities	NA:NA:NA:NA:NA	2018
Leonidas Anthopoulos:Amel Attour	An increasing amount of applications can be located in several cities that attempt to deal with mobility issues like traffic management, transportation safety, congestion control, taxi booking, car sharing, carpooling etc. The aim of this work in progress article is to collect information with regard to carpooling applications and attempt to recognize the underlying business models.	Smart Transportation Applications' Business Models: A Comparison	NA:NA	2018
Pernilla Bergmark	This paper looks into the use of Information and Communication Technology (ICT) for Smart Sustainable Cities (SSC). It specifically points towards ICT's potential to help cities mitigate climate change and to support a 2ºC or lower trajectory and to involve citizens in city planning and when implementing solutions. The paper also focuses on the modelling, assessment methodologies and indicators for ICT and sustainability aspects of cities, not least with regards to climate change mitigation. Especially, it highlights the city GG emissions assessment standard by ITU, and the indicator sets from ITU and ISO and their enhancement. The paper emphasizes a sustainability and citizen-centric perspective, seeing ICT as instrumental in this respect. For assessments, the above mentioned ITU-T standard is considered reusable for impacts beyond global warming. For indicators, further research on outcome and impact indicators is suggested, and also strengthening of the socio-economic and cultural aspects.	Reflections Regarding ICT and a Citizen-centric Future Path of Smart Sustainable Cities: AW4City 2018 Keynote	NA	2018
Vinay Kandpal	Nagpur has emerged as the topmost smart city in India. In just five months, Nagpur has beaten other cities chosen before it to get the best implementation of smart city plan. A recent stock-taking exercise conducted by urban development ministry has revealed that Nagpur, though chosen as a smart city in September 2016 much after 33 smart cities in two previous rounds has achieved the best investment conversion ratio. India's smart city program hopes to revolutionize city life and improve the quality of life for India's urban population. In the absence of a zonal plan, many parts of Dehradun have witnessed haphazard development over the years, which has already caused much damage to the vision of a planned smart city. Smart City would require smart economy, bright people, smart organization, smart communication, smart engineering, smart transit, fresh environment and bright living. Nevertheless, with mass migration leading to basic problems, like water shortages and overcrowding, the rate at which these cities will be developed will be the key. Several initiatives are being led by the Government of India to convert 100 Cities into Smart Cities. Government to Actively Use PPP Route and Encourage FDI for Effective Implementation of Smart Cities Project in India.	A Case Study on Smart City Projects in India: An Analysis of Nagpur, Allahabad and Dehradun	NA	2018
Jie Tang:Michalis Vazirgiannis:Yuxiao Dong:Fragkiskos D. Malliaros:Michael Cochez:Mayank Kejriwal:Achim Rettinger	It is our great pleasure to welcome you to the 2018 International Workshop on Learning Representations for Big Networks ([email protected]). This is the third edition of the BigNet workshop series, following its inauguration at the 25th ACM International Conference on Information and Knowledge Management (CIKM 2016) and the second edition at the 26th International World Wide Web Conference (WWW 2017). Recent years have witnessed the emergence of network representation learning research. Different from the feature engineering process in conventional analysis, network representation learning, also know as network embedding, aims to learn the latent low-dimensional representations for objects in networks, such as nodes, links, and groups. Its ultimate objective is to encode networks' structural properties into the latent representations, benefiting all existing network mining tasks, such as node classification, link prediction, community detection, etc. In the BigNet 2018 workshop, we aim to provide a forum for presenting the most recent advances in network representation learning to unearth rich knowledge.	BigNet 2018 Chairs' Welcome & Organization	NA:NA:NA:NA:NA:NA:NA	2018
Ayushi Dalmia:Ganesh J:Manish Gupta	Recently there have been a large number of studies on embedding large-scale information networks using low-dimensional, neighborhood and community aware node representations. Though the performance of these embedding models have been better than traditional methods for graph mining applications, little is known about what these representations encode, or why a particular node representation works better for certain tasks. Our work presented here constitutes the first step in decoding the black-box of vector embeddings of nodes by evaluating their effectiveness in encoding elementary properties of a node such as page rank, degree, closeness centrality, clustering coefficient, etc. We believe that a node representation is effective for an application only if it encodes the application-specific elementary properties of nodes. To unpack the elementary properties encoded in a node representation, we evaluate the representations on the accuracy with which they can model each of these properties. Our extensive study of three state-of-the-art node representation models (DeepWalk, node2vec and LINE) on four different tasks and six diverse graphs reveal that node2vec and LINE best encode the network properties of sparse and dense graphs respectively. We correlate the model performance obtained for elementary property prediction tasks with the high-level downstream applications such as link prediction and node classification, and visualize the task performance vector of each model to understand the semantic similarity between the embeddings learned by various models. Our first study of the node embedding models for outlier detection reveals that node2vec and DeepWalk identify outliers well for sparse and dense graphs respectively. Our analysis highlights that the proposed elementary property prediction tasks help in unearthing the important features responsible for the given node embedding model to perform well for a given downstream task. This understanding would facilitate in picking the right model for a given downstream task.	Towards Interpretation of Node Embeddings	NA:NA:NA	2018
Ryan A. Rossi:Rong Zhou:Nesreen K. Ahmed	This paper presents a general inductive graph representation learning framework called DeepGL for learning deep node and edge features that generalize across-networks. In particular, DeepGL begins by deriving a set of base features from the graph (e.g., graphlet features) and automatically learns a multi-layered hierarchical graph representation where each successive layer leverages the output from the previous layer to learn features of a higher-order. Contrary to previous work, DeepGL learns relational functions (each representing a feature) that naturally generalize across-networks and are therefore useful for graph-based transfer learning tasks. Moreover, DeepGL naturally supports attributed graphs, learns interpretable inductive graph representations, and is space-efficient (by learning sparse feature vectors). In addition, DeepGL is expressive, flexible with many interchangeable components, efficient with a time complexity of O(|E|), and scalable for large networks via an efficient parallel implementation. Compared with recent methods, DeepGL is (1) effective for across-network transfer learning tasks and large (attributed) graphs, (2) space-efficient requiring up to 6x less memory, (3) fast with up to 182x speedup in runtime performance, and (4) accurate with an average improvement in AUC of 20% or more on many learning tasks and across a wide variety of networks.	Deep Inductive Network Representation Learning	NA:NA:NA	2018
Ivan Brugere:Tanya Y. Berger-Wolf	Networks are fundamental models for data used in practically every application domain. In most instances, several implicit or explicit choices about the network definition impact the translation of underlying data to a network representation, and the subsequent question(s) about the underlying system being represented. Users of downstream network data may not even be aware of these choices or their impacts. We propose a task-focused network model selection methodology which addresses several key challenges. Our approach constructs network models from underlying data and uses minimum description length (MDL) criteria for selection. Our methodology measures efficiency, a general and comparable measure of the network's performance of a local (i.e. node-level) predictive task of interest. Selection on efficiency favors parsimonious (e.g. sparse) models to avoid overfitting and can be applied across arbitrary tasks and representations. We show stability, sensitivity, and significance testing in our methodology.	Network Model Selection Using Task-Focused Minimum Description Length	NA:NA	2018
Giang Hoang Nguyen:John Boaz Lee:Ryan A. Rossi:Nesreen K. Ahmed:Eunyee Koh:Sungchul Kim	Networks evolve continuously over time with the addition, deletion, and changing of links and nodes. Although many networks contain this type of temporal information, the majority of research in network representation learning has focused on static snapshots of the graph and has largely ignored the temporal dynamics of the network. In this work, we describe a general framework for incorporating temporal information into network embedding methods. The framework gives rise to methods for learning time-respecting embeddings from continuous-time dynamic networks. Overall, the experiments demonstrate the effectiveness of the proposed framework and dynamic network embedding approach as it achieves an average gain of 11.9% across all methods and graphs. The results indicate that modeling temporal dependencies in graphs is important for learning appropriate and meaningful network representations.	Continuous-Time Dynamic Network Embeddings	NA:NA:NA:NA:NA:NA	2018
Andriy Nikolov:Peter Haase:Daniel M. Herzig:Johannes Trame:Artem Kozlov	Vector embedding models have recently become popular for encoding both structured and unstructured data. In the context of knowledge graphs such models often serve as additional evidence supporting various tasks related to the knowledge base population: e.g., information extraction or link prediction to expand the original dataset. However, the embedding models themselves are often not used directly alongside structured data: they merely serve as additional evidence for structured knowledge extraction. In the metaphactory knowledge graph management platform, we use federated hybrid SPARQL queries for combining explicit information stated in the graph, implicit information from the associated embedding models, and information extracted using vector embeddings in a transparent way for the end user. In this paper we show how we integrated RDF data with vector space models to construct an augmented knowledge graph to be used in customer applications.	Combining RDF Graph Data and Embedding Models for an Augmented Knowledge Graph	NA:NA:NA:NA:NA	2018
Richard Han:Jeremy Blackburn:Homa Hosseinmardi:Qin Lv:Bert Huang:Shivakant Mishra	It is our great pleasure to welcome you to the WWW 2018 Workshop on Computational Methods in Cybersafety, Online Harassment, and Misinformation. The theme of cybersafety is an important emerging research topic on the Internet that manifests itself daily as users navigate the Web and networked applications. After two successful workshops on cybersafety, the main goal of this third edition of this workshop on cybersafety is to build and grow the cybersafety research community by bringing together the leading researchers and practitioners from academia, industry, government, and research labs working in the general area of cybersafety to discuss the unique challenges in addressing various cybersafety issues and to share experiences, solutions, tools, and techniques. The focus is on the detection, prevention, and mitigation of various cybersafety issues, as well as education and promoting safe practices. The focus of this workshop is on computational methods in cybersafety, including new algorithms, tools, data mining techniques, analysis, systems, and applications for the detection, prevention and mitigation of various cybersafety issues, as well as education and promoting safe practices. Our program features two invited keynote speakers. We will have Dr. April Edwards, Vice President for Academic Affairs and Dean of the Faculty at Elmhurst College, speak about Racial and Gender Differences in Cyberbullying Behavior. And we will have Dr. Neil Shah, Research Scientist at Snap Inc., speak about Anomaly Detection on Large Social Graphs. We will also feature four contributed presentations and publications selected from papers submitted to our workshop.	International Workshop on Cybersafety Chairs' Welcome & Organization	NA:NA:NA:NA:NA:NA	2018
Anna Sapienza:Sindhu Kiranmai Ernala:Alessandro Bessi:Kristina Lerman:Emilio Ferrara	Widespread adoption of networking technologies has brought about tremendous economic and social growth, but also exposed individuals and organization to new threats from malicious cyber actors. Recent attacks by WannaCry and NotPetya ransomware crypto-worms, infected hundreds of thousands of computer systems world wide, compromising data and critical infrastructure. In order to limit their impact, it is, therefore, critical to detect---and even predict---cyber attacks before they spread. Here, we introduce DISCOVER, an early cyber threat warning system, that mines online chatter from cyber actors on social media, security blogs, and darkweb forums, to identify words that signal potential cyber attacks. We evaluate DISCOVER and find that it can identify terms related to emerging cyber threats with precision above $80%$. DISCOVER also generates a time line of related online discussions on different Web sources that can be useful for analyzing emerging cyber threats.	DISCOVER: Mining Online Chatter for Emerging Cyber Threats	NA:NA:NA:NA:NA	2018
Emeric Bernard-Jones:Jeremiah Onaolapo:Gianluca Stringhini	We set out to understand the effects of differing language on the ability of cybercriminals to navigate webmail accounts and locate sensitive information in them. To this end, we configured thirty Gmail honeypot accounts with English, Romanian, and Greek language settings. We populated the accounts with email messages in those languages by subscribing them to selected online newsletters. We also hid email messages about fake bank accounts in fifteen of the accounts to mimic real-world webmail users that sometimes store sensitive information in their accounts. We then leaked credentials to the honey accounts via paste sites on the Surface Web and the Dark Web, and collected data for fifteen days. Our statistical analyses on the data show that cybercriminals are more likely to discover sensitive information (bank account information) in the Greek accounts than the remaining accounts, contrary to the expectation that Greek ought to constitute a barrier to the understanding of non-Greek visitors to the Greek accounts. We also extracted the important words among the emails that cybercriminals accessed (as an approximation of the keywords that they possibly searched for within the honey accounts), and found that financial terms featured among the top words. In summary, we show that language plays a significant role in the ability of cybercriminals to access sensitive information hidden in compromised webmail accounts.	BABELTOWER: How Language Affects Criminal Activity in Stolen Webmail Accounts	NA:NA:NA	2018
Arpita Chakraborty:Yue Zhang:Arti Ramesh	The possibility of anonymity and lack of effective ways to identify inappropriate messages have resulted in a significant amount of online interaction data that attempt to harass, bully, or offend the recipient. In this work, we perform a preliminary linguistic study on messages exchanged using one such popular web/smartphone application---Sarahah, that allows friends to exchange messages anonymously. Since messages exchanged via Sarahah are private, we collect them when the recipient shares it on Twitter. We then perform an analysis of the different kinds of messages exchanged through this application. Our linguistic analysis reveals that a significant number of these messages (~20%) include inappropriate, hurtful, or profane language intended to embarrass, offend, or bully the recipient. Our analysis helps in understanding the different ways in which anonymous message exchange platforms are used and the different types of bullying present in such exchanges.	Understanding Types of Cyberbullying in an Anonymous Messaging Application	NA:NA:NA	2018
Savvas Zannettou:Barry Bradlyn:Emiliano De Cristofaro:Haewoon Kwak:Michael Sirivianos:Gianluca Stringini:Jeremy Blackburn	Over the past few years, a number of new "fringe" communities, like 4chan or certain subreddits, have gained traction on the Web at a rapid pace. However, more often than not, little is known about how they evolve or what kind of activities they attract, despite recent research has shown that they influence how false information reaches mainstream communities. This motivates the need to monitor these communities and analyze their impact on the Web's information ecosystem. In August 2016, a new social network called Gab was created as an alternative to Twitter. It positions itself as putting "people and free speech first", welcoming users banned or suspended from other social networks. In this paper, we provide, to the best of our knowledge, the first characterization of Gab. We collect and analyze 22M posts produced by 336K users between August 2016 and January 2018, finding that Gab is predominantly used for the dissemination and discussion of news and world events, and that it attracts alt-right users, conspiracy theorists, and other trolls. We also measure the prevalence of hate speech on the platform, finding it to be much higher than Twitter, but lower than 4chan's Politically Incorrect board.	What is Gab: A Bastion of Free Speech or an Alt-Right Echo Chamber	NA:NA:NA:NA:NA:NA:NA	2018
Inaya Lahoud:Elsa Cardoso:Nada Matta	Welcome to the 3rd Educational Knowledge Management (EKM) workshop, which takes place at the WWW18 conference, in Lyon, France. The first edition was in 2014 in conjunction with the International Conference on Knowledge Engineering and Knowledge Management (EKAW), which was held in Linköping, Sweden, and the second one with EKAW 2016, in Bologna, Italy. We received 6 papers from all around the world covering a broad range of topics. Each paper was reviewed by three members of the program committee. After the reviewing process, three papers were accepted for inclusion into the WWW proceedings volume, two of them as full papers and one as a short paper. The EKM2018 workshop will run as a half-day event, including the papers' presentation and two invited speaker sessions. A Best Paper Award will be assigned, and authors will be invited to submit an extended version of their work to a special issue that will be published as part of the "International Journal of Continuing Engineering Education and Lifelong Learning." The first paper Construction and Applications of TeKnowbase A Knowledge Base of Computer Science concepts' by Rajna Upadhyay, Ashutosh Bindal, Manjeet Kumar, and Maya Ramanath describes the development and evaluation of TeKnowbase, and how to use it in a variety of applications for learning a new topic, classification of technical text and querying and ranking computer science articles. The second one untitled "Ontology-based recommender system in higher education" by Charbel Obeid, Inaya Lahoud, Hicham El Khoury, and Pierre-Antoine Champin, is a position paper discussing an ontology-based recommender system to support a student's choice of major and university. The third paper "Automatic Generation of Quizzes from DBpedia According to Educational Standards" by Oscar Rodríguez Rocha and Catherine Faron Zucker focuses on educational quizzes. The authors present an approach to generate quizzes automatically from existing knowledge bases available on the Web of Linked Open Data (LOD), according to the official French educational standards.	3rd EKM Workshop Chairs' Welcome & Organization	NA:NA:NA	2018
Serge Garlatti:Jean Marie Gilliot:Sacha Kieffer:Jérôme Eneau:Genevieve Lameul:Partricia Serrano-Alvarado:Hala Skaf-Molli:Emmanuel Desmontils	NA	Open Learner Models, Trust and Knowledge Management for Life Long Learning	NA:NA:NA:NA:NA:NA:NA:NA	2018
Elsa Cardoso	Learning Analytics (LA) is a recent research field, in which Business Intelligence and Analytics techniques are applied to learners and their contexts, with the purpose of acquiring a greater insight about the entire learning process (including outcomes). In this talk, we explore the LA landscape, delving into the definitions, techniques, challenges, and lessons learned.	The Past, Present, and Future of Learning Analytics	NA	2018
Prajna Upadhyay:Ashutosh Bindal:Manjeet Kumar:Maya Ramanath	In this paper, we make two main contributions. First, we describe the construction and evaluation of TeKnowbase, a knowledge-base of technical concepts in computer science. And second, we show how to use TeKnowbase in a variety of applications, including, generation of pre-requisite concepts for learning a new topic, classification of technical text and querying and ranking computer science articles.	Construction and Applications of TeKnowbase: A Knowledge Base of Computer Science Concepts	NA:NA:NA:NA	2018
Charbel Obeid:Inaya Lahoud:Hicham El Khoury:Pierre-Antoine Champin	Academic advising is limited in its ability to assist students in identifying academic pathways. Selecting a major and a university is a challenging process rife with anxiety. Students at high school are not sure how to match their interests with their working future or major. Therefore, high school students need guidance and support. Moreover, students need to filter, prioritize and efficiently get appropriate information from the web in order to solve the problem of information overload. This paper represents an approach for developing ontology-based recommender system improved with machine learning techniques to orient students in higher education. The proposed recommender system is an assessment tool for students' vocational strengths and weaknesses, interests and capabilities. The main objective of our ontology-based recommender system is to identify the student requirements, interests, preferences and capabilities to recommend the appropriate major and university for each one.	Ontology-based Recommender System in Higher Education	NA:NA:NA:NA	2018
Oscar Rodríguez Rocha:Catherine Faron Zucker	Educational quizzes are a powerful and popular tool to test the knowledge acquired by a learner and also to deepen her/his knowledge about a specific subject in an informal and entertaining way. Their production is a time-consuming task that can be automated by taking advantage of existing knowledge bases available on the Web of Linked Open Data (LOD). For these quizzes to be useful to learners, they must be generated according to the knowledge and skills defined by official educational standards for each subject and school year. This paper shows an approach to generate quizzes automatically according to the official French educational standards, from two different knowledge bases. Likewise, we show an evaluation of both knowledge bases.	Automatic Generation of Quizzes from DBpedia According to Educational Standards	NA:NA	2018
Franz Baader:Brigitte Grau:Yue Ma	NA	HQA18 Workshop Chairs' Welcome & Organization	NA:NA:NA	2018
Eric Gaussier	Semantic annotation in the biomedical domain raises the problem of classifying texts with large-scale taxonomies, a problem sometimes referred to as extreme classification. In this presentation, we will give an overview of this problem and the main solutions proposed, with a focus on textual collections and the BioASQ challenge.	Semantic Annotation in the Biomedical Domain: Large-scale Classification and BioASQ	NA	2018
Andreas Both	In the past, the research, as well as industry, brought Question Answering (QA) into daily use. However, there is still an obvious gap between the claim of providing access to any--structured or unstructured--knowledge stored in the world using an interface fitting the demands of regular users. On the one hand side, implementing Question Answering Systems is still hard and time-consuming, on the other hand side, the QA community is still struggling on defining a common ground for collaboration across research fields regarding, for example, realistic benchmarks, maintainability, and broad coverage of knowledge sources. In the talk, challenges of Question Answering will be highlight w.r.t. hybrid QA, domain-specific QA, cross knowledge base QA, etc. Particularly the industry perspective is also presented while aiming at a Question Answering platform which can be assembled from industry components as well as components of the research community. First steps towards this long-term vision are provided by the Qanary framework and similar frameworks aiming at a collaborative approach for the development QA systems which should lead to effective implementations, improved research results as well as a platform economy for QA. Such a platform would industry lead to a tighter collaboration while academics would have the opportunity of accessing precious data for further improvements.	Towards Component-based, Domain-specific, Efficient Question Answering Systems	NA	2018
Phuong Le-Hong:Duc-Thien Bui	In this paper, we describe the development of an end-to-end factoid question answering system for the Vietnamese language. This system combines both statistical models and ontology-based methods in a chain of processing modules to provide high-quality mappings from natural language text to entities. We present the challenges in the development of such an intelligent user interface for an isolating language like Vietnamese and show that techniques developed for inflectional languages cannot be applied as is. Our question answering system can answer a wide range of general knowledge questions with promising accuracy on a test set.	A Factoid Question Answering System for Vietnamese	NA:NA	2018
Zhen Jia:Abdalghani Abujabal:Rishiraj Saha Roy:Jannik Strötgen:Gerhard Weikum	Answering complex questions is one of the challenges that question-answering (QA) systems face today. While complexity has several facets, question dimensions like temporal and spatial intents necessitate specialized treatment. Methods geared towards such questions need benchmarks that reflect the desired aspects and challenges. Here, we take a key step in this direction, and release a new benchmark, TempQuestions, containing 1,271 questions, that are all temporal in nature, paired with their answers. As a key contribution that enabled the creation of this resource, we provide a crisp definition for temporal questions. Most questions require decomposing them into sub-questions, and the questions are of a kind that they would be best evaluated on a combination of structured data and unstructured text sources. Experiments with two QA systems demonstrate the need for further research on complex questions.	TempQuestions: A Benchmark for Temporal Question Answering	NA:NA:NA:NA:NA	2018
Atsushi Otsuka:Kyosuke Nishida:Katsuji Bessho:Hisako Asano:Junji Tomita	We propose a novel Frequently Asked Question (FAQ) retrieval technique with a neural query expansion model. With the growth in Question Answering systems and mobile communications, FAQ retrieval systems have become widely used in site searches and call center support. However, FAQ retrieval often has lexical gaps between queries and answer documents. To bridge these gaps, we design a query expansion model on the basis of an Encoder-Decoder model as a type of deep neural network. The model learns the words that appear in answers for questions using Q&A pair documents and generates the expanded queries from inputted queries to retrieve answer documents. We evaluate our proposed technique in a multi-domain FAQ retrieval task. Experimental results show that our technique retrieves FAQs more accurately than the previous methods.	Query Expansion with Neural Question-to-Answer Translation for FAQ-based Question Answering	NA:NA:NA:NA:NA	2018
Franz Baader:Stefan Borgwardt:Walter Forkel	Finding suitable candidates for clinical trials is a labor-intensive task that requires expert medical knowledge. Our goal is to design (semi-)automated techniques that can support clinical researchers in this task. We investigate the issues involved in designing formal query languages for selecting patients that are eligible for a given clinical trial, leveraging existing ontology-based query answering techniques. In particular, we propose to use a temporal extension of existing approaches for accessing data through ontologies written in Description Logics. We sketch how such a query answering system could work and show that eligibility criteria and patient data can be adequately modeled in our formalism.	Patient Selection for Clinical Trials Using Temporalized Ontology-Mediated Query Answering	NA:NA:NA	2018
Martino Mensio:Giuseppe Rizzo:Maurizio Morisio	QA systems offer a human friendly interface to navigate through knowledge, which can range from encyclopedic to domain-specific. Generally, a QA system is designed to provide an answer to a specific question once (so-called single turn) and state-of-the-art systems reach nowadays robust performance in such a scenario. However, most of the interactions with QA systems are based on multiple handshakes of question/answer pairs, where the human being refines the questions further, while the system can collect the necessary information and generate a compelling final answer through multiple turns. In this paper, we investigate and experiment a multi-turn QA system that is suited to work given a particular domain of knowledge and configurable goals. Our approach models the entire dialogue as a sequence of turns, i.e. questions and answers, using a Recurrent Neural Network which is firstly trained to understand natural language, classifying entities and intents using prior knowledge of domain-specific interactions, and provide answers according to the domain used as background knowledge. We have compared our approach with state-of-the-art sequence-based intent classification using a well-known and standardized gold standard observing an increase of 17.16% of F1. Results show the robustness of the approach and the competitive results motivate the adoption in multi-turn QA scenarios.	Multi-turn QA: A RNN Contextual Approach to Intent Classification for Goal-oriented Systems	NA:NA:NA	2018
Brigitte Grau:Anne-Laure Ligozat	Question answering has been the focus of a lot of researches and evaluation campaigns, either for text-based systems (TREC and CLEF evaluation campaigns for example), or for knowledge-based systems (QALD, BioASQ). Few systems have effectively combined both types of resources and methods in order to exploit the fruitfulness of merging the two kinds of information repositories. The only evaluation QA track that focuses on hybrid QA is QALD since 2014. As it is a recent task, few annotated data are available (around 150 questions). In this paper, we present a question answering dataset that was constructed to develop and evaluate hybrid question answering systems. In order to create this corpus, we collected several textual corpora and augmented them with entities and relations of a knowledge base by retrieving paths in the knowledge base which allow to answer the questions. The resulting corpus contains 4300 question-answer pairs and 1600 have a true link with DBpedia.	A Corpus for Hybrid Question Answering Systems	NA:NA	2018
Dennis Diefenbach:Kamal Singh:Pierre Maret	In the last two decades a new part of the web grew significantly, namely the Semantic Web. It contains many Knowledge Bases (KB) about different areas like music, books, publications, live science and many more. Question Answering (QA) over KBs is seen as the most promising approach to bring this data to end-users. We describe WDAqua-core1, a QA service for querying RDF knowledge-bases. It is multilingual, it supports different RDF knowledge bases and it understands both full natural language questions and keyword questions.	WDAqua-core1: A Question Answering service for RDF Knowledge Bases	NA:NA:NA	2018
Sanjay Kamath:Brigitte Grau:Yue Ma	Extractive Question Answering (QA) focuses on extracting precise answers from a given paragraph to questions posed in natural language. Deep learning models are widely used to address this problem and can fetch good results, provided there exists enough data for learning. Such large datasets have been released in open domain, but not in specific domains, such as the medical domain. However, the medical domain has a great amount of resources such as UMLS thesaurus, ontologies such as SNOMED CT, and tools such as Metamap etc that could be useful. In this paper, we apply transfer learning for getting a DNN baseline system on biomedical questions and we study if structured resources can help in selecting the answers based on the recognition of the Expected Answer Type (EAT), which has been proved useful in open domain QA systems. This study relies on different representations for LAT and we study if gold standard answers and answers of our model have some positive impact from the LAT.	Verification of the Expected Answer Type for Biomedical Question Answering	NA:NA:NA	2018
Lora Aroyo:Gianluca Demartini:Anna Lisa Gentile:Chris Welty	It is our great pleasure to welcome you to the WWW 2018 Augmenting Intelligence with Humans-in-the-loop ([email protected]WWW2018), http://w3id.org/huml/HumL-WWW2018/ The workshop program includes two invited talks. Praveen Paritosh (Google Research) explores the right incentives to motivate human contribution to create knowledge resources. Elena Simperl (University of Southampton) surveys how humans and bots contribute together to the development of the Wikidata knowledge graph. Seven full papers and one short paper were accepted, covering a wide range of topics related to the efficient and effective combination of the strong sides of both machine and crowd computation. Empirical results were provided and discussed with respect to (1) methods for data quality ensurance and labeling task efficiency, (2) the role of gamification elements for improving crowd performance as well as the role of quantum mathematics to simulate human behavior.	Augmenting Intelligence with Humans-in-the-Loop ([email protected]) Chairs' Welcome & Organization	NA:NA:NA:NA	2018
Lora Aroyo:Chris Welty	AI and collective intelligence systems universally suffer from a deficiency of context. There are innumerable possible contexts that may possibly change the interpretation of some signal, that may change the proper response to some stimulus. For example, an image understanding system that does not recognize an arrest event in a zoomed image of a person's face. How is it possible to know there is more information, outside of what the system can access, that affects the interpretation of data The solution to the context problem in practice today is a pragmatic, engineering one: analyze errors (in recommendations, question answers, image recognition, etc.), classify the kinds of contextual information that caused the wrong behavior, find the most common type of context that causes errors, and add information about that kind of context to the system. Clearly this approach is neither general nor scalable, and ignores the infamous long tail of possible contextual information that may affect a system's understanding and its behavior. In this paper we outline a new, more general, approach to recognizing context. The approach is grounded in a fairly simple intuition: the mathematics underlying quantum mechanics is far more appropriate for modeling, and therefore simulating, human cognitive behavior than the standard toolset from classical statistics. Notions such as Heisenberg's uncertainty principle, superpositions of states, and entanglement have direct and measurable analogs in collective intelligence.	The Quantum Collective	NA:NA	2018
Praveen Paritosh	Dictionaries, encyclopedias, knowledge graphs, annotated corpora, library classification systems and world maps are all examples of human-curated knowledge resources that have been highly valuable to science as well as amortized across multiple large-scale systems in practice. Many of these were started and built even before a crowdsourcing research community existed. While the last decade has seen unprecedented growth in research and practice in building crowdsourcing systems to do increasingly complex tasks at scale, many of these resources are still woefully incompletelacking coverage in languages and subject matter domains. Moreover, many knowledge resources needed to fill other semantic gaps for artificial intelligence systems simply don't exist or arent being built. Why I argue that we don't have the right incentives, and that in order to improve the incentives, we have some fundamental scientific questions to answer. While building a large knowledge resource, we have little more than intuitions when it comes to estimating the reusability, maintainability, and long-term value of the effort. These make it difficult to fund or manage such projects, often requiring herculean personalities or fortunate businesses. Building or expanding a resource is often not seen as "sexy," which results in lack of resources to answer those questions in any principled manner. These problems begin to outline a new science of curation, making progress on which could help improve the discussion around and funding for building sorely needed knowledge resources.	The Missing Science of Knowledge Curation: Improving Incentives for Large-scale Knowledge Curation	NA	2018
Elena Simperl	Wikidata is one of most successful knowledge graphs ever created. It expresses knowledge in the form of subject-property-value statements accompanied by provenance information. A project of the Wikimedia Foundation, Wikidata is supported by a community of currently 19 thousand active users and 234 bots, who together are responsible for editing more than 45 million entities since the start of the project in 2012. This makes Wikidata a prime example for what human-in-the-loop technology can achieve. In this talk, we are going to present several studies that aim to understand the links between its socio-technical fabric and its success.	Loops of Humans and Bots in Wikidata	NA	2018
Amrapali Zaveri:Pedro Hernandez Serrano:Manisha Desai:Michel Dumontier	Crowdsourcing involves the creating of HITs (Human Intelligent Tasks), submitting them to a crowdsourcing platform and providing a monetary reward for each HIT. One of the advantages of using crowdsourcing is that the tasks can be highly parallelized, that is, the work is performed by a high number of workers in a decentralized setting. The design also offers a means to cross-check the accuracy of the answers by assigning each task to more than one person and thus relying on majority consensus as well as reward the workers according to their performance and productivity. Since each worker is paid per task, the costs can significantly increase, irrespective of the overall accuracy of the results. Thus, one important question when designing such crowdsourcing tasks that arise is how many workers to employ and how many tasks to assign to each worker when dealing with large amounts of tasks. That is, the main research questions we aim to answer is: 'Can we a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks'. Thus, we introduce a two-staged statistical guideline, CrowdED, for optimal crowdsourcing experimental design in order to a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks. We describe the algorithm and present preliminary results and discussions. We implement the algorithm in Python and make it openly available on Github, provide a Jupyter Notebook and a R Shiny app for users to re-use, interact and apply in their own crowdsourcing experiments.	CrowdED: Guideline for Optimal Crowdsourcing Experimental Design	NA:NA:NA:NA	2018
Alexandros Chortaras:Anna Christaki:Nasos Drosopoulos:Eirini Kaldeli:Maria Ralli:Anastasia Sofou:Arne Stabenau:Giorgos Stamou:Vassilis Tzouvaras	The transformation that has been accomplished in Cultural Heritage (CH) during the last decades has resulted in the production of vast amounts of content from many different cultural institutions, such as museums, libraries and archives. A large part of this rich content has been aggregated in digital platforms that serve as cross-domain hubs, which however offer limited usability and accessibility of content due to insufficient data and metadata quality. In our effort to make CH more accessible and reusable, we introduce WITH, an aggregation platform that provides enhanced services and enables human-computer collaboration for data annotations and enrichment. WITH excels existing cultural content aggregation platforms by advancing digital cultural data through the combination of artificial intelligence automation and creative user engagement, thus facilitating its accessibility, visibility, and re-use. In particular, by using image and free text analysis methodologies for automatic metadata enrichment, in accordance to the human expertise for enrichment and validation through crowdsourcing approaches with gamification elements, WITH combines the intelligence of humans and computers to improve the quality of digital cultural content and its presentation, establishing new ways of collaboration between cultural organizations and their audiences.	WITH: Human-Computer Collaboration for Data Annotation and Enrichment	NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Rafael Zequeira Jiménez:Laura Fernández Gallardo:Sebastian Möller	Crowdsourcing provides an exceptional opportunity for the rapid collection of human input for data acquisition and labelling. This approach have been adopted in multiple domains and researchers are now able to reach a demographically diverse audience at low cost. However, it remains the question of whether the results are still valid and reliable. Previous work have introduced different mechanisms to ensure data reliability in crowdsourcing. This work examines to which extend, "trapping question" or "outliers detection" assure reliable results to the detriment of, overloading task content with stimuli that are not of interest for the researcher, or by discarding data points that might be the true opinion of a worker. To this end, a speech quality assessment study have been conducted in a web crowdsourcing platform, following the ITU-T Rec. P.800. Workers assessed the speech stimuli of the database 501 from the ITU-T Rec. P.863. We examine results' validity in terms of correlations to previous ratings collected in laboratory. Our outcomes shows that neither of the techniques under investigation improve results accuracy by itself, but a combination of both. Our goal is to provide empirical guidance for designing experiments in crowdsourcing while ensuring data reliability.	Outliers Detection vs. Control Questions to Ensure Reliable Results in Crowdsourcing.: A Speech Quality Assessment Case Study	NA:NA:NA	2018
Ismini Lourentzou:Daniel Gruhl:Steve Welch	Domain-specific relation extraction requires training data for supervised learning models, and thus, significant labeling effort. Distant supervision is often leveraged for creating large annotated corpora however these methods require handling the inherent noise. On the other hand, active learning approaches can reduce the annotation cost by selecting the most beneficial examples to label in order to learn a good model. The choice of examples can be performed sequentially, i.e. select one example in each iteration, or in batches, i.e. select a set of examples in each iteration. The optimization of the batch size is a practical problem faced in every real-world application of active learning, however it is often treated as a parameter decided in advance. In this work, we study the trade-off between model performance, the number of requested labels in a batch and the time spent in each round for real-time, domain specific relation extraction. Our results show that the use of an appropriate batch size produces competitive performance, even compared to a fully sequential strategy, while reducing the training time dramatically.	Exploring the Efficiency of Batch Active Learning for Human-in-the-Loop Relation Extraction	NA:NA:NA	2018
Giorgio Maria Di Nunzio:Maria Maistro:Federica Vezzani	Supervised machine learning algorithms require a set of labelled examples to be trained; however, the labelling process is a costly and time consuming task which is carried out by experts of the domain who label the dataset by means of an iterative process to filter out non-relevant objects of the dataset. In this paper, we describe a set of experiments that use gamification techniques to transform this labelling task into an interactive learning process where users can cooperate in order to achieve a common goal. To this end, first we use a geometrical interpretation of Naïve Bayes (NB) classifiers in order to create an intuitive visualization of the current state of the system and let the user change some of the parameters directly as part of a game. We apply this visualization technique to the classification of newswire and we report the results of the experiments conducted with different groups of people: PhD students, Master Degree students and general public. Then, we present a preliminary experiment of query rewriting for systematic reviews in a medical scenario, which makes use of gamification techniques to collect different formulation of the same query. Both the experiments show how the exploitation of gamification approaches help to engage the users in abstract tasks that might be hard to understand and/or boring to perform.	A Gamified Approach to Naïve Bayes Classification: A Case Study for Newswires and Systematic Medical Reviews	NA:NA:NA	2018
Roberto Enea:Maria Teresa Pazienza:Andrea Turbati:Alessandro Colantonio	Creating ontologies is an essential while challenging task to be performed by either a human or a system: on one hand it is excessively burdensome for a human operator, on the other it is very complex also for a machine due to the not negligible amount of "uncertainty" that it must be able to manage. In the last years, some attempts have been made to automate this process, but at present, due to the large number of aspects to be covered in the automatic creation of an ontology (such as Domain terminology extraction, Concept discovery, Concept hierarchy derivation, ") satisfactory solutions have not been reached yet. In order to produce efficient tools for both creation and enrichment of ontologies, the participation of the human in such a process still seems necessary. Our approach, that foresees a broader framework for ontology learning, is based by first on the automatic extraction of triples from heterogeneous sources, then on the presentation of the most reliable triples to the human operator for validation purposes. The system provides the user with a series of graphical representations that can give him an overview of the level of uncertainty of the automatically generated ontology. Then provides the user with the possibility to perform SPARQL what-if queries, (i.e. assuming as true the triples filtered according to the level of confidence, the source and the structure of the triples).	How to Support Human Operator in "Uncertainty" Managing during the Ontology Learning Process	NA:NA:NA:NA	2018
Wei Sun:Ying Li:Anshul Sheopuri:Thales Teixeira	Making successful video advertisements has long been considered a combination of art and business acumen. In this work, we propose a system to assist human designers to produce more effective advertisements with predictable outcomes. We formalize this concept with a dynamic Bayesian network (DBN), where we represent the knowledge base with data collected from large-scale field experiments in a novel setting. Face and eye tracking which continuously measures viewers emotional responses and viewing interest on 169 television advertisements for 2334 participants, along with moment-to-moment branding activities in the advertisements are used to estimate the model. The resulting DBN represents relationships across advertisement content, viewers emotional responses, as well as effectiveness metrics such as ad avoidance, sharing and influence on purchase. Conditioned on the specified requirement on the ad, a human designer can draw high scoring samples from the DBN, which represent the optimized sequences of branding activities and entertainment content.	Computational Creative Advertisements	NA:NA:NA:NA	2018
Luis-Daniel Ibáñez:John Domingue:Pascal Molli	It is our great pleasure to welcome you to the WWW 2018 3rd International Workshop on Linked Data and Distributed Ledgers (LD-DL). We envision the workshop as a forum for researchers and practitioners from Distributed Ledgers and Linked Data to come together to discuss common challenges; propose solutions to shortcomings of existing architectures; and identify synergies for joint initiatives. The ultimate goal is the creation of a Web of Interoperable Ledgers. We received 6 submissions from all around the world. We evaluated them regarding relevance, quality, and novelty, selecting 3 short papers and 1 long paper (66% acceptance rate) --ScienceMiles - Digital currency for researchers--Can Blockchains and Linked Data Advance Taxation? --A distributed database with explicit semantics and chained RDF graphs--When trust saves energy: A Reference Framework for Proof of Trust (PoT) Blockchains. We hope that you will find the tutorial program interesting, providing you with a valuable opportunity to learn and share ideas with other researchers and practitioners from institutions around the world.	3rd International Workshop on Linked Data and Distributed Ledgers Chairs' Welcome & Organization	NA:NA:NA	2018
Leila Bahri:Sarunas Girdzijauskas	Blockchains are attracting the attention of many technical, financial, and industrial parties, as a promising infrastructure for achieving secure peer-to-peer (P2P) transactional systems. At the heart of blockchains is proof-of-work (PoW), a trustless leader election mechanism based on demonstration of computational power. PoW provides blockchain security in trusless P2P environments, but comes at the expense of wasting huge amounts of energy. In this research work, we question this energy expenditure of PoW under blockchain use cases where some form of trust exists between the peers. We propose a Proof-of-Trust (PoT) blockchain where peer trust is valuated in the network based on a trust graph that emerges in a decentralized fashion and that is encoded in and managed by the blockchain itself. This trust is then used as a waiver for the difficulty of PoW; that is, the more trust you prove in the network, the less work you do.	When Trust Saves Energy: A Reference Framework for Proof of Trust (PoT) Blockchains	NA:NA	2018
Mirek Sopek:Przemyslaw Gradzki:Witold Kosowski:Dominik Kuziski:Rafa Trójczak:Robert Trypuz	In this paper we present a new idea of creating a Blockchain compliant distributed database which exposes its data with explicit semantics, is easily and natively accessible, and which applies Blockchain securitization mechanisms to the RDF graph data model directly, without additional packaging or specific serialisation. Essentially, the resulting database forms the linked chain of named RDF graphs and is given a name: GraphChain. Such graphs can then be published with the help of any standard mechanisms using triplestores or as linked data objects accessible via standard web mechanisms using the HTTP protocol to make them available on the web. They can also be easily queried using techniques like SPARQL or methods typical to available RDF graphs frameworks (like rdflib, Apache Jena, RDF4J, OWL API, RDF HDT, dotnetRDF and others). The GraphChain concept comes with its own, OWL-compliant ontology that defines all the structural, invariant elements of the GraphChain and defines their basic semantics. The paper describes also a few simple, prototypical GraphChain implementations with examples created using Java, .NET/C# and JavaScript/Node.js frameworks.	GraphChain: A Distributed Database with Explicit Semantics and Chained RDF Graphs	NA:NA:NA:NA:NA:NA	2018
Michał R. Hoffman	Permissioned distributed ledgers (permissioned blockchains) supporting smart contracts that automatically adjust accounts and coordinate records among multiple parties, present a valid platform opportunity for establishing a fully digital tax regime. We propose a permissioned blockchain-based system aimed at eliminating some of the losses that tax authorities globally are currently struggling with. These multi-billion flaws manifest themselves as the tax gap, or the inability to collect the full amount that is owed by a given entity to a particular authority. Illegitimate or inefficient tax operations could be prevented with a global suite of smart contracts deployed on top of a consortium distributed ledger with on-chain governance. We also introduce the vision for a VAT Invoice 2.0 modelled as a Linked Data document. A tax reference generated by a smart contract would allow anyone with the right permissions to immediately investigate the entire commercial chain for any taxable item on an ontology-based tax document.	Can Blockchains and Linked Data Advance Taxation	NA	2018
Zeeshan Jan:Allan Third:Luis-Daniel Ibanez:Michelle Bachler:Elena Simperl:John Domingue	Peer-reviewing is a community-driven activity where volunteer researchers assess the work of other researchers. Peer-reviewing is an important and time-consuming activity that has very little recognition. This lack of incentive may lead to poor-quality reviews and frustration from researchers. In this paper, we envision ScienceMiles, a Blockchain-based platform to manage the incentivization of peer-reviewers through a crypto-currency.	ScienceMiles: Digital Currency for Researchers	NA:NA:NA:NA:NA:NA	2018
Dirk Ahlers:Erik Wilde:Rossano Schifanella:Jalal S. Alowibdi:Muhammad Zubair Shafiq	It is our great pleasure to welcome you to the 8th International Workshop on Location and the Web (LocWeb2018) at WWW 2018. LocWeb 2018 will continue a successful workshop series at the intersection of location-based services and Web architecture. It focuses on Web-scale services and systems facilitating location-aware information access as well as on Spatial Social Behavior Analytics on the Web as part of social computing. The location topic is seen as a cross-cutting issue equally concerning information access, semantics and standards, social analysis and mining, and Web-scale systems and services. The workshop is an integrated venue where location and spatio-social aspects can be discussed in depth with an interested community. New application areas for Web architecture, such as the Internet of Things (IoT) and the Web of Things (WoT), will lead to increasingly rich and large sets of applications for which location is highly relevant as the connection to the physical world. Location has high importance in Web-based designs, and it continues to provide challenging research questions.	LocWeb2018 Chairs' Welcome & Organization	NA:NA:NA:NA:NA	2018
Luca Rossi:Eric Boscaro:Andrea Torsello	The last decade has seen a huge expansion in the use of social media to extract data about human behaviour. While metadata and textual information have taken the lion's share as data sources for social media analysis, geotagged image-based platforms represent an unprecedented and as yet almost untapped source of data to analyse human behaviour and characterise the physical space we live in. In this paper we investigate the use of Instagram photos to analyse tourism consumption. We take the city of Venice (Italy) as a case study and we collect a dataset of about 90k photos taken between January 2014 and December 2015. Using computer vision techniques, we build a supervised classifier which assigns each photo to one of six different categories. We then observe how the frequency and spatial distribution of these categories varies with time. This in turn allows us to confirm the existence of a number of touristic hotspots associated with different events, such as Venice Carnival and Biennale. Our analysis also uncovers the existence of touristic flows associated with these events, such as the Folklore Line that marks the path of tourists from "Santa Lucia" railway station to "San Marco" square during the Carnival period. Overall, our findings confirm the effectiveness of the proposed framework to investigate tourism consumption using Instagram data.	Venice through the Lens of Instagram: A Visual Narrative of Tourism in Venice	NA:NA:NA	2018
Kendall Taylor:Kwan Hui Lim:Jeffrey Chan	Travelling and touring are popular leisure activities enjoyed by millions of tourists around the world. However, the task of travel itinerary recommendation and planning is tedious and challenging for tourists, who are often unfamiliar with the various Points-of-Interest (POIs) in a city. Apart from identifying popular POIs, the tourist needs to construct a travel itinerary comprising a subset of these POIs, and to order these POIs as a sequence of visits that can be completed within his/her available touring time. For a more realistic itinerary, the tourist also has to account for travelling time between POIs and visiting times at individual POIs. Furthermore, this itinerary should incorporate tourist preferences such as desired starting and ending POIs (e.g., POIs that are near the tourist's hotel) and a subset of must-see POIs (e.g., popular POIs that a tourist must visit). We term this the TourMustSee problem, which is based on a variant of the Orienteering problem. Following which, we propose the LP+M algorithm for solving the TourMustSee problem as an Integer Linear Program (ILP). Using a Flickr dataset of POI visits in seven touristic cities, we compare LP+M against various ILP-based baselines, and the results show that LP+M recommends better travel itineraries in terms of POI popularity, total POIs visited, total touring time utilized and must-visit POI(s) inclusion.	Travel Itinerary Recommendations with Must-see Points-of-Interest	NA:NA:NA	2018
Antonio La Salandra:Piero Fraternali:Darian Frajberg	Location-based mobile outdoor applications are powerful tools that can engage users in social and environmental tasks and support the emerging paradigm of citizen science. In this paper we present PeakLensVR, a virtual reality location-based mobile app that enables users to capture with their mobile phone panoramic mountain images and later visualize such images, enriched with metadata about the peaks visible from the capture point, with a low-end VR device. The goal of PeakLensVR is to harness the emerging trend of geo-located augmented and virtual reality applications to foster a community of environmentally conscious users who volunteer in the collection of mountain images for environment monitoring purposes.	A Location-Based Virtual Reality Application for Mountain Peak Detection	NA:NA:NA	2018
Guido Boella:Louise Francis:Elena Grassi:Axel Kistner:Andreas Nitsche:Alexey Noskov:Luigi Sanasi:Adriano Savoca:Claudio Schifanella:Ioannis Tsampoulatidis	In this paper we describe the advancement of WeGovNow, an Horizon 2020 European Union project involving twelve partners from Germany, Sweden, Greece, Italy and United Kingdom, aimed at using state-of-the-art digital technologies in community engagement platforms to involve citizens in decision making processes within their local neighbourhood. Different software components, both previously existing and developed specially for the project and covering separate aspects of community engagement, were integrated in a single web platform offering an homogeneous experience to the users. One of the main common threads beyond this integration process is the ability to collect crowd mapped information and show them back to the users in an engaging way on maps, harmonizing data coming from the different components and making the mapped space easily explorable.	WeGovNow: A Map Based Platform to Engage the Local Civic Society	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Eduardo Graells-Garrido:Diego Caro:Omar Miranda:Rossano Schifanella:Oscar F. Peredo	People fulfill their informational needs through smartphones, however, little is known regarding how the urban fabric and the activities that take place in it affect the usage of mobile applications. In this regard, starting from an anonymized dataset of Deep Packet Inspection (DPI) data from the largest telecommunications operator in Chile, we focus on the following questions: What are the most popular applications used in the city Where are they spatially clustered When does an application is more frequently used And How does the urban context and the mobility patterns relate to application usage As a result, we observed that specific applications present high spatial clustering, while the most popular services are geographically dispersed throughout the entire city. Clusters appear in places of high floating population; however, hotspots vary in space depending on the application. Interestingly, we found that commuting plays an important role, both in terms of rush hours and transportation infrastructure. We present a discussion on these results, focusing on how the physical space and the daily commuting routine affect the pattern of data consumption and represent an important aspect in mobile users behavioral studies.	The WWW (and an H) of Mobile Application Usage in the City: The What, Where, When, and How	NA:NA:NA:NA:NA	2018
Martin Atzmueller:Sabrina Gaito:Roberto Interdonato:Rushed Kanawati:Christine Largeron:Matteo Magnani:Alessandra Sala	Attributed network models have seen an increasing success in recent years, thanks to their informative power and to their ability to model complex networked relations that characterize most real-world phenomena. Their use has been attractive to communities in different disciplines such as computer science, physics, social science, as well as in interdisciplinary research environments. The use of such models has been also supported by the increasing easiness in collecting multirelational data from the Web, e.g., from online social media platforms, crowdsourced data, online knowledge bases; within this view, the World Wide Web is an inestimable source of information, which can be conveniently represented with feature-rich network models, e.g., enclosing temporal aspects of the data, quantitative and/or qualitative properties of nodes, different relations between a common set of entities, different existence probabilities, or modeling connection between different entity types.	International Workshop on Mining Attributed Networks (MATNET 2018) Chairs' Welcome	NA:NA:NA:NA:NA:NA:NA	2018
Amani H. B. Eissa:Mohamed E. El-Sharkawi:Hoda M. O. Mokhtar	Social networks can be modeled as attributed networks whose nodes represent users, edges represent relationships among users (e.g. friendship/follow) and attribute vectors hold properties of nodes and/or edges. In this paper, we consider friends' recommendation based on interest-based communities generated from topic based attributed social networks (TbASN). In our model, an attribute vector is not just a container for explicit users' profile data that is stored in social network's dataset, but rather holds topic vectors that are derived from analyzing the implicit interest of users' that are aggregated from his/her posts on the social network (e.g. tweets in Twitter, posts in Facebook). In our framework, topics of interest are represented as a hierarchy of topics (Topics/Subtopics) forming hierarchical interest-based communities. Users within each interest-based community are clustered according to their profile features (age, location, education etc.). Those clusters are later used in recommendations where recommendations target members of the same cluster to guarantee the quality and coherence of recommendations. In addition, we propose a recommendation selection approach to handle the large number of recommended candidates. The main advantage of the proposed approach is that it considers multiple criteria for candidate selection including the number of common communities, the resemblance in basic features, as well as network proximity. In addition to recommending friends of similar interests, frequent pattern mining is used to discover frequently occurring interests in order to be used in recommending communities for users to join. Although our approach is generic and can be applied to most of the existing social networks, we used Twitter as our target social network.	Towards Recommendation Using Interest-Based Communities in Attributed Social Networks	NA:NA:NA	2018
Jihwan Lee:Sunil Prabhakar	Network embedding aims to learn low-dimensional vector representations for nodes in a network that preserve structural characteristics. It has been shown that such representations are helpful in several graph mining tasks such as node classification, link prediction, and community detection. Some recent works have attempted to extend the approach to attributed networks in which each node is associated with a set of attribute values. They have focused on homophily relationships by forcing nodes with similar attribute values to obtain similar vector representations. This is unnecessarily restrictive and misses the opportunity to harness other types of relationships revealed by patterns in attribute values of connected nodes for learning insightful relationships. In this paper, we propose a new network attributed embedding framework called A3embed that is aware of attribute associations. A3embed favors significant attribute associations, not merely homophily relationships, which contributes to its robustness to diverse attribute vectors and noisy links. The experimental results on real-world datasets demonstrate that the proposed framework achieves better performance on different graph mining tasks compared to existing models.	A3embed: Attribute Association Aware Network Embedding	NA:NA	2018
Rajesh Sharma:Danilo Montesi	One of the important problems in the domain of network science is the community detection. In the past, various topological based community detection algorithms have been proposed. Recently, researchers have taken into account at- tributes of the nodes while proposing community detection algorithms. In this work, we investigate if the nodes in a community, identified through topology based algorithms al- so exhibit attribute similarity. Using four different kinds of similarity metrics, we analyse the attribute similarity of the nodes within the communities derived using five different types of topological based community detection algorithms. Based on our analysis of three real social network datasets, we found on an average of 50% attribute similarity among the nodes in the communities.	Investigating Similarity of Nodes' Attributes in Topological Based Communities.	NA:NA	2018
Ryuta Matsuno:Tsuyoshi Murata	Network embedding is a method for converting nodes in a network into low dimensional vectors, preserving its structure and the similarities among the nodes. Embedding is widely used in many applications, e.g., social network analysis and knowledge discovery. Because of its wide usage, many studies have been proposed, such as DeepWalk, LINE and node2vec. These works are designed for single-layer networks, however, real world networks often possess not just one, but multiple types of connections. Hence it is more appropriate to represent them as multiplex networks, which consist of multiple layers each of which represents one type of relationship. Embedding multiplex networks is difficult because all layer structures have to be taken into consideration. In this paper, we propose MELL, a novel embedding method for multiplex networks, which incorporates an idea of layer vector that captures and characterizes each layer's connectivity. This method exploits the overall structure effectively, and embeds both directed and undirected multiplex networks, whether their layer structures are similar or complementary. We focus on link prediction tasks and test our method and other baseline methods using five data sets from different domains. The results show that our method outperforms all of the baseline methods for all of the data sets.	MELL: Effective Embedding Method for Multiplex Networks	NA:NA	2018
Manish Kumar:Anurag Singh:Hocine Cherifi	When an epidemic occurs, it is often impossible to vaccinate the entire population due to limited amount of resources. Therefore, it is of prime interest to identify the set of influential spreaders to immunize, in order to minimize both the cost of vaccine resource and the disease spreading. While various strategies based on the network topology have been introduced, few works consider the influence of the community structure in the epidemic spreading process. Nowadays, it is clear that many real-world networks exhibit an overlapping community structure, in which nodes are allowed to belong to more than one community. Previous work shows that the numbers of communities to which a node belongs is a good measure of its epidemic influence. In this work, we address the effect of nodes in the neighborhood of the overlapping nodes on epidemics spreading. The proposed immunization strategy provides highly connected neighbors of overlapping nodes in the network to immunize. The whole process requires information only at the node level and is well suited to large-scale networks. Extensive experiments on four real-world networks of diverse nature have been performed. Comparisons with alternative local immunization strategies using the fraction of the Largest Connected Component (LCC) after immunization,show that the proposed method is much more efficient. Additionally, it compares favorably to global measures such as degree and betweenness centrality.	An Efficient Immunization Strategy Using Overlapping Nodes and Its Neighborhoods	NA:NA:NA	2018
Lisette Espín-Noboa:Claudia Wagner:Fariba Karimi:Kristina Lerman	Relational inference leverages relationships between entities and links in a network to infer information about the network from a small sample. This method is often used when global information about the network is not available or difficult to obtain. However, how reliable is inference from a small labeled sample How should the network be sampled, and what effect does it have on inference error How does the structure of the network impact the sampling strategy We address these questions by systematically examining how network sampling strategy and sample size affect accuracy of relational inference in networks. To this end, we generate a family of synthetic networks where nodes have a binary attribute and a tunable level of homophily. As expected, we find that in heterophilic networks, we can obtain good accuracy when only small samples of the network are initially labeled, regardless of the sampling strategy. Surprisingly, this is not the case for homophilic networks, and sampling strategies that work well in heterophilic networks lead to large inference errors. This finding suggests that the impact of network structure on relational classification is more complex than previously thought.	Towards Quantifying Sampling Bias in Network Inference	NA:NA:NA:NA	2018
Henry Soldano:Guillaume Santini:Dominique Bouthinon:Sophie Bary:Emmanuel Lazega	In two-mode networks there are two kinds of vertices, i.e objects, each being possibly described with a proper attribute set. This means that to select a subnetwork according to vertex descriptions we have to consider a pair of vertex subsets. A common technique is to extract from a network an essential subnetwork, the core subgraph of the network. Formal Concept Analysis and closed pattern mining were previously applied to networks with the purpose of reducing extensions of patterns to be core subgraphs. To apply this methodology to two-mode networks, we need to consider the two vertex subsets of two-mode cores and define accordingly abstract closed bi-patterns. Each component of a bi-pattern is then associated to one mode. We also show that the same methodology applies to hub-authority cores of directed networks in which each vertex subset is associated to a role (in or out). We illustrate the methodology both on a two-mode network of epistemological data and on a directed advice network of lawyers.	Bi-Pattern Mining of Two Mode and Directed Networks	NA:NA:NA:NA:NA	2018
Didier Henry:Erick Stattner:Martine Collard	Today, social media are one of the fastest ways to have access to information related to several topics. Indeed, a diffused information on these supports can travel thousands of kilometres in only few seconds contrary to an article posted on a news site. Despite the fact that a large variety of studies have been conducted to understand how fast and how scale information spreads in social media, we observe that they have not yet been interested in the geographical aspect. In this paper, we perform a geographical and temporal analysis of Twitter trends spread between May and June 2017. We introduce interesting patterns which deal with the paths taken by information between countries. In addition, we observe relevant results by taking into account the topic. Finally, we conclude and give perspectives of research of this work.	Information Propagation Routes between Countries in Social Media	NA:NA:NA	2018
Issam Falih:Nistor Grozavu:Rushed Kanawati:Younès Bennani	Graph clustering techniques are very useful for detecting densely connected groups in large graphs. Many existing graph clustering methods mainly focus on the topological structure, but ignore the vertex properties. Existing graph clustering methods have been recently extended to deal with nodes attribute. First we motivate the interest in the study of this issue. Then we review the main approaches proposed to deal with this problem. We propose a comparative study of some existing attributed network community detection algorithm on both synthetic data and on real world data.	Community detection in Attributed Network	NA:NA:NA:NA	2018
Martin Atzmueller:Alvin Chin:Christoph Trattner	Bienvenue! It is our great pleasure to welcome you to the WWW 2018 International Workshop on Modeling Social Media (MSM'2018) - Applying Machine Learning and AI for Modeling Social Media. This is our 9th edition of our workshop. Social networks such as Facebook, Twitter, and LinkedIn have paved the way for generating huge amount of diverse, streaming bit data in a short period of time. Such social media data require the application of big data analytics to produce meaningful information to both information consumers and data generators. Machine learning and AI techniques are particularly effective in situations where deep and predictive insights need to be uncovered from such social media data sets that are large, diverse and fast changing. The workshop aims to address machine learning and AI methods, frameworks, algorithms, and the applications and evaluation of these approaches on social media, big data and the web. We received 11 papers from all around the world covering a broad range of topics, and we accepted 8 papers resulting in a 72% acceptance rate. We evaluated them regarding relevance, quality, and novelty, selecting 4 full papers and 4 short papers. Each paper was reviewed by 3 reviewers and then decisions were made from the reviews and the workshop chairs.	International Workshop on Modeling Social Media (MSM 2018) Chairs' Welcome & Organization	NA:NA:NA	2018
Hong Wei:Hao Zhou:Jangan Sankaranarayanan:Sudipta Sengupta:Hanan Samet	The tweet count prediction of a local spatial region is to forecast the number of tweets that are likely to be posted from that area over a relatively short period of time. It has many applications such as human mobility analysis, traffic planning, and abnormal event detection. In this paper, we formulate tweet count prediction as a spatiotemporal sequence forecasting problem and design an end-to-end convolutional LSTM based network with skip connection for this problem. Such a model enables us to exploit the unique properties of spatiotemporal data, consisting of not only the temporal characteristics such as temporal closeness, period and trend properties but also spatial dependencies. Our experiments on the city of Seattle, WA as well as a larger city of New York City show that the proposed method consistently outperforms the competitive baseline approaches.	Residual Convolutional LSTM for Tweet Count Prediction	NA:NA:NA:NA:NA	2018
Ashwini Tonge:Cornelia Caragea	Online image sharing in social networking sites such as Facebook, Flickr, and Instagram can lead to unwanted disclosure and privacy violations, when privacy settings are used inappropriately. Despite that social networking sites allow users to set their privacy preferences, this can be cumbersome for the vast majority of users. In this paper, we explore privacy prediction models for social media that can automatically identify private (or sensitive) content from images, before they are shared online, in order to help protect users' privacy in social media. More precisely, we study "deep" visual features that are extracted from various layers of a pre-trained deep Convolutional Neural Network (CNN) as well as "deep" image tags generated from the CNN. Experimental results on a Flickr dataset of thousands of images show that the deep visual features and deep image tags can successfully identify images' private content and substantially outperform previous models for this task.	On the Use of "Deep" Features for Online Image Sharing	NA:NA	2018
Yutaro Miura:Fujio Toriumi:Toshiharu Sugawara	We propose a model of a social networking service (SNS) with diminishing marginal utility in the framework of evolutionary computing and present our investigation on the effect of diminishing marginal utility on the dominant structure of strategies in all agents. SNSs such as Twitter and Facebook have been growing rapidly, but why they are prospering is unknown. SNSs have the characteristics of a public goods game because they are maintained by users posting many articles that incur some cost and because users can also be free riders, who just read articles. Thus, a number of studies aimed at understanding the conditions or mechanisms that keep social media thriving theoretically by introducing the meta-rewards game, which is a variation of a public goods game. The meta-rewards games assume constant marginal utility, meaning that the rewards by receiving comments increase linearly according to the number of comments, but describing the psychological rewards of humans is often inappropriate. In this paper, we present our modification of the model using the diminishing marginal utility and our comparison of the experimental results with those of the original meta-rewards game. We demonstrate that the structure of dominant strategies of all agents in our game is quite different from that in the original meta-rewards game and is more reasonable to explain the users' behavior in SNSs because their efforts in SNSs are limited even if they have many friends.	Evolutionary Learning Model of Social Networking Services with Diminishing Marginal Utility	NA:NA:NA	2018
Mehwish Nasim:Andrew Nguyen:Nick Lothian:Robert Cope:Lewis Mitchell	Content polluters, or bots that hijack a conversation for political or advertising purposes are a known problem for event prediction, election forecasting and when distinguishing real news from fake news in social media data. Identifying this type of bot is particularly challenging, with state-of-the-art methods utilising large volumes of network data as features for machine learning models. Such datasets are generally not readily available in typical applications which stream social media data for real-time event prediction. In this work we develop a methodology to detect content polluters in social media datasets that are streamed in real-time. Applying our method to the problem of civil unrest event prediction in Australia, we identify content polluters from individual tweets, without collecting social network or historical data from individual accounts. We identify some peculiar characteristics of these bots in our dataset and propose metrics for identification of such accounts. We then pose some research questions around this type of bot detection, including: how good Twitter is at detecting content polluters and how well state-of-the-art methods perform in detecting bots in our dataset.	Real-time Detection of Content Polluters in Partially Observable Twitter Networks	NA:NA:NA:NA:NA	2018
Vedant Nanda:Hemank Lamba:Divyansh Agarwal:Megha Arora:Niharika Sachdeva:Ponnurangam Kumaraguru	Selfies have become a prominent medium for self-portrayal on social media. Unfortunately, certain social media users go to extreme lengths to click selfies, which puts their lives at risk. Two hundred and sixteen individuals have died since March 2014 until January 2018 while trying to click selfies. It is imperative to be able to identify dangerous selfies posted on social media platforms to be able to build an intervention for users going to extreme lengths for clicking such selfies. In this work, we propose a convolutional neural network based classifier to identify dangerous selfies posted on social media using only the image (no metadata). We show that our proposed approach gives an accuracy of $98%$ and performs better than previous methods.	Stop the KillFies! Using Deep Learning Models to Identify Dangerous Selfies	NA:NA:NA:NA:NA:NA	2018
Javier Sanz-Cruzado:Sofía M. Pepa:Pablo Castells	Link prediction has mainly been addressed as an accuracy-targeting problem in the social networks field. We discuss different perspectives on the problem considering other dimensions and effects that the link prediction methods may have on the social network where they are applied. Specifically, we consider the structural effects the prediction can have if the predicted links are added to the network. We consider further utility dimensions beyond prediction accuracy, namely novelty and diversity. We discuss the adaptation, for this purpose, of specific network, novelty and diversity metrics from social network analysis, recommender systems, and information retrieval.	Structural Novelty and Diversity in Link Prediction	NA:NA:NA	2018
Gaurav Bhatt:Aman Sharma:Shivam Sharma:Ankush Nagpal:Balasubramanian Raman:Ankush Mittal	Identifying the veracity of a news article is an interesting problem while automating this process can be a challenging task. Detection of a news article as fake is still an open question as it is contingent on many factors which the current state-of-the-art models fail to incorporate. In this paper, we explore a subtask to fake news identification, and that is stance detection. Given a news article, the task is to determine the relevance of the body and its claim. We present a novel idea that combines the neural, statistical and external features to provide an efficient solution to this problem. We compute the neural embedding from the deep recurrent model, statistical features from the weighted n-gram bag-of-words model and handcrafted external features with the help of feature engineering heuristics. Finally, using deep neural layer all the features are combined, thereby classifying the headline-body news pair as agree, disagree, discuss, or unrelated. Through extensive experiments, we find that the proposed model outperforms all the state-of-the-art techniques including the submissions to the fake news challenge.	Combining Neural, Statistical and External Features for Fake News Stance Identification	NA:NA:NA:NA:NA:NA	2018
Marco Brambilla:Stefano Ceri:Florian Daniel:Marco Di Giovanni:Andrea Mauri:Giorgia Ramponi	Knowledge in the world continuously evolves, and ontologies are largely incomplete, especially regarding data belonging to the so-called long tail. We propose a method for discovering emerging knowledge by extracting it from social content. Once initialized by domain experts, the method is capable of finding relevant entities by means of a mixed syntactic-semantic method. The method uses seeds, i.e. prototypes of emerging entities provided by experts, for generating candidates; then, it associates candidates to feature vectors built by using terms occurring in their social content and ranks the candidates by using their distance from the centroid of seeds, returning the top candidates. Our method can run iteratively, using the results as new seeds. as new seeds. In this paper we address the following research questions: (1) How does the reconstructed domain knowledge evolve if the candidates of one extraction are recursively used as seeds (2) How does the reconstructed domain knowledge spread geographically (3) Can the method be used to inspect the past, present, and future of knowledge (4) Can the method be used to find emerging knowledge	Iterative Knowledge Extraction from Social Networks	NA:NA:NA:NA:NA:NA	2018
Alípio Jorge:João Vinagre:Pawel Matuszyk:Myra Spiliopoulou	It is our great pleasure to welcome you to the WWW 2018 Workshop on Online Recommender Systems and User Modeling (ORSUM). We have received eleven submissions covering highly relevant topics in the research related to recommender systems and user modeling. During this workshop its participants will have the opportunity to see eight presentations corresponding to the accepted papers and to discuss the recent advances on these topics both with authors and other researchers in the audience. The topics of the talks include, among others, location and news recommendation, local models for online recommendations, recommendations' diversity, page optimization, context-aware recommender systems, crowdsourcing and incremental matrix factorization methods.	ORSUM Chairs' Welcome & Organization	NA:NA:NA:NA	2018
Julien Subercaze:Christophe Gravier:Frederique Laforest	Real-time recommendation of Twitter users based on the content of their profiles is a very challenging task. Traditional IR methods such as TF-IDF fail to handle efficiently large datasets. In this paper we present a scalable approach that allows real time recommendation of users based on their tweets. Our model builds a graph of terms, driven by the fact that users sharing similar interests will share similar terms. We show how this model can be encoded as a compact binary footprint, that allows very fast comparison and ranking, taking full advantage of modern CPU architectures. We validate our approach through an empirical evaluation against the Apache Lucene's implementation of TF-IDF. We show that our approach is in average two hundred times faster than standard optimised implementation of TF-IDF with a precision of 58%. The work presented here has been published in The Web Intelligence Journal.	Real-time, Scalable, Content-based Twitter Users Recommendation	NA:NA:NA	2018
George Karypis	Recommender systems are designed to identify the items that a user will like or find useful based on the user's prior preferences and activities. These systems have become ubiquitous and are an essential tool for information filtering and (e-)commerce. Over the years, collaborative filtering, which derive these recommendations by leveraging past activities of groups of users, has emerged as the most prominent approach for solving this problem. This talk will present some of our recent work towards improving the performance of collaborative filtering-based recommender systems and understanding some of their fundamental limitations and characteristics. It will start by analyzing how the ratings that users provide to a set of items relate to their ratings of the set's individual items and, using these insights, will present rating prediction approaches that utilize distant supervision. It will then discuss extensions to approaches based on sparse linear and latent factor models that postulate that users' preferences are a combination of global and local preferences, which are shown to lead to better user modeling and as such improved prediction performance. Finally, the talk will conclude by discussing what can be accurately predicted by latent factor approaches and by analyzing the estimation error of sparse linear and latent factor models and how its characteristics impacts the performance of top N recommendation algorithms.	Recent Advances in Recommender Systems: Sets, Local Models, Coverage, and Errors	NA	2018
Eiman Aldahari:Vivek Shandilya:Sajjan Shiva	Crowdsourcing is an approach whereby employers call for workers online with different capabilities to process a task for monetary reward. With a vast amount of tasks posted every day, satisfying the workers, employers, and service providers who are the stakeholders of any crowdsourcing system is critical to its success. To achieve this, the system should address three objectives: (1) match the worker with suitable tasks that fit the worker's interests and skills and raise the worker's rewards and rating, (2) give the employer more acceptable solutions with lower cost and time and raise the employer's rating, and (3) raise the rate of accepted tasks, which will raise the aggregated commissions to the service provider and improve the average rating of the registered users (employers and workers) accordingly. For these objectives, we present a mechanism design that is capable of reaching holistic satisfaction using a multi-objective recommendation system. In contrast, all previous crowdsourcing recommendation systems are designed to address one stakeholder who could be either the worker or the employer. Moreover, our unique contribution is to consider each stakeholder to be self serving. Considering selfish behavior from every stakeholder, we provide a more qualified recommendation for each stakeholder.	Crowdsourcing Multi-Objective Recommendation System*	NA:NA:NA	2018
Michele Zanitti:Sokol Kosta:Jannick Sørensen	Recommender systems (RS) have seen widespread adoption across the Internet. However, by emphasizing personalization through the optimization of accuracy-focused metrics, over-personalization may emerge, with negative effects on the user experience. A countermeasure to the problem is to diversify recommendations. In this paper, we present a solution that addresses the problem in the context of a movie application domain. The solution enhances diversity on four related dimensions, namely global coverage, local coverage, novelty, and redundancy. The proposed solution is designed to diversify users profiles, modeled on categorical preferences, within the same group in the recommendation filtering. We evaluate our approach on the Movielens dataset and show that our algorithm yields better results compared to random selection distant neighbors and performs comparably to one of the current state of the art solutions.	A User-Centric Diversity by Design Recommender System for the Movie Application Domain	NA:NA:NA	2018
Leonardo Cella	Traditional collaborative filtering, and content-based approaches attempt to learn a static recommendation model in a batch fashion. These approaches are not suitable in highly dynamic recommendation scenarios, like news recommendation and computational advertisement. Due to this well-known limitation, in the last decade a lot of efforts have been spent over the study of online learning techniques. Currently, a lot of attention has been devoted to improvements on the theoretical guarantees, without caring too much about computational cost and memory footprint. However, in the era of big-data content features tend to be high-dimensional, which leads to a direct challenge for traditional on-line learning algorithms (e.g., multi-armed bandits) since these are mostly designed for low-dimensional feature spaces. In this work we face the aforementioned problem, investigating an approximated context-aware bandit learner. Our model takes into account the problem of finding the actual low-dimensional manifold spanned by data content-features. In particular, we propose to store the covariance matrix of the previously seen contexts in a compressed space, without losing too much in terms of recommendation quality. With this work we provide an overview over the main properties, describe the adopted techniques, and report on preliminary experimental results on a synthetic dataset. We also discuss a drawback of the proposed method that may appear in typical scenarios and suggest future research avenues.	Efficient Context-Aware Sequential Recommender System	NA	2018
Gabriele Sottocornola:Panagiotis Symeonidis:Markus Zanker	In the context of news recommendations, many time-aware approaches were proposed. These approaches have tried to capture the recency of news with respect to their short life span, by using either decaying weights on past articles or even forgetting them. However, most of these approaches have missed to consider sessions, which encapsulate inside them the articles that a user has interacted with in a short time period. In this paper, we provide news recommendations based on user sessions to reveal their short-term intentions. We also combine content-based with collaborative filtering to deal with the severe data sparsity problem that exists in our real-life data set. We have experimentally seen that the users' interests evolve over time and that our strategies can adapt fast to these changes.	Session-based News Recommendations	NA:NA:NA	2018
Jia Wang:Yungang Feng:Elham Naghizade:Lida Rashidi:Kwan Hui Lim:Kate Lee	Studying large, widely spread Twitter data has laid the foundation for many novel applications from predicting natural disasters and epidemics to understanding urban dynamics. Recent studies have focused on exploring people's emotional response to their urban environment, e.g., green spaces versus built up areas, through analysing the sentiment of tweets within that area. Since green spaces have the capacity to improve citizen's well-being, we developed a system that is capable of recommending green spaces to users. Our system is unique in the sense that the recommendations are tailored with regard to users' preferred activity as well as the degree of positive sentiments in each green space. We show that the incoming flow of tweets can be used to refine the recommendations over time. Furthermore, We implemented a web-based, user-friendly interface to solicit user inputs and display recommendation results.	Happiness is a Choice: Sentiment and Activity-Aware Location Recommendation	NA:NA:NA:NA:NA:NA	2018
Weiru Zhang:Chao Wei:Xiaonan Meng:Yi Hu:Hao Wang	Modern search engines present result pages composed of two most prominent types of information: sponsored and organic search results. The whole-page results must satisfy user's information inquiry while sponsored ad alongside the search results has become a key monetization strategy for the platform. Against the backdrop of this situation, a basic question has received comparatively little attention: how many ads are good enough to get higher user satisfaction and better monetization Most search engines always display a fixed number of ads or use heuristic rules to determine the number of ads. In this paper, we formulate the task of finding the best number of ads into a linear programming optimization problem, for which we propose a novel online algorithm to solve. We have conducted several offline experiments and tested our approach in Alibaba E-commerce platform. The experimental results show that the platform could achieve higher revenue and more clicks simultaneously by the proposed algorithm.	The Whole-Page Optimization via Dynamic Ad Allocation	NA:NA:NA:NA:NA	2018
Susan C. Anyosa:João Vinagre:Alípio M. Jorge	Recommender systems try to predict which items a user will prefer. Traditional models for recommendation only take into account the user-item interaction, usually expressed by explicit ratings. However, in these days, web services continuously generate auxiliary data from users and items that can be incorporated into the recommendation model to improve recommendations. In this work, we propose an incremental Matrix Co-factorization model with implicit user feedback, considering a real-world data-stream scenario. This model can be seen as an extension of the conventional Matrix Factorization that includes additional dimensions to be decomposed in the common latent factor space. We test our proposal against a baseline algorithm that relies exclusively on interaction data, using prequential evaluation. Our experimental results show a significant improvement in the accuracy of recommendations, after incorporating an additional dimension in three music domain datasets.	Incremental Matrix Co-factorization for Recommender Systems with Implicit Feedback	NA:NA:NA	2018
Marie Al-Ghossein:Talel Abdessalem:Anthony Barré	With the explosion of the volume of user-generated data, designing online recommender systems that learn from data streams has become essential. These systems rely on incremental learning that continuously update models as new observations arrive and they should be able to adapt to drifts in real-time. User preferences evolve over time and tracking their evolution is not an easy task. In addition to the low number of observations available per user, the preferences change at different moments and in different ways for each individual. In this paper, we propose a novel approach based on local models to address this problem. Local models are known for their ability to capture diverse preferences among user subsets. Our approach automatically detects the drift of preferences that leads a user to adopt a behavior closer to the users of another subset, and adjusts the models accordingly. Our experiments on real world datasets show promising results and prove the effectiveness of using local models to adapt to changes in user preferences.	Dynamic Local Models for Online Recommendation	NA:NA:NA	2018
Rémy Cazabet:Andrea Passarella:Giulio Rossetti:Fabrizio Silvestri	NA	OSNED 2018 Chairs' Welcome & Organization	NA:NA:NA:NA	2018
Gaku Morio:Katsuhide Fujita	Large-scale online civic engagements (OCEs) with more than 100 participants have become possible due to recent developments in online social media technology. OCEs have the potential to achieve consensus building and collective decision-making with a large number of citizens, which is difficult to achieve in face-to-face contexts. However, most users in a large-scale OCE are rarely constantly active. Therefore, an important problem for the activation of a large-scale OCE is to facilitate the discussion by predicting which citizens will have significant influence in the discussion. This paper examines the activation prediction problem in a large-scale OCE. We propose a novel influence model based on the impulse response of activity histories and argumentative pressures, as well as an effective testing algorithm. The experimental results demonstrate that the proposed models with impulse response and the lexical pressures show better accuracy compared with baselines. In addition, the testing time required by the proposed method can be reduced significantly by employing a node-cutting algorithm.	Predicting Argumentative Influence Probabilities in Large-Scale Online Civic Engagement	NA:NA	2018
Caitlin Gray:Lewis Mitchell:Matthew Roughan	Modelling information cascades over online social networks is important in fields from marketing to civil unrest prediction, however the underlying network structure strongly affects the probability and nature of such cascades. Even with simple cascade dynamics the probability of large cascades are almost entirely dictated by network properties, with well-known networks such as Erdos-Renyi and Barabasi-Albert producing wildly different cascades from the same model. Indeed, the notion of 'superspreaders' has arisen to describe highly influential nodes promoting global cascades in a social network. Here we use a simple model of global cascades to show that the presence of locality in the network increases the probability of a global cascade due to the increased vulnerability of connecting nodes. Rather than 'super-spreaders', we find that the presence of these highly connected 'super-blockers' in heavy-tailed networks in fact reduces the probability of global cascades, while promoting information spread when targeted as the initial spreader.	Super-blockers and the Effect of Network Structure on Information Cascades	NA:NA:NA	2018
Clemens Deusser:Nora Jansen:Jan Reubold:Benjamin Schiller:Oliver Hinz:Thorsten Strufe	Social media interaction happens in a broad variety of context and magnitude. The vast majority of posts cause little to no discussion, while some start trends and become viral. We study the virality, explicitly of "Buzzes" - posts that evoke intense interaction over a short period of time, as they have been observed frequently, some- times with severe consequences for individuals and companies in the physical world. Early detection of a Buzz may help mitigate or prevent negative consequences of large scale social media outrage against companies or persons, by giving them a chance to react at an early stage. Collecting a labeled set of over 100,000 posts on Facebook pages, we first explore properties that define a Buzz using logistic regres- sion. This method helps us to interpret the results and derive prac- tical recommendations. We subsequently train classifiers and apply machine learning based classification techniques to demonstrate the potential capabilities of automated prediction. We achieve high recall with moderate precision, where feature boosting on broad feature sets yields the most promising results. Our study reveals that Buzzes are well described by a high num- ber of comments from previously passive users, a high number of likes given to comments, and a prolonged discussion period - properties that can be used to distinguish inconsequential posts from potentially volatile ones.	Buzz in Social Media: Detection of Short-lived Viral Phenomena	NA:NA:NA:NA:NA:NA	2018
Qiu Fang Ying:Dah Ming Chiu:Srinivasan Venkatramanan:Xiaopeng Zhang	In this paper, we study the posting behavior of OSN users, in particular the posting frequency and temporal patterns, and consider possible interpretations of how users use the platform. At the aggregate (macro) level, we find two distinct peaks, one during morning working hours, and one in the evening. The morning peak is more pronounced for frequent posters, while the evening peak is pronounced in the remaining users. We postulate that this difference results from qualitatively different usage of the OSN platform (e.g. for work, with customers, etc.) than purely social interactions (e.g., friends, family, etc.). We also study user posting behavior at an individual (micro) level and apply LDA to cluster user temporal patterns, interpret our results. Our study provides possibly new insights into user activity in today's OSNs, and suggests a framework for profiling users based on their posting activities. In the process, we provide a novel application of LDA, to temporal user posting behavior by equating the time epochs of posts to words in documents. We believe our approach will complement other methods of user profiling based on static demographic information and friendship network information.	Profiling OSN Users Based on Temporal Posting Patterns	NA:NA:NA:NA	2018
Sebastian Schams:Jan Hauffa:Georg Groh	To improve the quality of communication in Online Social Networks and Media (OSNEM), we envision a system that models a person's contributive social capital (CSC), which encompasses their competence, trustworthiness, and social responsibility. Having the CSC score available may inspire social behavior and mutual support. The system is based on three pillars: the analysis of OSNEM activity, interactions in virtual social capital market systems, and personal endorsements. In this paper we present our investigations regarding the first pillar. To obtain a dataset, we ran an experiment where 165 participants interacted on a custom social networking platform and assessed each other. Ground truth data was derived from these assessments. The dataset shows characteristics that are similar to larger OSNs. With different machine learning algorithms we investigated the hypothesis that contributive social capital can be extracted from network properties and networking activity, which were assessed with features such as the number of contributions of each participant. The prediction of contributive social capital showed an improvement over the baseline. A ranking of the participants following their predicted CSC scores showed a moderate correlation with the ranking according to the ground truth assessment. We also investigated the relative importance of the features for the analysis, and the effect of excluding inactive users to better understand network dynamics on a micro level. The selected features are also available in most other OSNEM platforms, like Facebook and Twitter. This allows a large-scale application of our investigations.	Analyzing a User's Contributive Social Capital Based on Acitivities in Online Social Networks and Media	NA:NA:NA	2018
Mauro Coletto:Claudio Lucchese:Salvatore Orlando	The popularity of online social platforms has also determined the emergence of violent and abusive behaviors reflecting real life issues into the digital arena. Cyberbullying, Internet banging, pedopornography, sexting are examples of these behaviors, as witnessed in the social media environments. Several studies have shown how to approximately detect those behaviors by analyzing the social interactions and in particular the content of the exchanged messages. The features considered in the models basically include detection of o ensive language through NLP techniques and vocabularies, social network structural measures and, if available, user context information. Our goal is to investigate those users who adopt offensive language and hate speech in Twitter by analyzing their profile pictures. Results show that violent people smile less and they are dominating by anger, fear and sadness.	Do Violent People Smile: Social Media Analysis of their Profile Pictures	NA:NA:NA	2018
Stefano Cresci:Marinella Petrocchi:Angelo Spognardi:Stefano Tognazzi	We envisage a revolutionary change in the approach to spambot detection: instead of taking countermeasures only after having collected evidence of new spambot mischiefs, in a near future techniques will be able to anticipate the ever-evolving spammers.	From Reaction to Proaction: Unexplored Ways to the Detection of Evolving Spambots	NA:NA:NA:NA	2018
Chiara Boldrini:Mustafa Toprak:Marco Conti:Andrea Passarella	Ego networks have proved to be a valuable tool for understanding the relationships that individuals establish with their peers, both in offline and online social networks. Particularly interesting are the cognitive constraints associated with the interactions between the ego and the members of their ego network, whereby individuals cannot maintain meaningful interactions with more than 150 people, on average. In this work, we focus on the ego networks of journalists on Twitter, and we investigate whether they feature the same characteristics observed for other relevant classes of Twitter users, like politicians and generic users. Our findings are that journalists are generally more active and interact with more people than generic users. Their ego network structure is very aligned with reference models derived from the social brain hypothesis and observed in general human ego networks. Remarkably, the similarity is even higher than the one of politicians and generic users ego networks. This may imply a greater cognitive involvement with Twitter than with other social interaction means. Moreover, the ego networks of journalists are much stabler than those of politicians and generic users, and the ego-alter ties are often information-driven.	Twitter and the Press: an Ego-Centred Analysis	NA:NA:NA:NA	2018
Laura Koesten:Elena Demidova:Vadim Savenkov:John Breslin:Oscar Corcho:Stefan Dietze:Elena Simperl	The web of data has seen tremendous growth recently. New forms of structured data have emerged in the form of web markup, such as schema.org and web tables. Exploiting these rich, heterogeneous and evolving data sources has become increasingly important for many different types of applications, including (federated) search, question answering and fact verification. The objective of the PROFILES & DATA:SEARCH Workshop is to bring together researchers and practitioners interested in the development of data search techniques, data profiling, and dataset retrieval on the web. This includes looking at the specifics of data-centric information seeking behaviours, understanding interaction challenges in data search on the web, and analysing the cognitive processes involved in the consumption of structured data by users. At the same time, we aim to discuss technologies addressing data search including semantics, information retrieval for web data (ranking algorithms and indexing), in particular in the context of decentralised and distributed systems, such as the web. We are interested in approaches to analyse, characterise and discover data sources. We want to facilitate a discussion around data search across formats and domain-specific applications. The PROFILES & DATA:SEARCH Workshop includes papers on a variety of topics such as profiling and data search, including querying and searching for structured data, profiling applications for cultural heritage, as well as data quality improvements through schema inference, content analysis and communities.	PROFILES & DATA: SEARCH International Workshop on Profiling and Searching Data on the Web Chairs' Welcome & Organization	NA:NA:NA:NA:NA:NA:NA	2018
Aidan Hogan	Graphs are being increasingly adopted as a flexible data model in scenarios (e.g., Google's Knowledge Graph, Facebook's Graph API, Wikidata, etc.) where multiple editors are involved in content creation, where the schema is ever changing, where data are incomplete, where the connectivity of resources plays a key rolescenarios where relational models traditionally struggle. But with this flexibility comes a conceptual cost: it can be difficult to summarise and understand, at a high level, the content that a given graph contains. Hence profiling graphs becomes of increasing importance to extract order, a posteriori, from the chaotic processes by which such graphs are generated. This talk will motivate the use of graphs as a data model, abstract recent trends in graph data management, and then turn to the issue of profiling and summarising graphs: what are the goals of such profiling, the principles by which graphs can be summarised, the main techniques by which this can/could be achieved The talk will emphasise the importance of profiling graphs while highlighting a variety of open research questions yet to be tackled.	Profiling Graphs: Order from Chaos	NA	2018
Maarten de Rijke	Over the years, search engines have developed to return a broad range of retrievable items, from documents to answers, people, locations, and products. Research datasets are increasingly being turned in retrievable items too. This raises a number of interesting challenges. Starting from the user end (What do users want from datasets) to increasing the retrievability of datasets (What kind of contextual information is available to enrich datasets so as to make the more easily retrieval) to optimizing rankers for datasets in the absence of large volumes of interaction data (How can we train learning to rank datasets algorithms in weakly supervised ways).	Learning to Search for Datasets	NA	2018
Emilia Kacprzak:Laura Koesten:Jeni Tennison:Elena Simperl	The amount of data generated and published on the web is increasing rapidly, but search for structured data on the web still presents challenges. In this paper we explore dataset search by analysing queries specifically generated for this work through a crowdsourcing experiment and comparing them to a search log analysis of queries on data portals. The change in search environment together with the task we gave people altered the generated queries. We found that queries issued in our experiment were much longer than search queries for datasets on data portals. They further contained seven times more mentions of geospatial and of temporal information and are more likely to be structured as questions. These insights can be used to tailor search functionalities to the particular information needs and characteristics of dataset search.	Characterising Dataset Search Queries	NA:NA:NA:NA	2018
Mohamed Ben Ellefi:Odile Papini:Djamal Merad:Jean-Marc Boi:Jean-Philip Royer:Jérôme Pasquet:Jean-Christophe Sourisseau:Filipe Castro:Mohammad Motasem Nawaf:Pierre Drap	Cultural heritage (CH) resources are very heterogeneous since the information was collected from vast diversity of cultural sites and digitally recorded in different formats. With the progress of 3D technologies, photogrammetry techniques become the adopted solution for representing CH artifacts by turning photos from small finds, to entire landscapes, into accurate 3D models. To meet knowledge representation with cultural heritage photogrammetry, this paper proposes an ontology-profiling method for modeling a real case of archaeological amphorae. The ontological profile consists of all needed information to represent a CH resource including typology attributes, geo-spatial information and photogrammetry process. An example illustrating the applicability of this profiling method to the problem of CH resources conceptualization is presented. We also outline our perspectives for using ontologies in data-driven science, in particular on modeling a complete pipeline that manages both the photogrammetric process and the archaeological knowledge.	Cultural Heritage Resources Profiling: Ontology-based Approach	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Semih Yumusak:Andreas Kamilaris:Erdogan Dogdu:Halife Kodaz:Elif Uysal:Riza Emre Aras	The Semantic Web promotes common data formats and exchange protocols on the web towards better interoperability among systems and machines. Although Semantic Web technologies are being used to semantically annotate data and resources for easier reuse, the ad hoc discovery of these data sources remains an open issue. Popular Semantic Web endpoint repositories such as SPARQLES, Linking Open Data Project (LOD Cloud), and LODStats do not include recently published datasets and are not updated frequently by the publishers. Hence, there is a need for a web-based dynamic search engine that discovers these endpoints and datasets at frequent intervals. To address this need, a novel web meta-crawling method is proposed for discovering Linked Data sources on the Web. We implemented the method in a prototype system named SPARQL Endpoints Discovery (SpEnD). In this paper, we describe the design and implementation of SpEnD, together with an analysis and evaluation of its operation, in comparison to the aforementioned static endpoint repositories in terms of time performance, availability, and size. Findings indicate that SpEnD outperforms existing Linked Data resource discovery methods.	A Discovery and Analysis Engine for Semantic Web	NA:NA:NA:NA:NA:NA	2018
Sean Soderman:Anusha Kola:Maksim Podkorytov:Michael Geyer:Michael Gubanov	Variety of Big data is a significant impediment for anyone who wants to search inside a large-scale structured dataset. For example, there are millions of tables available on the Web, but the most relevant search result does not necessarily match the keyword-query exactly due to a variety of ways to represent the same information. Here we describe Hybrid.AI, a learning search engine for large-scale structured data that uses automatically generated machine learning classifiers and Unified Famous Objects (UFOs) to return the most relevant search results from a large-scale Web tables corpora. We evaluate it over this corpora, collecting 99 queries and their results from users, and observe significant relevance gain.	Hybrid.AI: A Learning Search Engine for Large-scale Structured Data	NA:NA:NA:NA:NA	2018
Zhiyu Chen:Haiyan Jia:Jeff Heflin:Brian D. Davison	Impoverished descriptions and convoluted schema labels are common challenges in data-centric tasks such as schema matching and data linking, especially when datasets can span domains. To address these issues, we consider the task of schema label generation. Typically, schema labels are created by dataset providers and are useful for users to understand a dataset. The motivation behind the task is that a lot of data linking systems require overlapping information between two datasets and rely on unique identifiers of schema labels. Moreover, it is common for schema labels in different datasets to have different identifiers even when they refer to the same concept. With no naming standard for schema labels, unintelligible labels are widely found in real-world datasets. For example, many schema labels contain abbreviations and compound nouns that hinder automated matching of attributes in corresponding datasets. Through schema label generation, more common (and thus understandable) schema labels can be provided to allow for broader schema matches in contexts such as dataset search and data linking. We develop a variety of features based on analysis of dataset content to enable machine learning methods to recommend useful labels. We test our approach on two real-world data collections and demonstrate that our method is able to outperform the alternative approach.	Generating Schema Labels through Dataset Content Analysis	NA:NA:NA:NA	2018
Sebastian Neumaier:Lörinc Thurnay:Thomas J. Lampoltshammer:Tomá Knap	The present work describes the ADEQUATe platform: a framework to monitor the quality of (Governmental) Open Data catalogs, to re-publish improved and linked versions of the datasets and their respective metadata descriptions, and to include the community in the quality improvement process. The information acquired by the linking and (meta)data improvement steps is then integrated in a semantic search engine. In the paper, we first describe the requirements of the platform, which are based on focus group interviews and a web-based survey. Second, we use these requirements to formulate the goals and show the architecture of the overall platform, and third, we showcase the potential and relevance of the platform to resolve the requirements by describing exemplary user journeys exploring the system. The platform is available at: https://www.adequate.at/	Search, Filter, Fork, and Link Open Data: The ADEQUATe platform: data- and community-driven quality improvements	NA:NA:NA:NA	2018
Pinelopi Troullinou:Mathieu d'Aquin:Ilaria Tiddi	This volume of proceedings presents the papers from the 2nd edition of the interdisciplinary workshop Re-coding Black Mirror, held on April 24, 2018 in Lyon, France and co-located with The WEB Conference (WWW2018). Participating to the topical debate of data ethics and algorithmic governance, Re-coding Black Mirror offers the research community tools to reflect on its role in the construction of the technological future and the potential societal implications. The workshop becomes a venue for computer scientists, data scientists and social scientists to create bridges of knowledge. The complexity of the societal phenomena emerging from the development in web technologies urge for interdisciplinary collaboration. Following the slightly futuristic approach to technology of the British-made sci-fi series Black Mirror, we called scientists to create their dystopic scenarios developed from their own existing technologies. Through this thought experiment, researchers considered potential ethical and social risks of technological advancements offering in some cases possible solutions.	Re-coding Black Mirror Chairs' Welcome & Organization	NA:NA:NA	2018
Sven Helmer	We analyze the scenario depicted in the "Black Mirror" episode "Fifteen Million Merits" from an economic point of view, focusing on treating the attention of a user or consumer as a commodity. We continue by sketching the technological requirements for building such an economic framework, looking at advertisement platforms, payment schemes, and surveillance technology. As we show, a lot of the technology already exists and we expect the gaps to be filled in the very near future. Additionally, we briefly discuss the impact on social and work environments. While we believe that a scenario as extreme as shown in the episode is unlikely, we think that certain facets of it could find their way into our society.	May I Have Your Attention, Please: - Building a Dystopian Attention Economy	NA	2018
Tabea Tietz:Francesca Pichierri:Maria Koutraki:Dara Hallinan:Franziska Boehm:Harald Sack	What happens to our social media profiles when we die The episode "Be Right Back" as part of Netflix's series "Black Mirror" provides a possible scenario. A digital avatar is created to communicate with close relatives which learns from past social media activities of the deceased user. While the users entrust their social media content to one or more companies, even after their death, it may be reasonable to ask: What will the company really do with a deceased user's data: sell it to manipulate users or create advertisements In this paper we tackle the issues of ownership, ethics, and transparency of post mortem user data.	Digital Zombies - the Reanimation of our Digital Selves	NA:NA:NA:NA:NA:NA	2018
Martino Mensio:Giuseppe Rizzo:Maurizio Morisio	A future where the conversation with machines can potentially involve mutual emotions between the parties may be not so far in time. Inspired by the episode of Black Mirror "Be Right Back'' and Replika, a futuristic app that promises to be "your best friend'', in this work we are considering the positive and negative points of including an automated learning conversational agent inside the personal world of feelings and emotions. These systems can impact both single individuals and society, worsening an already critical situation. Our conclusion is that a regulation on the artificial emotional content should be considered before actually going beyond some one-way-only limits.	The Rise of Emotion-aware Conversational Agents: Threats in Digital Emotions	NA:NA:NA	2018
Kevin Koidl	Social Media has transformed modern day society. It can be argued that one of the main drivers behind this transformation are novel ways to effectively distribute content in a highly targeted fashion and at scale. Recently, this effectiveness has come under attack based on new phenomena known as Fake News, Filter Bubble and Echo Chambers. The public debate about the impact of these phenomena on modern day society ranges from demanding a complete social media shutdown to government intervention and censorship. Furthermore, it appears that Social Media Platform providers are not sure what countermeasures are needed to address these new challenges. The main concern is that Black Mirror like scenarios will emerge simply by allowing privately held companies decide what content is conforming to public norms leading to a distortion of values. This paper presents an alternative solution by focusing on empowering meaningful relationships and not content engagement. The main motivation behind the proposed solution is to create social networks that follow a Trust by Design paradigm. This paper introduces and discusses the above-­mentioned challenges and presents a novel new social media concept seeking to overcome current challenges.	Towards Trust­-based Decentralized Ad-Hoc Social Networks	NA	2018
Linda Anticoli:Marco Basaldella	In this paper we explore possible negative drawbacks in the use of wearable sensors, i.e., wearable devices used to detect different kinds of activity, e.g., from step and calories counting to heart rate and sleep monitoring. These technologies, which in the latter years witnessed a rapid development in terms of accuracy and diffusion, are now available on different platforms at reasonable prices and can lead to an healthier behavior in people using them. Nevertheless, we will try to investigate possibly harming behaviors related to these devices. We will provide different scenarios in which wearable sensors, in connection with social media, data mining, or other technologies, could prove harmful for their users.	Shut Up and Run: the Never-ending Quest for Social Fitness	NA:NA	2018
Patrick Wang:Rafael Angarita:Ilaria Renna	Social media is an amazing platform for enhancing public exposure. Anyone, even social bots, can reach out to a vast community and expose one's opinion. But what happens when fake news is (un)intentionally spread within a social media This paper reviews techniques that can be used to fabricate fake news and depicts a scenario where social bots evolve in a fully semantic Web to infest social media with automatically generated deceptive information.	Is this the Era of Misinformation yet: Combining Social Bots and Fake News to Deceive the Masses	NA:NA:NA	2018
Martin Cooney:Sepideh Pashami:Anita Sant'Anna:Yuantao Fan:Slawomir Nowaczyk	What would happen in a world where people could "see'' others' hidden emotions directly through some visualizing technology Would lies become uncommon and would we understand each other better Or to the contrary, would such forced honesty make it impossible for a society to exist The science fiction television show Black Mirror has exposed a number of darker scenarios in which such futuristic technologies, by blurring the lines of what is private and what is not, could also catalyze suffering. Thus, the current paper first turns an eye towards identifying some potential pitfalls in emotion visualization which could lead to psychological or physical harm, miscommunication, and disempowerment. Then, some countermeasures are proposed and discussed--including some level of control over what is visualized and provision of suitably rich emotional information comprising intentions--toward facilitating a future in which emotion visualization could contribute toward people's well-being. The scenarios presented here are not limited to web technologies, since one typically thinks about emotion recognition primarily in the context of direct contact. However, as interfaces develop beyond today's keyboard and monitor, more information becomes available also at a distance--for example, speech-to-text software could evolve to annotate any dictated text with a speaker's emotional state.	Pitfalls of Affective Computing: How can the automatic visual communication of emotions lead to harm, and what can be done to mitigate such risks	NA:NA:NA:NA:NA	2018
Luca Viganò:Diego Sempreboni	What if we delegated so much to autonomous AI and intelligent machines that They passed a law that forbids humans to carry out a number of professions We conceive the plot of a new episode of Black Mirror to reflect on what might await us and how we can deal with such a future.	Gnirut: The Trouble With Being Born Human In An Autonomous World	NA:NA	2018
Diego Sempreboni:Luca Viganò	Consider the following set-up for the plot of a possible future episode of the TV series Black Mirror: human brains can be connected directly to the net and MiningMind Inc. has developed a technology that merges a reward system with a cryptojacking engine that uses the human brain to mine cryptocurrency (or to carry out some other mining activity). Part of our brain will be committed to cryptographic calculations (mining), leaving the remaining part untouched for everyday operations, i.e., for our brain's normal daily activity. In this short paper, we briefly argue why this set-up might not be so far fetched after all, and explore the impact that such a technology could have on our lives and our society.	MMM: May I Mine Your Mind	NA:NA	2018
Harshvardhan J. Pandit:Dave Lewis	The use of personal data is a double-edged sword that on one side provides benefits through personalisation and user profiling, while the other raises several ethical and moral implications that impede technological progress. Laws often try to reflect the shifting values of social perception, such as the General Data Protection Regulation (GDPR) catering to explicit consent over use of personal data, though actions may still be legal without being perceived as acceptable. Black Mirror is a TV series that serves to imagine scenarios that test the boundary of such perceptions, and is often described as being futuristic. In this paper, we discuss how existing technologies have already coalesced towards calculating a probability metric or rating as presented by the episode 'Nosedive'. We present real-world instances of such technologies and their applications, and how they can be easily expanded using the interminable web. The dilemma posed by the ethics of such technological applications is discussed using the 'Ethics Canvas', our methodology and tool for encouraging discussions on ethical implications in responsible innovation.	Ease and Ethics of User Profiling in Black Mirror	NA:NA	2018
Marie-Laure Mugnier:Catherine Roussey:Pierre Senellart	It is our great pleasure to welcome you to the WWW 2018 Reasoning on Data Workshop. This workshop will gather people on a timely issue at the crossroad on knowledge representation and reasoning, data management, and the Semantic Web: How to use knowledge to make better use of data The workshop will more precisely focus on reasoning techniques that allow us to exploit domain knowledge in data access. An emblematic task is query answering, but knowledge can be exploited within the whole data lifecycle.	Reasoning on Data Workshop Chairs' Welcome and Organization	NA:NA:NA	2018
Mehdi Terdjimi:Lionel Médini:Michael Mrissa	Today's Web applications tend to reason about cyclic data (i.e. facts that re-occur periodically) on the client side. Although they can benefit from efficient incremental maintenance algorithms capable of handling frequent data updates, existing rule-based algorithms cause successive re-derivations of previously inferred information. In this paper, we propose an incremental maintenance approach for rule-based reasoning that prevents successive re-computations of fact derivations. We tag (i.e. annotate) facts to keep trace of their provenance and validity. We compare our solution with the DRed-based incremental reasoning algorithm and show that it significantly outperforms this algorithm for fact updates in re-occurring situations, to the cost of tagging facts at their first insertion. Our experiments show that this cost can be recovered within a small number of cycles of deletions and reinsertions of explicit facts. We discuss the utility and limitations of our approach on Web clients and provide implementation packages of this reasoner that can be directly integrated in Web applications, on both server and client sides.	Web Reasoning Using Fact Tagging	NA:NA:NA	2018
Viet-Phi Huynh:Paolo Papotti	Fact checking is the task of determining if a given claim holds. Several algorithms have been developed to check facts with reference information in the form of knowledge bases. While individual algorithms have been experimentally evaluated, we provide a first publicly available benchmark evaluating fact checking implementations across a range of assumptions about the properties of the facts and the reference data. We used our benchmark to compare algorithms designed on different principles and assumptions, as well as algorithms that can solve similar tasks developed in closely related communities. Our evaluation provided us with a number of new insights concerning the factors that impact the performance of the different methods.	Towards a Benchmark for Fact Checking with Knowledge Bases	NA:NA	2018
Jérôme David:Jérôme Euzenat:Pierre Genevès:Nabil Layaïda	Query transformations are ubiquitous in semantic web query processing. For any situation in which transformations are not proved correct by construction, the quality of these transformations has to be evaluated. Usual evaluation measures are either overly syntactic and not very informative the result being: correct or incorrect or dependent from the evaluation sources. Moreover, both approaches do not necessarily yield the same result. We suggest that grounding the evaluation on query containment allows for a data-independent evaluation that is more informative than the usual syntactic evaluation. In addition, such evaluation modalities may take into account ontologies, alignments or different query languages as soon as they are relevant to query evaluation.	Evaluation of Query Transformations without Data: Short paper	NA:NA:NA:NA	2018
Franz Baader:Pavlos Marantidis:Maximilian Pensel	Ontology-mediated query answering can be used to access large data sets through a mediating ontology. It has drawn considerable attention in the Description Logic (DL) community where both the complexity of query answering and practical query answering approaches based on rewriting were investigated in detail. Surprisingly, there is still a gap in what is known about the data complexity of query answering w.r.t. ontologies formulated in the inexpressive DL FL0. While it is known that the data complexity of answering conjunctive queries w.r.t. FL0 ontologies is coNP-complete, the exact complexity of answering instance queries was open until now. In the present paper, we show that answering instance queries w.r.t. FL0 ontologies is in P for data complexity. Together with the known lower bound of P-completeness for a fragment of FL0, this closes the gap mentioned above.	The Data Complexity of Answering Instance Queries in FL0	NA:NA:NA	2018
Marie-Francine Moens:Gareth J. F. Jones:Saptarshi Ghosh:Debasis Ganguly:Tanmoy Chakraborty:Kripabandhu Ghosh	User-generated content on online social media (OSM) platforms has become an important source of real-time information during emergency events. The SMERP workshop series aims to provide a forum for researchers working on utilizing OSM for emergency preparedness and aiding post-emergency relief operations. The workshop aims to bring together researchers from diverse fields - Information Retrieval, Data Mining and Machine Learning, Natural Language Processing, Social Network Analysis, Computational Social Science, Human Computer Interaction - who can potentially contribute to utilizing social media for emergency relief and preparedness. The first SMERP workshop was held in April 2017 in conjunction with the ECIR 2017 conference. This 2nd SMERP Workshop with The Web Conference 2018 includes two keynote talks, a peer-reviewed research paper track, and a panel discussion.	WWW'18 Workshop on Exploitation of Social Media for Emergency Relief and Preparedness: Chairs' Welcome & Organization	NA:NA:NA:NA:NA:NA	2018
Dheeraj Kumar:Satish V. Ukkusuri	Hurricane evacuation is a complex process and a better understanding of the evacuation behavior of the coastal residents could be helpful in planning better evacuation policy. Traditionally, various aspects of the household evacuation decisions have been determined by post-evacuation questionnaire surveys, which are usually time-consuming and expensive. Increased activity of users on social media, especially during emergencies, along with the geo-tagging of the posts, provides an opportunity to gain insights into user's decision-making process, as well as to gauge public opinion and activities using the social media data as a supplement to the traditional survey data. This paper leverages the geo-tagged Tweets posted in the New York City (NYC) in wake of Hurricane Sandy to understand the evacuation behavior of the residents. Based on the geo-tagged Tweet locations, we classify the NYC Twitter users into one of the three categories: outside evacuation zone, evacuees, and non-evacuees and examine the types of Tweets posted by each group during different phases of the hurricane. We establish a strong link between the social connectivity with the decision of the users to evacuate or stay. We analyze the geo-tagged Tweets to understand evacuation and return time and evacuation location patterns of evacuees. The analysis presented in this paper could be useful for authorities to plan a better evacuation campaign to minimize the risk to the life of the residents of the emergency hit areas.	Utilizing Geo-tagged Tweets to Understand Evacuation Dynamics during Emergencies: A case study of Hurricane Sandy	NA:NA	2018
Anastasia Moumtzidou:Stelios Andreadis:Ilias Gialampoukidis:Anastasios Karakostas:Stefanos Vrochidis:Ioannis Kompatsiaris	Disaster monitoring based on social media posts has raised a lot of interest in the domain of computer science the last decade, mainly due to the wide area of applications in public safety and security and due to the pervasiveness not solely on daily communication but also in life-threating situations. Social media can be used as a valuable source for producing early warnings of eminent disasters. This paper presents a framework to analyse social media multimodal content, in order to decide if the content is relevant to flooding. This is very important since it enhances the crisis situational awareness and supports various crisis management procedures such as preparedness. Evaluation on a benchmark dataset shows very good performance in both text and image classification modules.	Flood Relevance Estimation from Visual and Textual Content in Social Media Streams	NA:NA:NA:NA:NA:NA	2018
Samujjwal Ghosh:Maunendra Sankar Desarkar	Proper formulation of features plays an important role in short-text classification tasks as the amount of text available is very little. In literature, Term Frequency - Inverse Document Frequency (TF-IDF) is commonly used to create feature vectors for such tasks. However, TF-IDF formulation does not utilize the class information available in supervised learning. For classification problems, if it is possible to identify terms that can strongly distinguish among classes, then more weight can be given to those terms during feature construction phase. This may result in improved classifier performance with the incorporation of extra class label related information. We propose a supervised feature construction method to classify tweets, based on the actionable information that might be present, posted during different disaster scenarios. Improved classifier performance for such classification tasks can be helpful in the rescue and relief operations. We used three benchmark datasets containing tweets posted during Nepal and Italy earthquakes in 2015 and 2016 respectively. Experimental results show that the proposed method obtains better classification performance on these benchmark datasets.	Class Specific TF-IDF Boosting for Short-text Classification: Application to Short-texts Generated During Disasters	NA:NA	2018
Ribhav Soni:Sukomal Pal	Microblogging sites like Twitter, Facebook, etc., are important sources of first-hand accounts during disaster situations, and have the potential to significantly aid disaster relief efforts. The IRMiDis track at FIRE 2017 focused on developing and comparing IR approaches to automatically identify and match tweets that indicate the need or availability of a resource, leading to the creation of a benchmark dataset for future improvements in this task. However, based on our experiments, we argue that the gold standard data obtained in the track is substantially incomplete. We also discuss some reasons why it may have been so, and provide some suggestions for making more robust ground truth data in such tasks.	Gold Standard Creation for Microblog Retrieval: Challenges of Completeness in IRMiDis 2017	NA:NA	2018
Ritam Dutt:Kaustubh Hiware:Avijit Ghosh:Rameshwar Bhaskaran	We present SAVITR, a system that leverages the information posted on the Twitter microblogging site to monitor and analyse emergency situations. Given that only a very small percentage of microblogs are geo-tagged, it is essential for such a system to extract locations from the text of the microblogs. We employ natural language processing techniques to infer the locations mentioned in the microblog text, in an unsupervised fashion and display it on a map-based interface. The system is designed for efficient performance, achieving an F-score of 0.81, and is approximately two orders of magnitude faster than other available tools for location extraction.	SAVITR: A System for Real-time Location Extraction from Microblogs during Emergencies	NA:NA:NA:NA	2018
Cheng-Te Li:Lun-Wei Ku	With the rapid growing of social networking services (e.g., Facebook and Twitter), being able to process data come from such platforms has gained much attention in recent years. SocialNLP is a new inter-disciplinary area of natural language processing (NLP) and social computing. There are three plausible directions of SocialNLP: (1) addressing issues in social computing using NLP techniques; (2) solving NLP problems using information from social media; and (3) handling new problems related to both social computing and natural language processing. Several challenges are foreseeable in SocialNLP. First, the message lengths on social media are usu-ally short, and thus it is difficult to apply traditional NLP approaches directly. Second, social media contains heterogeneous information (e.g. tags, friends, followers, likes, and retweets) that should be considered together with the contents for better quality of analysis. Finally, social media contents always involve multiple persons with slangs and jargons, and usually require special techniques to process. We organize SocialNLP in WWW 2018 with three goals. First, social media data is essentially generated and collected from online social services that are functioned based on Web techniques. One can leverage Web techniques to investigate various user behaviors and investigate the interactions between users. Second, user-generated data in social media is mainly in the form of text. Theories and techniques on Web information retrieval and natural language processing are desired for semantic understanding, accurate search, and efficient processing of big social media data. Third, from the perspective of application, if social media data can be effectively processed to distill the collective knowledge of users, novel Web applications, such as emergency management, social recommendation, and future prediction, can be developed with higher accuracy and better user experience. We expect SocialNLP workshop in WWW community can provide mutually-reinforced benefits for researchers in areas of Web techniques, information retrieval and social media analytics.	[email protected] 2018 Chairs' Welcome & Organization	NA:NA	2018
Xuetong Chen:Martin D. Sykora:Thomas W. Jackson:Suzanne Elayan	Depression is among the most commonly diagnosed mental disorders around the world. With the increasing popularity of online social network platforms and the advances in data science, more research efforts have been spent on understanding mental disorders through social media by analysing linguistic style, sentiment, online social networks and other activity traces. However, the role of basic emotions and their changes over time, have not yet been fully explored in extant work. In this paper, we proposed a novel approach for identifying users with or at risk of depression by incorporating measures of eight basic emotions as features from Twitter posts over time, including a temporal analysis of these features. The results showed that emotion-related expressions can reveal insights of individuals' psychological states and emotions measured from such expressions show predictive power of identifying depression on Twitter. We also demonstrated that the changes in an individual's emotions as measured over time bear additional information and can further improve the effectiveness of emotions as features, hence, improve the performance of our proposed model in this task.	What about Mood Swings: Identifying Depression on Twitter with Temporal Measures of Emotions	NA:NA:NA:NA	2018
Sinya Peng:Vincent S. Tseng:Che-Wei Liang:Man-Kwan Shan	Social media provides a vast continuous supply of dynamic and diverse information contents from the crowd, which serves as useful resources for predictive analytical applications. Although there exist already a number of studies on emerging topics detection, they focused on modelling of textual contents and emerging detection mechanism over topic popularity. To meet the real-life demands, prediction of emerging product topic, rather than detection, in the early stage is required. Besides, despite that some relevant studies considered social structure information, they suffer from the assumption that the complete network is available and the diffusion process only depends on social influence among members of networks. Moreover, not all social media sites provide the functionality to facilitate the development of online social networks. In this paper, we tackle the problem of emerging product topics prediction in social network with implicit networks. Two tasks, one for long-term forecast in pre-production stage and the other for short-term forecast in post-release stage, are investigated. We present a novel framework named Emerging Topics Predictor (ETP). Two novel features, namely author diversity and competition features, are also proposed to accommodate the diffusion process with implicit networks based on the rationale of product marketing. Through empirical evaluation on movie reviews from two real social media sites, ETP is shown to provide effective and efficient performance in predicting the emerging topics as early as possible. In particular, the experiment results show the promising effect of author diversity in emerging prediction. To the best of our knowledge, this work is among the very first studies on emerging product topic prediction in social media with considerations of implicit networks.	Emerging Product Topics Prediction in Social Media without Social Structure Information	NA:NA:NA:NA	2018
Saratchandra Indrakanti:Gyanit Singh:Justin House	Product reviews on modern e-commerce websites have evolved into repositories of valuable firsthand opinions on products. Showcasing the opinions that reviewers express on a product in a succinct way can not only promote the product, but also provide an engaging experience that simplifies the shopping journey for online shoppers. In the case of traditional media such as movies and books, employingblurbs or excerpts from critic reviews for promotional purposes is an established practice among movie publicists and book editors that has proven to be an effective way of capturing attention of customers. Such excerpts can be discovered from e-commerce product reviews to highlight interesting reviewer opinions and add emotive elements to otherwise bland e-commerce product pages. While traditional movie or book blurbs are manually extracted, they must be automatically extracted from e-commerce product reviews owing to the scale of catalogues. Further, traditional blurbs are generally phrased to be very positive in tone and sometimes may take some words out of context. However, excerpts for e-commerce products must represent the true opinions of the reviewers and must capture the context in which the words were used to retain trust of users. To that end, we introduce the problem of extracting engaging excerpts from e-commerce product reviews in this paper. We present methods to automatically discover such excerpts from reviews at scale by leveraging natural language properties such as syntactic structure of sentences and sentiment, and discuss some of the underlying challenges. We further present an evaluation of the effectiveness of the proposed methods in terms of the quality of the blurbs generated and their ranking orders produced.	Blurb Mining: Discovering Interesting Excerpts from E-commerce Product Reviews	NA:NA:NA	2018
Reshmi Gopalakrishna Pillai:Mike Thelwall:Constantin Orasan	The ability to automatically detect human stress and relaxation is crucial for timely diagnosing stress-related diseases, ensuring customer satisfaction in services and managing human-centric applications such as traffic management. Traditional methods employ stress-measuring scales or physiological monitoring which may be intrusive and inconvenient. Instead, the ubiquitous nature of the social media can be leveraged to identify stress and relaxation, since many people habitually share their recent life experiences through social networking sites. This paper introduces an improved method to detect expressions of stress and relaxation in social media content. It uses word sense disambiguation by word sense vectors to improve the performance of the first and only lexicon-based stress/relaxation detection algorithm TensiStrength. Experimental results show that incorporating word sense disambiguation substantially improves the performance of the original TensiStrength. It performs better than state-of-the-art machine learning methods too in terms of Pearson correlation and percentage of exact matches. We also propose a novel framework for identifying the causal agents of stress and relaxation in tweets as future work.	Detection of Stress and Relaxation Magnitudes for Tweets	NA:NA:NA	2018
Lipika Dey:Tirthanker Dasgupta:Priyanka Sinha	Breakthroughs in Artificial Intelligence (AI) and World Wide Web technologies have opened a new direction in Enterprise Intelligence, that is gradually transforming the way enterprises perform business and interact with their customers. This change is largely driven by the widespread consumer adoption of sophisticated AI technologies and web based social media. Consequently, almost all business enterprises face a number of challenges such as adoption of new business paradigms; customer centric business processes; issues with large, multi-modal, multilingual, and multicultural data, analyzing behavioral signals from social media; agility, security and many more. Therefore, the primary goal of this workshop is to bring together industry professionals and researchers working in the area of AI, Natural Language Processing (NLP), Machine-Learning, linguistics, social science, HCI, design and computer vision and those whose work concerns the intersection of these areas, together to provide a venue for the multidisciplinary discussion of how ubiquitous AI technologies can help in extracting social and enterprise intelligence for smart enterprise transformation while addressing the aforementioned challenges.	Social Sensing and Enterprise Intelligence: Towards a Smart Enterprise Transformation (SSEI 2018) Chairs' Welcome and Organization	NA:NA:NA	2018
Dominik Slezak	The AI methods are regaining a lot of attention in the areas of data analytics and decision support. Given the increasing amount of information and computational resources available, it is now possible for intelligent algorithms to learn from the data and assist humans more efficiently. Still, there is a question about the goals of learning and a form of the resulting data-driven knowledge. It is evident that humans do not operate with precise information in decision-making and, thus, it might be unnecessary to provide them with complete outcomes of analytical processes. Consequently, the next question arises whether approximate results of computations or results derived from the approximate data could be delivered more efficiently than their standard counterparts. Such questions are analogous to the ones about precision of calculations conducted by machine learning and KDD methods, whereby heuristic algorithms could be boosted by letting them rely on approximate computations. This leads us toward discussion of the importance of approximations in the areas of machine intelligence and business intelligence and, more broadly, the meaning of approximate derivations for various aspects of AI. In this talk, this discussion is supported by four industry-related case studies\footnoteThe first case study refers entirely to the author's work for Security On-Demand (\urlhttps://www.securityondemand.com/ ). The work on the second case study was co-financed by the EU Smart Growth Operational Programme 2014-2020 under the Innovation Voucher project POIR.02.03.02-14-0009/15-00. The work on the third case study is co-financed by the EU Smart Growth Operational Programme 2014-2020 under the project POIR.01.01.01-00-0831/17-00. The work on the fourth case study is co-financed by the EU Smart Growth Operational Programme 2014-2020 under the GameINN project POIR.01.02.00-00-0184/17-00 : \beginenumerate ıtem The approximate database engine based on the paradigms of rough-granular computing applied in the area of cyber-security analytics \citeslezak:queryengine,\citeslezak:scalablefeature, \citeslezak:cyberengine ıtem The similarity-based feature engineering methodology embedded into an HR support system working with heterogeneous information sources \citeslezak:jobs ; ıtem The ensemble-based attribute approximation approach that will be used in the area of online health support \citeOvu ; ıtem The process of approximate data generation that will be used for tuning an online gaming coaching platform \citeEsensei.\endenumerate	Toward Approximate Intelligence: Approximate Query Engines & Approximate Data Exploration	NA	2018
Galit B. Yom-Tov:Shelly Ashtar:Daniel Altman:Michael Natapov:Neta Barkay:Monika Westphal:Anat Rafaeli	We adjust sentiment analysis techniques to automatically detect customer emotion in on-line service interactions of multiple business domains. Then we use the adjusted sentiment analysis tool to report insights about the dynamics of emotion in on-line service chats, using a large data set of Telecommunication customer service interactions. Our analyses show customer emotions starting out negative and evolving into positive as the interaction ends. Also, we identify a close relationship between customer emotion dynamicsduring the service interaction and the concepts of service failure and recovery. This connection manifests in customer service quality evaluationsafter the interaction ends. Our study shows the connection between customer emotion and service quality as service interactions unfold, and suggests the use of sentiment analysis tools for real-time monitoring and control of web-based service quality.	Customer Sentiment in Web-Based Service Interactions: Automated Analyses and New Insights	NA:NA:NA:NA:NA:NA:NA	2018
Changzhou Li:Yao Lu:Junfeng Wu:Yongrui Zhang:Zhongzhou Xia:Tianchen Wang:Dantian Yu:Xurui Chen:Peidong Liu:Junyu Guo	Clustering narrow-domain short texts, such as academic abstracts, is an extremely difficult clustering problem. Firstly, short texts lead to low frequency and sparseness of words, making clustering results highly unstable and inaccurate; Secondly, narrow domain leads to great overlapping of insignificant words and makes it hard to distinguish between sub-domains, or fine-grained clusters. The vocabulary size is also too small to construct a good word bag needed by traditional clustering algorithms like LDA to give a meaningful topic distribution. A novel clustering model, Partitioned Word2Vec-LDA (PW-LDA), is proposed in this paper to tackle the described problems. Since the purpose sentences of an abstract contain crucial information about the topic of the paper, we firstly implement a novel algorithm to extract them from the abstracts according to its structural features. Then high-frequency words are removed from those purpose sentences to get a purified-purpose corpus and LDA and Word2Vec models are trained. After combining the results of both models, we can cluster the abstracts more precisely. Our model uses abstract text instead of keywords to cluster because keywords may be ambiguous and cause unsatisfied clustering results shown by previous work. Experimental results show that the clustering results of PW-LDA are much more accurate and stable than state-of-the-art techniques.	LDA Meets Word2Vec: A Novel Model for Academic Abstract Clustering	NA:NA:NA:NA:NA:NA:NA:NA:NA:NA	2018
Pankaj Trivedi:Arvind Singh	Payment transaction engine at PayU processes multimillion trans- actions every day through multiple payment gateways. Routing a transaction through an appropriate payment gateway is crucial to the engine for optimizing the availability and cost. The problem is that every transaction needs to choose one of K available payment gateways characterized by an unknown probability reward distri- bution. The reward for a gateway is a combination of its health and cost factors. The reward for a gateway is only realized when transaction is processed by the gateway i.e. by its success or failure. The objective of dynamic routing is to maximize the cumulative expected rewards over some given horizon of transactions' life. To do this, the dynamic switching system needs to acquire informa- tion about gateways (exploration) while simultaneously optimizing immediate rewards by selecting the best gateway at the moment (exploitation); the price paid due to this trade o is referred to as the regret. The main objective is to minimize the regret and maximize the rewards. The basic idea is to choose a gateway according to its probability of being the best gateway. The routing problem is a direct formulation of reinforcement learning (RL) problem. In an RL problem, an agent interacts with a dynamic, stochastic, and incompletely known environment, with the goal of finding an action-selection strategy, or policy, that optimizes some long-term performance measure. Thompson Sampling algorithm has experimentally been shown to be close to optimal.	Stochastic Multi-path Routing Problem with Non-stationary Rewards: Building PayU's Dynamic Routing	NA:NA	2018
Nabeel Albishry:Tom Crick:Theo Tryfonas:Tesleem Fagade	With an increasing number of consumers using social media platforms to share both their satisfaction and displeasure about the products and services they use every day, organisations with a customer service focus are recognising the importance of rapid--and genuine--online engagement with their customers. In turn, consumers increasingly judge organisations on the quality of customer service and degree of responsiveness to online queries. This paper presents an extensible framework for evaluating direct engagements of customer service teams with customers on Twitter. Furthermore, this framework provides the capability to measure and analyse indirect engagement with industry sector rivals, especially their patterns, frequency and intensity. By applying graph analysis to these Twitter interactions, our framework generates various analytical measures and visual representations, exemplified through a case study based on seven major UK telecoms companies. With a dataset consisting of 15,000 tweets and 3,500 user profiles, the results provide sustained evidence for indirect engagements between business rivals, with customer queries acting as a trigger for intense competition between companies based in the same industry sub-domain.	An Evaluation of Performance and Competition in Customer Services on Twitter: A UK Telecoms Case Study	NA:NA:NA:NA	2018
Manish Puri:Xu Du:Aparna S. Varde:Gerard de Melo	This research focuses on mining ordinances (local laws) and public reactions to them expressed on social media. We place particular emphasis on ordinances and tweets relating to Smart City Characteristics (SCCs), since an important aim of our work is to assess how well a given region heads towards a Smart City. We rely on SCCs as a nexus between a seemingly infinite number of ordinances and tweets to be able to map them, and also to facilitate SCC-based opinion mining later for providing feedback to urban agencies based on public reactions. Common sense knowledge is harnessed in our approach to reflect human judgment in mapping. This paper presents our research in ordinance and tweet mapping with SCCs, including the proposed mapping approach, our initial experiments, related discussion, and future work emerging therein. To the best of our knowledge, ours is among the first works to conduct mining on ordinances and tweets for Smart Cities. This work has a broader impact with a vision to enhance Smart City growth.	Mapping Ordinances and Tweets using Smart City Characteristics to Aid Opinion Mining	NA:NA:NA:NA	2018
Marc Spaniol:Ricardo Baeza-Yates:Julien Masanès	Time is a key dimension to understand the Web. It is fair to say that it has not received yet all the attention it deserves and TempWeb is an attempt to help remedy this situation by putting time as the center of its reflection. Studying time in this context actually covers a large spectrum, from the extraction of temporal information and knowledge, to diachronic studies for the design of infrastructural and experimental settings enabling a proper observation of this dimension.	TempWeb 2018 Chairs' Welcome and Organization	NA:NA:NA	2018
Andreas Spitz:Jannik Strötgen:Michael Gertz	For the temporal analysis of news articles or the extraction of temporal expressions from such documents, accurate document creation times are indispensable. While document creation times are available as time stamps or HTML metadata in many cases, depending on the document collection in question, this data can be inaccurate or incomplete in others. Especially in digitally published online news articles, publication times are often missing from the article or inaccurate due to (partial) updates of the content at a later time. In this paper, we investigate the prediction of document creation times for articles in citation networks of digitally published news articles, which provide a network structure of knowledge flows between individual articles in addition to the contained temporal expressions. We explore the evolution of such networks to motivate the extraction of suitable features, which we utilize in a subsequent prediction of document creation times, framed as a regression task. Based on our evaluation of several established machine learning regressors on a large network of English news articles, we show that the combination of temporal and local structural features allows for the estimation of document creation times from the network.	Predicting Document Creation Times in News Citation Networks	NA:NA:NA	2018
Bowen Zhang:Wing Cheong Lau	Predicting the popularity of a discussion topic in an online social network (OSN) or the responses to an online fund-raising campaign is a practical challenge of immense value. Previous work tries to predict the popularity of an online campaign by modeling information diffusion as a homogeneous temporal point process within a network of a single-type of actors. However, real-world information propagation often involved multiple types of actors. In particular, there are the so-called opinion leaders, e.g. online celebrities or influential OSN users with a huge number of followers, who can create a great impact on the visibility and thus the final popularity of an event by simply mentioning it in their tweets or postings. In this paper, we propose MASEP, a Multi-actor Self-exciting Process, to model and predict the popularity of different online campaigns involving multiple types of actors. MASEP combines a self-exciting branching process with a periodical decay process to capture the dynamics and interdependent relationship between opinion leaders and ordinary users during an online campaign. A closed-form expression is derived for the temporal campaign popularity under the MASEP model. Based on this closed-form expression, we can efficiently perform regression against the empirical activity measurements of an online campaign during its early stage to estimate the parameters of the corresponding MASEP model. The final popularity of the campaign can then be predicted. To demonstrate the efficacy of the MASEP-based approach, we apply it to predict the popularity of three types of online campaigns from different large-scale real-world datasets, namely, the total number of posts in retweeting cascades, the overall count of individual hashtags in posting streams, and the final number of sponsors for crowd-funding campaigns. In particular, using the initial 30% of each campaign data trace for training, our approach can achieve absolute prediction error (APE) of 13.25%, 15.7%, and 36.9% respectively for datasets of 3 different types of campaigns. This corresponds to a 26.1% to 63.2% reduction in prediction error when comparing to state-of-the-art approaches including SEISMIC, SpikeM, and STRM.	Temporal Modeling of Information Diffusion using MASEP: Multi-Actor Self-Exciting Processes	NA:NA	2018
Behrooz Mansouri:Mohammad Sadegh Zahedi:Ricardo Campos:Mojgan Farhoodi:Alireza Yari	The development of information retrieval algorithms and temporal information retrieval ones has been extensively carried out over the last few years. While several studies have been conducted, most of these researches relate to English, leading to a lack of knowledge in several other important languages. This includes the Persian one. In this work, we aim to shorten this gap by contributing, disseminating and enlarging the knowledge we have on temporal information retrieval aspects in Persian, which is one of the dominant languages in the Middle East, widely spoken in several countries. To achieve this objective, we propose to understand the use of temporal expressions on a large-scale Persian search engine query log consisting of 27M queries. In particular, we focus on explicit (e.g., June 2017) and relative temporal expressions (e.g., tomorrow) and try to understand (1) how often temporal expressions are used in web queries; (2) which type of temporal expressions (Date, Time, Duration and Set) are commonly used; (3) to which time (past, current or future) do temporal expressions mostly refer to; (4) to which category they often belong; (5) how often do user's reformulate their queries by adding temporal expressions; and (6) how using temporal expressions affects user's satisfaction. We believe that answering these questions may be beneficial for a large number of tasks including, user's behavior understanding and search engines' improvement effectiveness.	Understanding the use of Temporal Expressions on Persian Web Search	NA:NA:NA:NA:NA	2018
Henry S. Thompson:Jian Tong	We report here on the results of two studies using two and four monthly web crawls respectively from the Common Crawl (CC) initiative between 2014 and 2017, whose initial goal was to provide empirical evidence for the changing patterns of use of so-called persistent identifiers. This paper focusses on the tooling needed for dealing with CC data, and the problems we found with it. The first study is based on over 10^12 URIs from over 5 x 10^9 pages crawled in April 2014 and April 2017, the second study adds a further 3 x 10^9 pages from the April 2015 and April 2016 crawls. We conclude with suggestions on specific actions needed to enable studies based on CC to give reliable longitudinal information.	Can Common Crawl Reliably Track Persistent Identifier (PID) Use Over Time	NA:NA	2018
Melisachew Wudage Chekol:Heiner Stuckenschmidt	The emergence of open information extraction as a tool for constructing and expanding knowledge graphs has aided the growth of temporal data, for instance, YAGO, NELL and Wikidata. While YAGO and Wikidata maintain the valid time of facts, NELL records the time point at which a fact is retrieved from some Web corpora. Collectively, these knowledge graphs (KGs) store facts extracted from Wikipedia and other sources. Due to the imprecise nature of the extraction tools that are used to build and expand KGs, such as NELL, the facts in the KGs are weighted (a confidence value representing the correctness of a fact). Additionally, NELL can be considered as a transaction time KG because every fact is associated with extraction date. On the other hand, YAGO and Wikidata use the valid time model because they only maintain facts together with their validity time (temporal scope). In this paper, we propose a bitemporal model (that combines transaction and valid time models) for maintaining and querying probabilistic temporal knowledge graphs. We report our evaluation results of the proposed approach.	Towards Probabilistic Bitemporal Knowledge Graphs	NA:NA	2018
Behrooz Mansouri:Mohammad Sadegh Zahedi:Ricardo Campos:Mojgan Farhoodi:Maseud Rahgozar	Web searches are done by users every day on a million-daily basis. Many of these web searches are related to events, social occasions that attracts society's attention. Events may happen multiple times on cyclic or non-periodic occasions. These are known as spiky events. When these events occur, multiple spikes can be observed in query logs triggered by a change in the user's behaviour and an increase in the frequency of the user's queries. In this paper, we aim to understand the user's search behaviour towards this kind of events. To this regard, we propose a new taxonomy of spiky events which categorizes queries into two groups: periodic (ongoing, historical, traditional) and aperiodic (predictable and unpredictable), and study how various features concerning the query and the clicked web pages describe the user's behaviour, before, during, and after the event. To conduct this research, we consider 100 spiky events and rely on a two-year Persian search engine query log to analyse their related queries and associated information. The results obtained show that users have a different behaviour regarding the query frequency, length and temporality, depending on the category of the spiky event and that query formulation and clicked pages are also different for each category before, during and after the event. Understanding these user's behaviours and their relationship with the different categories may play an important role for any search engine looking to provide better services for their users.	Understanding User's Search Behavior towards Spiky Events	NA:NA:NA:NA:NA	2018
Julien Leblay:Melisachew Wudage Chekol	Knowledge Graphs (KGs) are a popular means to represent knowledge on the Web, typically in the form of node/edge labelled directed graphs. We consider temporal KGs, in which edges are further annotated with time intervals, reflecting when the relationship between entities held in time. In this paper, we focus on the task of predicting time validity for unannotated edges. We introduce the problem as a variation of relational embedding. We adapt existing approaches, and explore the importance example selection and the incorporation of side information in the learning process. We present our experimental evaluation in details.	Deriving Validity Time in Knowledge Graph	NA:NA	2018
Robert West:Leila Zia:Dario Taraborelli:Jure Leskovec	It is our great pleasure to welcome you to the Wiki Workshop at the Web Conference 2018. The goal of this workshop is to bring together researchers exploring all aspects of Wikimedia projects such as Wikipedia, Wikidata, and Wikimedia Commons. With members of the Wikimedia Foundation's Research team on the organizing committee and with the experience of successful workshops in 2015, 2016, and 2017, we aim to continue facilitating a direct pathway for exchanging ideas between the organization that coordinates Wikimedia projects and the researchers interested in studying them. We received 17 paper submissions (and counting) from all around the world covering a broad range of topics. Our program committee evaluated them regarding relevance, quality, and novelty, selecting 8 to be presented as posters and published in the workshop proceedings, and numerous others for poster presentation only.	Welcome on Behalf of the Wiki Workshop Chairs & Organization	NA:NA:NA:NA	2018
Christoph Hube:Besnik Fetahu	Quality in Wikipedia is enforced through a set of editing policies and guidelines recommended for Wikipedia editors. Neutral point of view (NPOV) is one of the main principles in Wikipedia, which ensures that for controversial information all possible points of view are represented proportionally. Furthermore, language used in Wikipedia should be neutral and not opinionated. However, due to the large number of Wikipedia articles and its operating principle based on a voluntary basis of Wikipedia editors; quality assurances and Wikipedia guidelines cannot always be enforced. Currently, there are more than 40,000 articles, which are flagged with NPOV or similar quality tags. Furthermore, these represent only the portion of articles for which such quality issues are explicitly flagged by the Wikipedia editors, however, the real number may be higher considering that only a small percentage of articles are of good quality or featured as categorized by Wikipedia. In this work, we focus on the case of language bias at the sentence level in Wikipedia. Language bias is a hard problem, as it represents a subjective task and usually the linguistic cues are subtle and can be determined only through its context. We propose a supervised classification approach, which relies on an automatically created lexicon of bias words, and other syntactical and semantic characteristics of biased statements. We experimentally evaluate our approach on a dataset consisting of biased and unbiased statements, and show that we are able to detect biased statements with an accuracy of 74%. Furthermore, we show that competitors that determine bias words are not suitable for detecting biased statements, which we outperform with a relative improvement of over 20%.	Detecting Biased Statements in Wikipedia	NA:NA	2018
Vevake Balaraman:Simon Razniewski:Werner Nutt	The collaborative knowledge base Wikidata is the central storage of Wikimedia projects, containing over 45 million data items. It acts as the hub for interlinking Wikipedia pages about a specific item in different languages, automates features such as infoboxes in Wikipedia, and is increasingly used for other applications such as data enrichment and question answering. Tracking the quality of Wikidata is an important issue for this project. In this paper we focus particularly on the completeness aspect. Several automated techniques have been adopted by Wikis to track and manage completeness, yet these techniques are generally subjective and do not provide a clear quality estimate at the level of entities. In this paper, we present an approach towards measuring Relative Completeness in Wikidata by comparison with data present for similar entities. This relative completeness approach is easily scalable with the introduction of new classes in the knowledge base, and has been implemented for all available entities in Wikidata. The results provide an intuition on the completeness of an entity comparing it with other similar entities. Here, we present our implementation approach along with a discussion on strategies and open challenges.	Recoin: Relative Completeness in Wikidata	NA:NA:NA	2018
Lijun Lyu:Besnik Fetahu	Wikipedia is one of the top visited resources on the Web, furthermore, it is used extensively as the main source of information for applications like Web search, question & answering etc. This is mostly attributed to Wikipedia's coverage in terms of topics and real-world entities and the fact that Wikipedia articles are constantly updated with new and emerging facts. However, only a small fraction of articles are considered to be of good quality. The large majority of articles are incomplete and have other quality issues. A strong quality indicator is the presence of external references from third-party sources (e.g. news sources) as suggested by the verifiability principle in Wikipedia. Even for the existing references in Wikipedia there is an inherent lag in terms of the publication time of cited resources and the time they are cited in Wikipedia articles. We propose a near real-time suggestion of news references for Wikipedia from a daily news stream. We model daily news into specific events, spanning from a day up to year. Thus, we construct an event-chain from which we determine when the information in an event has converged and consequentially based on a learning-to-rank approach suggest the most authoritative and complete news article to Wikipedia articles involved in a specific event. We evaluate our news suggestion approach on a set of 41 events extracted from Wikipedia currents event portal, and on new corpus consisting of daily news between the period of 2016-2017 with more than 14 million news articles. We are able to suggest news articles to Wikipedia pages with an overall accuracy of MAP=0.77 and with a minimal lag w.r.t the publication time of the news article.	Real-time Event-based News Suggestion for Wikipedia Pages from News Streams	NA:NA	2018
Thomas Pellissier Tanon:Lucie-Aimée Kaffee	Stability in Wikidata's schema is essential for the reuse of its data. In this paper, we analyze the stability of the data based on the changes in labels of properties in six languages. We find that the schema is overall stable, making it a reliable resource for external usage.	Property Label Stability in Wikidata: Evolution and Convergence of Schemas in Collaborative Knowledge Bases	NA:NA	2018
Laxmi Amulya Gundala:Francesca Spezzano	In this paper, we describe our on-going research on the problem of predicting needed hyperlinks between pairs of Wikipedia pages (u,v) that are not connected, yet show readers' search navigation from u to v. We propose a solution that first estimates how long will these searches last and then predicts new hyperlinks according to descending order of duration. Our initial experimental results show that our best solution achieves an AUROC of 0.77 on the Wikipedia Clickstream dataset and a [email protected]% of 1.0 and significantly beats the baselines.	Readers' Demanded Hyperlink Prediction in Wikipedia	NA:NA	2018
Finn Årup Nielsen	The linkage of ImageNet WordNet synsets to Wikidata items will leverage deep learning algorithm with access to a rich multilingual knowledge graph. Here I will describe our on-going efforts in linking the two resources and issues faced in matching the Wikidata and WordNet knowledge graphs. I show an example on how the linkage can be used in a deep learning setting with real-time image classification and labeling in a non-English language and discuss what opportunities lies ahead.	Linking ImageNet WordNet Synsets with Wikidata	NA	2018
Sebastián Ferrada:Nicolás Bravo:Benjamin Bustos:Aidan Hogan	Despite its importance to the Web, multimedia content is often neglected when building and designing knowledge-bases: though descriptive metadata and links are often provided for images, video, etc., the multimedia content itself is often treated as opaque and is rarely analysed. IMGpedia is an effort to bring together the images of Wikimedia Commons (including visual information), and relevant knowledge-bases such as Wikidata and DBpedia. The result is a knowledge-base that incorporates similarity relations between the images based on visual descriptors, as well as links to the resources of Wikidata and DBpedia that relate to the image. Using the IMGpedia SPARQL endpoint, it is then possible to perform visuo-semantic queries, combining the semantic facts extracted from the external resources and the similarity relations of the images. This paper presents a new web interface to browse and explore the dataset of IMGpedia in a more friendly manner, as well as new visuo-semantic queries that can be answered using 6 million recently added links from IMGpedia to Wikidata. We also discuss future directions we foresee for the IMGpedia project.	Querying Wikimedia Images using Wikidata Facts	NA:NA:NA:NA	2018
Tomás Sáez:Aidan Hogan	Info-boxes provide a summary of the most important meta-data relating to a particular entity described by a Wikipedia article. However, many articles have no info-box or have info-boxes with only minimal information; furthermore, there is a huge disparity between the level of detail available for info-boxes in English articles and those for other languages. Wikidata has been proposed as a central repository of facts to try to address such disparities, and has been used as a source of information to generate info-boxes. However, current processes still rely on human intervention either to create generic templates for entities of a given type or to create a specific info-box for a specific article in a specific language. As such, there are still many articles of Wikipedia without info-boxes but where relevant data are provided by Wikidata. In this paper, we investigate fully automatic methods to generate info-boxes for Wikipedia from the Wikidata knowledge graph. The primary challenge is to create ranking mechanisms that provide an intuitive prioritisation of the facts associated with an entity. We discuss this challenge, propose several straightforward metrics to prioritise information in info-boxes, and present an initial user evaluation to compare the quality of info-boxes generated by various metrics.	Automatically Generating Wikipedia Info-boxes from Wikidata	NA:NA	2018
Payam Barnaghi:Jean-Paul Calbimonte:Daniele Dell'Aglio	Applications in different domains require reactive processing of massive, dynamically generated streams of data. This trend is increasingly visible also on the Web, where more and more streaming sources are becoming available. These originate from social networks, sensor networks, the Internet of Things (IoT) and many other technologies that use the Web as a platform for sharing data. This has resulted in new Web-centric efforts such as the Web of Things (WoT), which focuses on exposing and describing the IoT resources on the Web; or the Social Web which provides protocols, vocabularies, and APIs to facilitate access to social communications and interactions on the Web.	Web Stream Processing Workshop Chairs' Welcome & Organization	NA:NA:NA	2018
Mathias De Brouwer:Femke Ongenae:Glenn Daneels:Esteban Municio:Jeroen Famaey:Steven Latré:Filip De Turck	Enabling real-time collection and analysis of cyclist sensor data could allow amateur cyclists to continuously monitor themselves, receive personalized feedback on their performance, and communicate with each other during cycling events. Semantic Web technologies enable intelligent consolidation of all available context and sensor data. Stream reasoning techniques allow to perform advanced processing tasks by correlating the consolidated data to enable personalized and context-aware real-time feedback. In this paper, these technologies are leveraged and evaluated to design a Proof-of-Concept application of a personalized real-time feedback platform for amateur cyclists. Real-time feedback about the user's heart rate and heart rate training zones is given through a web application. The performance and scalability of the platform is evaluated on a Raspberry Pi. This shows the potential of the framework to be used in real-life cycling by small groups of amateur cyclists, who can only access low-end devices during events and training.	Personalized Real-Time Monitoring of Amateur Cyclists on Low-End Devices: Proof-of-Concept & Performance Evaluation	NA:NA:NA:NA:NA:NA:NA	2018
Maria Bermudez-Edo:Payam Barnaghi	The data gathered from smart cities can help citizens and city manager planners know where and when they should be aware of the repercussions regarding events happening in different parts of the city. Most of the smart city data analysis solutions are focused on the events and occurrences of the city as a whole, making it difficult to discern the exact place and time of the consequences of a particular event. We propose a novel method to model the events in a city in space and time. We apply our methodology for vehicular traffic data basing our models in (convolutional) neuronal networks.	Spatio-Temporal Analysis for Smart City Data	NA:NA	2018
Julián Andrés Rojas Meléndez:Brecht Van de Vyvere:Arne Gevaert:Ruben Taelman:Pieter Colpaert:Ruben Verborgh	For smart decision making, user agents need live and historic access to open data from sensors installed in the public domain. In contrast to a closed environment, for Open Data and federated query processing algorithms, the data publisher cannot anticipate in advance on specific questions, nor can it deal with a bad cost-efficiency of the server interface when data consumers increase. When publishing observations from sensors, different fragmentation strategies can be thought of depending on how the historic data needs to be queried. Furthermore, both publish/subscribe and polling strategies exist to publish live updates. Each of these strategies come with their own trade-offs regarding cost-efficiency of the server-interface, user-perceived performance and cpu use. A polling strategy where multiple observations are published in a paged collection was tested in a proof of concept for parking spaces availability. In order to understand the different resource trade-offs presented by publish/subscribe and polling publication strategies, we devised an experiment on two machines, for a scalability test. The preliminary results were inconclusive and suggest more large scale tests are needed in order to see a trend. While the large-scale tests will be performed in future work, the proof of concept helped to identify the technical Open Data principles for the 13 biggest cities in Flanders.	A Preliminary Open Data Publishing Strategy for Live Data in Flanders	NA:NA:NA:NA:NA:NA	2018
Gustavo Gonçalves:Flávio Martins:João Magalhães	The rise of large data streams introduces new challenges regarding the delivery of relevant content towards an information need. This need can be seen as a broad topic of information. By identifying sub-streams within a broader data stream, we can retrieve relevant content that matches the multiple facets of the topic; thus summarizing information, and matching the initial need. In this paper, we propose to study the generation of sub-streams over time and compare various aggregation methods to summarize information. Our experiments were made using the standard TREC Real-Time Summarization (RTS) 2017 dataset.	Analysis of Subtopic Discovery Algorithms for Real-time Information Summarization	NA:NA:NA	2018
Danh Le-Phuoc	The join operator is a core component of an RDF Stream Processing engine. The join operations usually dominate the processing load of a query execution plan. Due to the constantly updating nature of continuous queries, the query optimiser has to frequently change the optimal execution plan for a query. However, optimising the join executing plan for every execution step might be prohibitively expensive, hence, dynamic optimisation of continuous join operations is still a challenging problem so far. Therefore, this paper proposes the first adaptive optimisation approach towards this problem in the context of RDF Stream Processing. The approach comes with two dynamic cost-based optimisation algorithms which use a light-weight process to search for the best execution plan for every execution step. The experiments show the encouraging results towards this direction.	Adaptive Optimisation For Continuous Multi-Way Joins Over RDF Streams	NA	2018
Erik Wilde:Mike Amundsen:Mehdi Medjaoui	Welcome to the 9th International Workshop on Web APIs and Service Architecture (WS-REST). First held in 2010 at WWW in Raleigh, North Carolina, USA, this 2018 edition of the WS-REST Workshop Series is proud to be a part of the renowned WWW conference series in Lyon, France. WS-REST 2018 brings together a community of researcher and practitioners interested in Web APIs and service architecture. Bringing Research and Industry Together In keeping with the history of WS-REST events, the 2018 edition strives to bring together vital content from both the Web Services and REST communities. This year we are hosting two individual tracks (Research and Industry) as a way to continue and strengthen this collaboration between academic and applied experience. The research track submissions received careful peer-review and will be published in the Web Conference proceedings. Industry track submissions focus on field-tested examples and use-cases in the form of extended abstracts or position papers and were selected by the organizers with advice from select program committee members. APIs, Services, and REST APIs have become the connective fabric of the Web and any application area that uses Internet or Web technologies. The goal of WS-REST 2018 is to provide a forum for researchers and practitioners where they can openly and freely exchange ideas about how they are using Web technologies in their APIs, what works and what does not work for them, and what challenges they see in the current landscape of standards and technologies. Our goal is to capture both the state of the art when it comes to Web APIs and service architecture, but to also provide a forum that identifies some of the most pressing issues in that space, and can help solving them.	Workshop on Web APIs and Service Architecture (WS-REST) Chairs' Welcome & Organization	NA:NA:NA	2018
Anastasios Dimanidis:Kyriakos C. Chatzidimitriou:Andreas L. Symeonidis	Speeding up the development process of Web Services, while adhering to high quality software standards is a typical requirement in the software industry. This is why industry specialists usually suggest "driven by" development approaches to tackle this problem. In this paper, we propose such a methodology that employs Specification Driven Development and Behavior Driven Development in order to facilitate the phases of Web Service requirements elicitation and specification. Furthermore, we introduce gherkin2OAS, a software tool that aspires to bridge the aforementioned development approaches. Through the suggested methodology and tool, one may design and build RESTful services fast, while ensuring proper functionality.	A Natural Language Driven Approach for Automated Web API Development: Gherkin2OAS	NA:NA:NA	2018
Tobias Fertig:Peter Braun	Representational State Transfer (REST) is an efficient and by now established architectural style for distributed hypermedia systems. However, REST has not been designed for offline operations, yet many applications must also keep functioning when going offline for more than a few seconds. Burdening the programmer with knowledge about offline status is undesirable. RESTful applications can be described by a formal model. Therefore, we define a function to derive a formal model of the proxy for handling offline support on the client-side. We then extend existing caching approaches so that a client-side proxy can transparently hide the offline status from the application. We validate our solution with a proxy layer that covers all test cases derived from the model. Using our model and proxy, clients do not have to know and worry about whether they are online or offline.	Towards Offline Support for RESTful Systems	NA:NA	2018
Henry Vu:Tobias Fertig:Peter Braun	Being an architectural style rather than a specification or a standard, the proper design of REpresentational State Transfer (REST) APIs is not trivial, since developers have to deal with a flood of recommendations and best practices, especially the proper application of the hypermedia constraint requires some decent experience. Furthermore, testing RESTful APIs is a missing topic within literature and especially, hypermedia testing is not mentioned at all. To deal with this state of affairs, we have elaborated a Model-Driven Software Development (MDSD) approach for creating RESTful APIs. As this project matured, we also explored the possibility of Model-Driven Testing (MDT). This work addresses the challenges of hypermedia testing and proposes approaches to overcome them with MDT techniques. We present the results of hypermedia testing for RESTful APIs using a model verification approach that were discovered within our research. MDT enables the verification of the underlying model of a RESTful API and ensuring its correctness before initiating any code generation. Therefore, we can prevent a poorly designed model from being transformed into a poorly designed RESTful API.	Verification of Hypermedia Characteristic of RESTful Finite-State Machines	NA:NA:NA	2018
Sebastian R. Bader:Maria Maleshkova	A central vision of the Internet of Things is the representation of the physical world in a consistent virtual environment. Especially in the context of smart factories the connection of the different, heterogeneous production modules through a digital shop floor promises faster conversion rates, data-driven maintenance or automated machine configurations for use cases which haven't been recognized at design time. Nevertheless, these scenarios demand IoT representations of all participating machines and components, which requires high installation efforts and hardware adjustments. We propose an incremental process for bringing the shop floor closer to the IoT vision. Currently the majority of systems, components or parts are not yet connected with the internet and might not even provide the possibility to be technically equipped with sensors. However, those could be essential parts for a realistic digital shop floor representation. We therefore propose Virtual Representations, which are capable of independently calculating a physical object's condition by dynamically collecting and interpreting already available data through RESTful Web APIs. The internal logic of such Virtual Representations are further adjustable at runtime since changes to its respective physical object, its environment or updates to the resource itself should not cause any downtime.	Virtual Representations for an Iterative IoT Deployment	NA:NA	2018
Akshay Soni:Aasish Pappu:Robert Busa-Fekete:Krzysztof Dembczynski	Extreme Multilabel Classification (XMLC) is a very active and rapidly growing research area that deals with the problem of labeling an item with a small set of tags out of an extremely large number of potential tags. Applications include content understanding, document tagging, image tagging, biological sequence tagging, recommendation, etc. While the difficulty and the potential applications of XMLC are well understood in the core machine learning community, to the best of our knowledge, XMLC has not made inroads in the field of Information Retrieval (IR) and related areas. The aim of this workshop is to bring researchers from academia and industry in order to further advance this very exciting field and come up with potential applications of XMLC in new areas.	Extreme Multilabel Classification for Social Media Chairs' Welcome and Organization	NA:NA:NA:NA	2018
Anshumali Shrivastava	In this talk, I will present Merged-Averaged Classifiers via Hashing (MACH) for K-classification with ultra-large values of K. Compared to traditional one-vs-all classifiers that require $O(Kd)$ memory and inference cost, MACH only need $O(dłogK)$ (d is dimensionality) memory while only requiring $O(KłogK + dłogK )$ operation for inference. MACH is a generic K-classification algorithm, with provably theoretical guarantees, without any assumption on the relationship between classes. MACH uses universal hashing to reduce classification with a large number of classes to few (logarithmic many) independent classification tasks with small (constant) number of classes. I will show the first quantification of discriminability-memory tradeoff in multi-class classification. Using the simple idea of hashing, we can train ODP dataset with 100,000 classes and 400,000 features on a single Titan X GPU, with the classification accuracy of 19.28%, which is the best-reported accuracy on this dataset. Before this work, the best performing baseline is a one-vs-all classifier that requires 40 billion parameters (160 GB model size) and achieves 9% accuracy. In contrast, MACH can achieve 9% accuracy with 480x reduction in the model size (of mere 0.3GB). With MACH, we also demonstrate complete training of feature extracted fine-grained imagenet dataset (compressed size 104GB), with 21,000 classes, on a single GPU. To the best of our knowledge, this is the first work to demonstrate complete training of these extreme-class datasets on a single Titan X. Furthermore, the algorithm is trivially parallelizable. Our experiments show that we can train ODP datasets in 7 hours on a single GPU or in 15 minutes with 25 GPUs. Similarly, we can train classifiers over the fine-grained imagenet dataset in 24 hours on a single GPU which can be reduced to little over 1 hour with 20 GPUs.	Training 100,000 Classes on a Single Titan X in 7 Hours or 15 Minutes with 25 Titan Xs	NA	2018
Manik Varma	I will introduce extreme classi cation which is a new area of machine learning research focusing on multi-class & multi-label problems involving millions of categories. Extreme classification has opened up a new paradigm for thinking about key applications such as tagging, ranking and recommendation. I will discuss algorithms for some of these applications and present results on tagging on Wikipedia, product recommendation on Amazon and search and advertising on the Bing search engine. More details can be found on The Extreme Classification Repository webpage at http://manikvarma.org/downloads/XC/XMLRepository.html	Extreme Classification: Tagging on Wikipedia, Recommendation on Amazon & Advertising on Bing	NA	2018
Marius Kloft	Training of multi-class or multi-label classification machines are embarrassingly parallelizable via the one-vs.-rest approach. However, training of all-in-one multi-class learning machines such as multinomial logistic regression or all-in-one multi-class SVMs (MC-SVMs) is not parallelizable out of the box. In my talk, I present optimization strategies to distribute the training of some all-in-one multi-class SVMs over the classes, which makes them appealing for the use in extreme classification.	Distributed Optimization of All-in-one SVMs for Extreme Classfication	NA	2018
Lu Tang:Sougata Chaudhuri:Abraham Bagherjeiran:Ling Zhou	Structured prediction, where outcomes have a precedence order, lies at the heart of machine learning for information retrieval, movie recommendation, product review prediction, and digital advertising. Ordinal ranking, in particular, assumes that the structured response has a linear ranked order. Due to the extensive applicability of these models, substantial research has been devoted to understanding them, as well as developing efficient training techniques. One popular and widely cited technique of training ordinal ranking models is to exploit the linear precedence order and systematically reduce it to a binary classification problem. This facilitates the usage of readily available, powerful binary classifiers, but necessitates an expansion of the original training data, where the training data increases by $K-1$ times of its original size, with K being the number of ordinal classes. Due to prevalent nature of problems with large number of ordered classes, the reduction leads to datasets which are too large to train on single machines. While approximation methods like stochastic gradient descent are typically applied here, we investigate exact optimization solutions that can scale. In this paper, we present a divide-and-conquer (DC) algorithm, which divides large scale binary classification data into a cluster of machines and trains logistic models in parallel, and combines them at the end of the training phase to create a single binary classifier, which can then be used as an ordinal ranker. It requires no synchronization between the parallel learning algorithms during the training period, which makes training on large datasets feasible and efficient. We prove consistency and asymptotic normality property of the learned models using our proposed algorithm. We provide empirical evidence, on various ordinal datasets, of improved estimation and prediction performance of the model learnt using our algorithm, over several standard divide-and-conquer algorithms.	Learning Large Scale Ordinal Ranking Model via Divide-and-Conquer Technique	NA:NA:NA:NA	2018
Kishaloy Halder:Lahari Poddar:Min-Yen Kan	In public online discussion forums, the large user base and frequent posts can create challenges for recommending threads to users. Importantly, traditional recommender systems, based on collaborative filtering, are not capable of handlingnever-seen-before items (threads). We can view this task as a form of Extreme Multi-label Classification (XMLC), where for a newly-posted thread, we predict the set of users (labels) who will want to respond to it. Selecting a subset of users from the set of all users in the community poses significant challenges due to scalability, and sparsity. We propose a neural network architecture to solve thisnew thread recommendation task. Our architecture uses stacked bi-directional Gated Recurrent Units (GRU) for text encoding along with cluster sensitive attention for exploiting correlations among the large label space. Experimental evaluation with four datasets from different domains show that our model outperforms both the state-of-the-art recommendation systems as well as other XMLC approaches for this task in terms of MRR, Recall, and NDCG.	Cold Start Thread Recommendation as Extreme Multi-label Classification	NA:NA:NA	2018
Mathieu d'Aquin:Elena Cabrio	It is our great pleasure to welcome you to the WWW 2018 Challenges Track. It is the first time that the WWW conference includes such a track, which aim was to showcase the maturity of the state of the art on tasks common to the Web community and adjacent academic communities, in a controlled setting of rigorous evaluation. Through our call for challenge organisation, we also wanted to see which open questions might be seen as most relevant in this community today, and how groups of researchers might come together around shared resources (e.g. datasets) to address those questions in a hands-on, practical way. We received five proposals for challenges, and selected four of them, which were proposed by well established researchers in their respective domains. The topics addressed varied from being purely focused on a domain specific task, without constraints on the technical approach to take (e.g. music genre recognition) to fundamentally technical tasks, which can be seen as useful across domains (question answering), with two of the challenges representing a mix of both.	Challenges Track Chairs' Welcome	NA:NA	2018
Michaël Defferrard:Sharada P. Mohanty:Sean F. Carroll:Marcel Salathé	We here summarize our experience running a challenge with open data for musical genre recognition. Those notes motivate the task and the challenge design, show some statistics about the submissions, and present the results.	Learning to Recognize Musical Genre from Audio: Challenge Overview	NA:NA:NA:NA	2018
Benjamin Murauer:Günther Specht	This paper summarizes our contribution to the CrowdAI music genre classification challenge "Learning to Recognise Musical Genre from Audio on the Web'' as part of the WebConference 2018. We utilize different approaches from the field of music analysis to predict the music genre of given mp3 music files, including a convolutional neural network for spectrogram classification, deep neural networks and ensemble methods using various numerical audio features. Our best results were obtained by an extreme gradient boosting classifier.	Detecting Music Genre Using Extreme Gradient Boosting	NA:NA	2018
Jaehun Kim:Minz Won:Xavier Serra:Cynthia C. S. Liem	The automated recognition of music genres from audio information is a challenging problem, as genre labels are subjective and noisy. Artist labels are less subjective and less noisy, while certain artists may relate more strongly to certain genres. At the same time, at prediction time, it is not guaranteed that artist labels are available for a given audio segment. Therefore, in this work, we propose to apply the transfer learning framework, learning artist-related information which will be used at inference time for genre classification. We consider different types of artist-related information, expressed through artist group factors, which will allow for more efficient learning and stronger robustness to potential label noise. Furthermore, we investigate how to achieve the highest validation accuracy on the given FMA dataset, by experimenting with various kinds of transfer methods, including single-task transfer, multi-task transfer and finally multi-task learning.	Transfer Learning of Artist Group Factors to Musical Genre Classification	NA:NA:NA:NA	2018
Amelie Gyrard:Manas Gaur:Swati Padhee:Amit Sheth:Mihaela Juganaru-Mathieu	The Web of Things (WoT) is an extension of the Internet of Things (IoT) to ease the access to data generated by things/devices using the benefits of Web technologies. Data is exploited by WoT applications to monitor healthcare or even control home automation devices. The purpose of the Knowledge Extraction for the Web of Things (KE4WoT) challenge is to automatically extract the relevant knowledge from already designed smart WoT applications in various applicative domains. Those applications design and release Knowledge Bases (e.g., datasets and/or models) on the web.	Knowledge Extraction for the Web of Things (KE4WoT): WWW 2018 Challenge Summary	NA:NA:NA:NA:NA	2018
Fabricio F. de Faria:Ricardo Usbeck:Alessio Sarullo:Tingting Mu:Andre Freitas	This challenge focuses on the use of semantic representation methods to support Visual Question Answering: given a large image collection, find a set of images matching natural language queries. The task supports advancing the state-of-the-art in Visual Question Answering by focusing on methods which explore the interplay between contemporary machine learning techniques, semantic representation and reasoning mechanisms.	Question Answering Mediated by Visual Clues and Knowledge Graphs	NA:NA:NA:NA:NA	2018
Macedo Maia:Siegfried Handschuh:André Freitas:Brian Davis:Ross McDermott:Manel Zarrouk:Alexandra Balahur	The growing maturity of Natural Language Processing (NLP) techniques and resources is dramatically changing the landscape of many application domains which are dependent on the analysis of unstructured data at scale. The finance domain, with its reliance on the interpretation of multiple unstructured and structured data sources and its demand for fast and comprehensive decision making is already emerging as a primary ground for the experimentation of NLP, Web Mining and Information Retrieval (IR) techniques for the automatic analysis of financial news and opinions online. This challenge focuses on advancing the state-of-the-art of aspect-based sentiment analysis and opinion-based Question Answering for the financial domain.	WWW'18 Open Challenge: Financial Opinion Mining and Question Answering	NA:NA:NA:NA:NA:NA:NA	2018
Chung-Chi Chen:Hen-Hsen Huang:Hsin-Hsi Chen	This paper decribes our experimental methods and results in FiQA 2018 Task 1. There are two subtasks : (1) to predict continuous sentiment score between -1 to 1, and (2) to determine which aspect(s) are related to the content of financial tweets. First, we propose a preprocessing procedure for decomposing financial tweets. Second, we collect over 334K labeled financial tweets to enlarge the scale of the experiments. Third, the sentiment prediction task is separated into two steps in this paper, i.e., (1) bullish/bearish and (2) sentiment degree. We compare the results of the CNN, CRNN and Bi-LSTM models. Besides, we further combine the results of the best models in both steps as the model of subtask 1. Finally, we make an investigation of aspects in depth, and propose some clues for dealing with the 14 aspects.	Fine-Grained Analysis of Financial Tweets	NA:NA:NA	2018
Shijia E.:Li Yang:Mohan Zhang:Yang Xiang	Aspect-based financial sentiment analysis, which aims to classify the text instance into a pre-defined aspect class and predict the sentiment score for the mentioned target. In this paper, we propose a neural network model, Attention-based LSTM model with the Aspect information (ALA), to solve the financial opinion mining problem introduced by the WWW 2018 shared task. The proposed neural network model can adapt to the financial dataset so that the neural network can effectively understand the semantic information of the short text. We evaluate our model with the 10-fold cross-validation, and we compare our model with a variety of related deep neural network models.	Aspect-based Financial Sentiment Analysis with Deep Neural Networks	NA:NA:NA:NA	2018
Shijia E.:Shiyao Xu:Yang Xiang	The goal of question answering with financial data is selecting sentences as answers from the given documents for a question. The core of the task is computing the similarity score between the question and answer pairs. In this paper, we incorporate statistical features such as the term frequency-inverse document frequency (TF-IDF) and the word overlap in convolutional neural networks to learn optimal vector representations of question-answering pairs. The proposed model does not depend on any external resources and can be easily extended to other domains. Our experiments show that the TF-IDF and the word overlap features can improve the performance of basic neural network models. Also, with our experimental results, we can prove that models based on the margin loss training achieve better performance than the traditional classification models. When the number of candidate answers for each question is 500, our proposed model can achieve 0.622 in Top-1 accuracy (Top-1), 0.654 in mean average precision (MAP), 0.767 in normalized discounted cumulative gain (NDCG), and 0.701 in bilingual evaluation understudy (BLEU). If the number of candidate answers is 30, all the values of the evaluation metrics can reach more than 90%.	Incorporating Statistical Features in Convolutional Neural Networks for Question Answering with Financial Data	NA:NA:NA	2018
Hitkul Jangid:Shivangi Singhal:Rajiv Ratn Shah:Roger Zimmermann	Aspect based sentiment analysis aims to detect an aspect (i.e. features) in a given text and then perform sentiment analysis of the text with respect to that aspect. This paper aims to give a solution for the FiQA 2018 challenge subtask 1. We perform aspect-based sentiment analysis on the microblogs and headlines of financial domain. We use a multi-channel convolutional neural network for sentiment analysis and a recurrent neural network with bidirectional long short-term memory units to extract aspect from a given headline or microblog. Our proposed model produces a weighted average F1 score of 0.69 for the aspect extraction task and predicts sentiment intensity scores with a mean squared error of 0.112 on 10-fold cross validation. We believe that the developed system has direct applications in the financial domain.	Aspect-Based Financial Sentiment Analysis using Deep Learning	NA:NA:NA:NA	2018
Dayan de França Costa:Nadia Felix Felipe da Silva	This paper describes our system which participate in Task 1 of FiQA 2018. The task's focuses was to predict sentiment and aspects of financial microblog posts and headlines. The sentiment analysis for a specific company had to be predicted using a scale between -1 and 1, while the aspect prediction had to be predicted using a set of aspects which was given in train data. We had used Support Vector Regression (SVR) to predict the sentiments in both cases (microblog posts and headlines).	INF-UFG at FiQA 2018 Task 1: Predicting Sentiments and Aspects on Financial Tweets and News Headlines	NA:NA	2018
Guangyuan Piao:John G. Breslin	In this paper, we describe our ensemble approach for sentiment and aspect predictions in the financial domain for a given text. This ensemble approach uses Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) with a ridge regression and a voting strategy for sentiment and aspect predictions, and therefore, does not rely on any handcrafted feature. Based on 5-cross validation on the released training set, the results show that CNNs overall perform better than RNNs on both tasks, and the ensemble approach can boost the performance further by leveraging different types of deep learning approaches.	Financial Aspect and Sentiment Predictions with Deep Neural Networks: An Ensemble Approach	NA:NA	2018
